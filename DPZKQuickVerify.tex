%\newcommand{\enc}{\mathsf{Enc}}



\subsection{Witness Encoding}\label{sec:witencoding}
We start by describing a randomized encoding of the prover's extended witness $\wit\in \FF^N$ (henceforth referred as witness), where $N$ denotes the number of wires in the Boolean circuit representing the statement. Let $p,m$ and
$s$ be integers such that $N=pms$. We canonically view the
witness $\wit$ 
as $p\times m\times s$ matrix with entries $\wit[i,j,k]$ for $i\in [p]$,
$j\in [m]$ and $k\in [s]$. The encoding is specified by an independence 
parameter $\bi$, integers $\ell := s+\bi$, $h>2m$, $n>2\ell$, and sequences
$\bm{\zeta},\bm{\eta},\bm{\alpha}$ of distinct points in $\FF$ with cardinality 
$\ell,n,h$ respectively. We write $\bm{\zeta}=(\zeta_1,\ldots,\zeta_\ell)$,
$\bm{\eta}=(\eta_1,\ldots,\eta_n)$ and $\bm{\alpha}=(\alpha_1,\ldots,\alpha_h)$. 
Next we define the interpolation domain $G$ as $G=\{(\alpha_j,\zeta_k): j\in[m],
k\in [\ell]\}$ and evaluation domain $H$ as $H=\{(\alpha_j,\eta_k): j\in [h],
k\in [n]\}$. Finally, we encode $\wit$ as follows and denote the below randomized computation as $\ewit\gets \enc(\wit)$.
 We may denote $\enc(\wit)$ as the random variable denoting the encodings of $\wit$:
\begin{enumerate}[{\rm (i)}]
\item First we embed $\wit$ into a $p\times m\times \ell$ matrix $\hat{\wit}$
where $\hat{\wit}[i,j,k]=\wit[i,j,k]$ for $k\leq s$, while the entries
$\hat{\wit}[i,j,k]$ for $k>s$ are sampled from $\FF$ uniformly at random.
\item We construct bivariate polynomials $Q^i(x,y)$ with $deg_x(Q)<m$ and
$deg_y(Q) $ $<\ell$ such that $Q^i$ interpolates the slice
$\hat{\wit}[i,\cdot,\cdot]$ on $G$, i.e,
$Q^i(\alpha_j,\zeta_k)=\hat{\wit}[i,j,k]$. 
\item Let $\ewit$ denote the $p\times h\times n$ matrix, where the slice
$\ewit[i,\cdot,\cdot]$ consists of evaluations of $Q^i$ on $H$, i.e,
$\ewit[i,j,k]=Q^i(\alpha_j,\eta_k)$ for $i\in [p], j\in [h]$ and $k\in [n]$.
Then $\ewit$ is a randomized encoding of $\wit$.
\end{enumerate}
 It is easily seen
that $\ewit[i,\cdot,\cdot]\in \rsc{\eta}{n,\ell}\otimes \rsc{\alpha}{h,m}$. 
We remark that the above
encoding can be computed using $O(N\log N)$ field operations (see Appendix for
details \commentA{missing section no.}).
%%  Computing the encoding in O(Nlog N) time: Move to Appendix
%\noindent{\em Efficiently computing the encoding}: Although the previous
%description of the encoding involves the bivariate polynomials, 
%the prover does not explicitly need to compute the bivariate polynomials to
%construct the encoding. We describe an efficient method to construct the
%encoding. Given an $m\times \ell$
%matrix $\matx$, the prover first computes polynomials $p_j$ for $j\in
%[m]$ as $p_j(y) := \ifft(\matx[j,\cdot], \bm{\zeta})$. Then it constructs a $m\times n$
%matrix $\maty$, where the $j^{th}$ row of $\maty$ is evaluation of $p_j$ on the set
%$\bm{\eta}$, i.e, $\maty[j,\cdot] := \fft(p_j, \bm{\eta})$. Next, the prover constructs
%polynomials $q_k$ for $k\in [n]$ by interpolating the column $\maty[\cdot,k]$ on
%$\bm{\alpha}^0 = (\alpha_1,\ldots,\alpha_m)$, i.e, $q_k(x) := \ifft(\maty[\cdot,k],
%\bm{\alpha}^0)$. It obtains the encoding $\enc(\matx)$ as the $h\times n$ matrix
%$\matz$ whose columns are evaluations of polynomials $q_k$ on the set $\bm{\eta}$, i.e,
%$\matz[\cdot,k]=\fft(q_k,\bm{\alpha})$. The above computation involves
%$O(mn\log(mn))$ operations in $\FF$ for each $m\times \ell$ matrix. Thus, computing $\enc(\wit)$ takes
%$O(pmn\log(mn))$ which is $O(N\log{N})$. We also remark that $\maty$ is a submatrix
%of $\matz$, and thus
%$p_j=\ifft((\matz[j,1],\ldots,\matz[j,\ell]),(\eta_1,\ldots,\eta_{\ell}))$ for $j\in
%[m]$. This allows us to consistently define polynomials $p_j$ for $m<j\leq h$ by
%$p_j=\ifft((\matz[j,1],\ldots,\matz[j,\ell]),(\eta_1,\ldots,\eta_{\ell}))$.
%%
The encoding $\enc$ satisfies the following {\em bounded independence} property proved in Appendix \commentA{appendix number is missing}:
\begin{lemma}[Bounded Independence]\label{lem:boundedindependence}
Let $B\subseteq [n]$ be a set of size $\bi$. Let $\mc{U}(p,h,b)$ denote the
set of $p\times h\times b$ matrices $\matx$ such
that $\matx[i,\cdot,k]$ is a codeword in $\rsc{\alpha}{h,m}$ for all $i\in
[p],k\in [\bi]$. Then for any $p\times m\times s$ matrix $\wit$, the random
variable $\ewit_B := \{\ewit[\cdot,\cdot,B]: \ewit\sample \enc(\wit)\}$ is
distributed uniformly on $\mc{U}(p,h,b)$.
\end{lemma}


\subsection{Codes and Matrices}\label{sec:codesandmatrices}
For code $\rsc{\eta}{n,\ell}$, let $\Lambda_{n,\ell}$ denote the $n\times \ell$ matrix for the linear transformation that maps a vector $x\in \FF^\ell$ 
to the unique codeword $y$ in  $\rsc{\eta}{n,\ell}$ such that $y_i=x_i$ for $i\in [\ell]$. For codes  $\rsc{\alpha}{h,m}$, $\rsc{\eta}{n,s+\ell}$, $ \rsc{\eta}{n,2\ell}$, and $\rsc{\alpha}{h,2m}$, let  $\Lambda_{h,m},\Lambda_{n,s+\ell},\Lambda_{n,2\ell}$ and $\Lambda_{h,2m}$ be similar matrices. We denote the corresponding
parity-check matrices as   $\mc{H}_{n,\ell},\mc{H}_{h,m},\mc{H}_{n,s+\ell},\mc{H}_{n,2\ell}$. 

We notate the set of three dimensional $p\times h\times n$ matrices as $\mc{M}_{p,h,n}$ and
the set of two dimensional $h\times n$ matrices as $\mc{M}_{h, n}$. We
assume standard distance metrics on the sets $\mc{M}_{p,h,n}$ and $\mc{M}_{h,n}$.
Let $\mc{W}_1$ denote the set of matrices
$U$ in $\mc{M}_{p,h,n}$ such that the $n$-length vector $U[i,j,\cdot]$ is a
codeword in $\rsc{\eta}{n,\ell}$ for all $i,j$. Similarly let $\mc{W}_2$ denote the set of
matrices $U$ such that the $h$-length vector $U[i,\cdot,k]$ is a codeword in
$\rsc{\alpha}{h,m}$ for all $i,k$.  Let $\mc{W}=\mc{W}_1\cap \mc{W}_2$. $\mc{W}$ denotes the set of {\em well-formed} encodings  and consists of $U$ such that each slice $U[i,\cdot,\cdot]\in \rsc{\eta}{n,\ell}\otimes \rsc{\alpha}{h,m}$. For $U^\ast\in \mc{M}_{p,h,n}$ define
$d(U^\ast,\mc{W}_i)=\min\{\Delta_i(U^\ast,U):U\in \mc{W}_i\}$ for $i=1,2$.

\begin{comment}
We now fix the notation for some of the codes that will be
frequently used throughout this section. We use $L_1$ and $L_2$ to denote codes
$\rsc{\eta}{n,\ell}$ and $\rsc{\alpha}{h,m}$ respectively. Let $\mc{C}_1 := \ric{L_1}{h}$ 
and $\mc{C}_2 :=\cic{L_2}{n}$ denote the interleaved codes of $L_1$ and $L_2$. In addition, 
we use codes $L_3 := \rsc{\eta}{n,s+\ell}$, $L_4 := \rsc{\eta}{n,2\ell}$ and 
$L_5=\rsc{\alpha}{h,2m}$ to encode some intermediate computations in our protocols.
Let $\Lambda_{n,\ell}$ denote the matrix for the linear transformation that maps a vector $x\in \FF^\ell$ 
to the unique codeword $y$ in $L_1$ such that $y_i=x_i$ for $i\in [\ell]$. Thus $\Lambda_{n,\ell}$ is 
an $n\times \ell$ matrix. Let $\Lambda_{h,m},\Lambda_{n,s+\ell},\Lambda_{n,2\ell}$ and $\Lambda_{h,2m}$ be similar matrices
for the codes $L_2,L_3,L_4$ and $L_5$ respectively. We denote the
parity check matrices for $L_i$  by $\mc{H}_i$ for $i\in \{1,\ldots,5\}$. 
We notate the set of three dimensional $p\times h\times n$ matrices as $\mc{M}_{p,h,n}$ and
the set of two dimensional $h\times n$ matrices as $\mc{M}_{h, n}$. We
assume standard distance metrics on the sets $\mc{M}_{p,h,n}$ and $\mc{M}_{h,n}$.
\end{comment}

\subsection{Commitment of product codewords}\label{sec:matrixcommitment}

We now discuss the commitment scheme for matrices in $\rsc{\eta}{n,\ell} \otimes \rsc{\alpha}{h,m}$. Similar scheme works for other product codes also. 
A matrix $U$ in the above product code is
completely determined by the sub-matrix $\overline{U}=\{U[j,k]: j\in [m], k\in [\ell]\}$ and can be expressed as $U=\Lambda_{h,m}\overline{U}\Lambda_{n,\ell}^T$. 
A commitment to $U$ is defined as  a vector of commitments $\bm{c}=(c_1,\ldots,c_\ell)$
where $c_k=\comm(\overline{U}[\cdot,k])$ is the vector commitment for $k^{th}$
column of $\overline{U}$ for $k\in [\ell]$. We use the notation $\bm{c} = (c_1,\ldots,c_\ell) \gets \pccom(U)$. Given $\bm{c}$, commitment to $k$th column of $U$ for $k > \ell$ 
can be computed as  $c_k=\prod_{a\in [\ell]}(c_a)^{\Lambda_{n,\ell}^T[a,k]}$, thanks to the homomorphicity.

%The commitment scheme presents no difficulty in  proving inner-product relations for a column (or a linear combination of them) with a public vector. For example, to prove $\innp{x}{U[\cdot,k]}=v$ for any $k \in [n]$ and a public vector $x$, we first homomorphically compute commitment to $\overline{U}[\cdot,k]$ as $c_k=\prod_{a\in [\ell]}(c_a)^{\Lambda_1^T[a,k]}$.  Then we reformulate the inner-product as  $\innp{x}{U[\cdot,k]}=\innp{x}{\Lambda_2\overline{U}[\cdot,k]}=\innp{x^T\Lambda_2}{\overline{U}[\cdot,k]}$ where the last formulation involves a public vector $x^T\Lambda_2$ and private vector corresponding to commitment $\mathsf{cm}$. We make repeated use of such inner-product checks.   

\subsection{Oracle Construction}\label{sec:construct_oracle} 
Unlike prior IOP constructions such as \cite{ligero, aurora}, we additionally
obtain a homomorphic commitment on the encoded witness $\enc(\wit)$ and provide
oracle access to the commitment. 
For all $(i,k)\in [p]\times
[n]$, we  compute the commitment %sample randomness $\delta_{ik}\sample \FF$, and obtain
$c_{ik}=\comm(V_{ik})$ for the vector 
$V_{ik}=(\ewit[i,1,k],\ldots,\ewit[i,m,k])\in \FF^m$. 
Finally we define $p\times n$ matrix $\comoracle$ as
$\comoracle[i,k]=c_{ik}$. We write this as $\comoracle \gets \ocom(\ewit)$. We provide oracle access to $\comoracle$ where for a
query $Q\subseteq [n]$, the oracle responds with columns $\comoracle[\cdot,k]$ for
$k\in Q$. Note that $i^{th}$ row of the matrix $\comoracle$ commits to the
$i^{th}$ slice of $\ewit$.   This is different from  commitment of a product codeword 
(Section ~\ref{sec:matrixcommitment})  where only 
$\ell$ columns are committed. This is
because the verifier cannot access all $\ell$ commitments to the first $\ell$ columns
as it only has oracle access to $\comoracle$. 
\begin{comment}
\subsection{Well-formed Encodings}\label{sec:wellformedenc}
Let $\mc{W}$ denote the subset of $\mc{M}_{p,h,n}$
consisting of matrices $U$ such that $U[i,\cdot,\cdot]\in L_1\otimes L_2$ for all $i\in [p]$. 
We call an encoding $\ewit$ to be {\em well-formed} if $\ewit\in \mc{W}$. Note
that an encoding $\ewit\gets \enc(\wit)$ for any $\wit\in \FF^N$ is well formed. 
Let $\mc{W}_1$ denote the set of matrices
$U$ in $\mc{M}_{p,h,n}$ such that the $n$-length vector $U[i,j,\cdot]$ is a
codeword in $L_1$ for all $i,j$. Similarly let $\mc{W}_2$ denote the set of
matrices $U$ such that the $h$-length vector $U[i,\cdot,k]$ is a codeword in
$L_2$ for all $i,k$. It can be seen that $\mc{W}=\mc{W}_1\cap \mc{W}_2$. For $U^\ast\in \mc{M}_{p,h,n}$ define
$d(U^\ast,\mc{W}_i)=\min\{\Delta_i(U^\ast,U):U\in \mc{W}_i\}$ for $i=1,2$.
\end{comment}
\subsection{Witness Decoding}\label{sec:witdecoding}
We describe a decoding procedure $\dec$ for obtaining a witness $\wit$ from an encoding
$\ewit$. Let $\ewit\in \mc{W}$ be a well-formed encoding. Such an encoding can
be decoded slice by slice, i.e, for each $i\in [p]$, we interpolate bivariate
polynomial $Q^i\in \FF[x,y]$ with $deg_x(Q^i)<m$ and $deg_y(Q^i)<\ell$ such that
$Q^i$ interpolates $\ewit[i,\cdot,\cdot]$ on evaluation domain $H=\{(\alpha_j,\eta_k): j\in [h],
k\in [n]\}$. This can be accomplished using
standard algorithms. The decoded witness $\wit$ is then given by
$\wit[i,j,k]=Q^i(\alpha_j,\zeta_k)$. \commentA{nitin: can you simplify the following. it's a notational mess}We extend the above decoding procedure to
recover from slightly malformed encodings. Let $\ewit^*\in \mc{M}_{p,h,n}$ be such
that $d(\ewit^*,\ewit_1)<e_1 <(n-\ell)/2 $ and $d(\ewit^*,\ewit_2)<e_2 <(h-m)/2$ for some $\ewit_1\in
\mc{W}_1$ and $\ewit_2\in \mc{W}_2$. Let $E_1$ denote
the indices of the planes where $\ewit^*$ differs from $\ewit_1$ and 
$E_2$ denote the indices of the slabs where $\ewit^*$ differs from $\ewit_2$.
Let $F_1=[n]\backslash E_1$ and
$F_2=[h]\backslash E_2$. Note that each
slice $\ewit^*[i,\cdot,\cdot]$ satisfies
$d(\ewit^*[i,\cdot,\cdot],\mc{C}_1)<e_1$  and
$d(\ewit^*[i,\cdot,\cdot],\mc{C}_2)<e_2$. Since $e_1<(n-\ell)/2$ and
$e_2<(h-m)/2$, we conclude that $\ewit_1[i,\cdot,\cdot]$ and
$\ewit_1[i,\cdot,\cdot]$ are the unique codewords in $\mc{C}_1$ and $\mc{C}_2$
such that $d_1(\ewit[i,\cdot,\cdot],\ewit_1[i,\cdot,\cdot])<e_1$ and
$d_2(\ewit[i,\cdot,\cdot],\ewit_2[i,\cdot,\cdot])<e_2$. These codewords and thus
sets $F_1,F_2$ may be
determined using algorithms for decoding Reed Solomon codes \cite{CodingTheory}.
 From Lemma ~\ref{lem:bicdecoding}, it follows that for all $i\in [p]$, there exists $U^i\in \rsc{\eta}{n,\ell}\otimes \rsc{\alpha}{h,m}$ such
that $U^i[j,k]=\ewit^*[i,j,k]$ for $(j,k)\in F_2\times F_1$. We define the
``correction'' of $\ewit^*$ as $\ewit$ where
$\ewit[i,j,k]=U^i[j,k]$. Finally, we define the decoding $\dec(\ewit^*) =
\dec(\ewit)$.


\subsection{Linear Check Protocol}\label{sec:lincheck}
In this section, we describe an IPCP that allows a prover to prove knowledge of
witness $\wit\in \FF^N$ satisfying a linear constraint of the form $A\wit = b$
for some $A\in \FF^{M\times N}$ and $b\in \FF^M$. As before we veiw $\wit$ as
$p\times m\times s$ matrix where $N=pms$. As desribed previously in Sections
\ref{sec:witencoding} and \ref{sec:construct_oracle}, the prover obtains $\ewit\gets 
\enc(\wit)$ and $\pi=\comoracle \gets \ocom(\ewit)$. The prover then sets the oracle 
$\pi := \comoracle$, which can then be queried by the verifier. The broad
 outline of the protocol given below follows the steps--(a) reduce the problem to checking an inner-product argument via a product codeword commitment; (b) check consistency of the product codeword commitment and the oracle (c) check if the oracle committed a well-formed codeword. The full protocol appears in
Figure \ref{fig:linearcheck}.\smallskip

\noindent {\em Reduction to Inner-product}: To check $A\wit=b$, the verifier 
samples random $r\sample \FF^M$ and asks the prover to prove $r^TA\wit=r^Tb$.
Both prover and verifier view the
vector $r^TA\in \FF^N$ as a $p\times m\times s$ matrix $R$ and interpolate
polynomials $R^i(x,y)$ for $i\in [p]$ with $deg_x(R^i)<m$ and $deg_y(R^i)<s$
satisfying $R^i(\alpha_j,\zeta_k)=R[i,j,k]$. Let $Q^i$, $i\in [p]$ denote the
polynomials used in interpolating (and encoding) witness $\wit$. Then, 
$\wit[i,j,k]=Q^i(\alpha_j,\zeta_k)$. The check $\innp{R}{\wit}=r^Tb$ reduces to
$\sum_{i,j,k}R^i(\alpha_j,\zeta_k).Q^i(\alpha_j,\zeta_k)=r^Tb$ where $i,j$ and
$k$ run over indices in $[p],[m]$ and $[s]$ respectively. 
%The preceding identity can be succinctly expressed as $\sum_{k\in [s]}p(\zeta_k)=r^Tb$, where $p(\cdot)$ denotes the polynomial $\sum_{j=1}^m\sum_{i=1}^p R^i(\alpha_j,\cdot)Q^i(\alpha_j,\cdot)$. Let $\zetabar$ denote the vector $(\zeta_1,\ldots,\zeta_s)$ and let $p(\zetabar)$ denote the vector $(p(\zeta_1),\ldots,p(\zeta_s))$. Then the previous polynomial identity reduces to the inner product check $\innp{1^s}{p(\zetabar)}=r^Tb$.  
For $j\in [h]$, denoting  the polynomial $\sum_{i\in
[p]}R^i(\alpha_j,\cdot)Q^i(\alpha_j,\cdot)$ as $p_j$ (which will be of degree less than $s + \ell$), we can rewrite the expression as $\sum_{k\in[s]}\sum_{j\in[m]}p_j(\zeta_k)=r^Tb$.  We define the matrix $P$ by $P[j,k]=p_j(\eta_k)$ for $j\in [h],k\in [n]$ which is a product codeword from $\rsc{\eta}{n,s+\ell} \otimes \rsc{\alpha}{h,2m}$ and fully determined by sub-matrix $\overline{P}$ consisting of the first $2m$ rows and the first $s+\ell$ columns of $P$. Now our idea is to commit  $P$ using  commitment for product codeword i.e. $(c_1,\ldots,c_{s+\ell}) \gets \pccom(P)$ and write the above expression as an inner-product argument involving $\overline{P}$ such as $\innp{*}{\overline{P}\times\varphi} = r^Tb$, where $*$ is a public vector and $\varphi$ is a vector and so the commitment for $\overline{P}\times\varphi$ can be computed given the commitment of the columns of $\overline{P}$.

\commentA{now complete and derive the above inner product form}. 

Define $p(\cdot) = \sum_{j\in [m]} p_j(\cdot)$, $p(\zetabar) = [p(\zeta_1),\ldots,p(\zeta_s)]$, and $p(\etabar) = [p(\eta_1),\ldots,p(\eta_{s+\ell})]$. \newline
\noindent {\em Consistency of $P$ and $p(\zetabar)$} Let $\Phi$ be a $s\times (s+\ell)$ matrix such that $p(\zetabar)^T = \Phi \times p(\etabar)^T$. \\
  %$[p_j(\zeta_1),\ldots,p_j(\zeta_s)]^T =\Phi \times [p_j(\eta_1),\ldots,p_j(\eta_{s+\ell})]^T$ $\forall j\in[h]$ . Define $p(\cdot) = \sum_{j\in [m]} p_j(\cdot)$. 
 %$\Rightarrow [\sum_{j\in[m]} p_j(\zeta_1),\ldots,\sum_{j\in[m]}p_j(\zeta_s)]^T =\Phi \times [\sum_{j\in[m]}p_j(\eta_1),\ldots,\sum_{j\in[m]} p_j(\eta_{s+\ell})]^T$. Define $p(\cdot) = \sum_{j\in [m]} p_j(\cdot)$.
 Observe that, $p(\etabar)^T = \overline{P}^T \times [1^m || 0^m]^T$\\
 $\Rightarrow p(\zetabar)^T =\Phi \times \overline{P}^T \times [1^m || 0^m]^T$\\
 $\Rightarrow p(\zetabar)^T =[1^m || 0^m] \times \overline{P}\times \Phi^T$\\
 Therefore, the check $\sum_{k\in[s]}\sum_{j\in[m]}p_j(\zeta_k)=r^Tb$ reduces to $\innp{p(\zetabar)}{[1^s]} =r^Tb$ that can be viewed as $\innp{[1^m || 0^m] \times \overline{P}\times \Phi^T}{[1^s]} = \innp{[1^m||0^m]^T}{\overline{P}\times \Phi^T \times [1^s]^T} = r^Tb$. Here, $[1^m||0^m]^T$ is the public vector and the linear transformation $\varphi = \Phi^T \times [1^s]^T$. 
 Given commitment of $P$,  $c_1,\ldots,c_{s+\ell}$, the
 commitment to $\overline{P}\varphi$ can be computed as
 $\mathsf{cm}=\prod_{k=1}^{s+\ell}(c_k)^{\varphi_k}$. %Using an inner-product argument
 %the prover can show that the commitment $\mathsf{cm}$ opens to vector $z$ such
 %that $\innp{1^m||0^m}{z}=r^Tb$. Binding property of the commitment ensures that
 %$z=\overline{P}\varphi$ with overwhelming probability.
 In the full protocol, 
 the prover initially commits to a random $P_0\in \FF^{2m}$ subject to $\innp{1^m}{P_0}=0$ and 
 uses $\beta P_0 + \overline{P}\times \varphi$ as witness in the inner product protocol. Here 
 $\beta\sample \FF\backslash \{0\}$ is randomly chosen by the verifier. This randomization
 precludes the need for the inner-product argument to be zero knowledge, and as we will later
 see, helps reduce interaction among provers in its distributed variant.
\smallskip  

\begin{comment}
\noindent {\em A product codeword Commitment}: 
We introduce an $h\times n$ matrix $P$
which serves as a bridge between the oracle $\pi$ and the vector
$p(\zetabar)$. For $j\in [h]$, let $p_j$ denote the polynomial $\sum_{i\in
[p]}R^i(\alpha_j,\cdot)Q^i(\alpha_j,\cdot)$ (which will be of degree less than $s + \ell$). We define the matrix $P$ by
$P[j,k]=p_j(\eta_k)$ for $j\in [h],k\in [n]$. Noting that the matrix $P$ is in the
product of codes $\rsc{\eta}{n,s+\ell}$ and $\rsc{\alpha}{h,2m}$ and is fully determined by sub-matrix $\overline{P}$ consisting of the first $2m$ rows and
the first $s+\ell$ columns of $P$, we let the
prover commit to product code-word $P$, via column commitments of  $\overline{P}$ as $c_1,\ldots,c_{s+\ell}$. Intuitively, the code structure on $P$
helps in achieving soundness, as a cheating prover would be forced to commit to 
a $P$ which is substantially different from an honest $P$, and thus increasing
the likelihood of getting ``caught'' in the consistency checks below.\smallskip 

\noindent {\em Consistency of matrix $P$ and $p(\zetabar)$}:
Let $\etabar$ denote the vector
$(\eta_1,\ldots,\eta_{s+\ell})$, and let $\Phi$ denote the $s\times (s+\ell)$
matrix such that $p(\zetabar)=\Phi \times p(\etabar)$. Further, observe that
$p(\etabar)=\overline{P}^T[1^m||0^m]^T$ and thus
$p(\zetabar)=\Phi\overline{P}^T[1^m||0^m]^T$. Now, since
$\innp{1^s}{p(\zetabar)}=p(\zetabar)^T[1^s]=[1^m||0^m]\overline{P}\Phi^T[1^s]=\innp{1^m||0^m}{\overline{P}\varphi}$
where $\varphi=\Phi^T[1^s]$, the inner-product check $\innp{1^s}{p(\zetabar)}=r^Tb$ reduces to $\innp{1^m||0^m}{\overline{P}\varphi} = r^Tb$. 
Given commitment of $P$,  $c_1,\ldots,c_{s+\ell}$, the
commitment to $\overline{P}\varphi$ can be computed as
$\mathsf{cm}=\prod_{k=1}^{s+\ell}(c_k)^{\varphi_k}$. %Using an inner-product argument
%the prover can show that the commitment $\mathsf{cm}$ opens to vector $z$ such
%that $\innp{1^m||0^m}{z}=r^Tb$. Binding property of the commitment ensures that
%$z=\overline{P}\varphi$ with overwhelming probability.
 In the full protocol, 
the prover initially commits to a random $P_0\in \FF^{2m}$ subject to $\innp{1^m}{P_0}=0$ and 
uses $\beta P_0 + \overline{P}\varphi$ as witness in the inner product protocol. Here 
$\beta\sample \FF\backslash \{0\}$ is randomly chosen by the verifier. This randomization
precludes the need for the inner-product argument to be zero knowledge, and as we will later
see, helps reduce interaction among provers in its distributed variant.\smallskip 
\end{comment}


\noindent{\em Consistency of oracle $\pi$ and $P$}: The verifier additionally
needs to determine if the committed $P$ and the oracle $\pi$ are consistent or not. The verifier proceeds to check the
consistency at randomly sampled $t$ positions given by $\{(j_u,k_u): u\in [t]\}$ from
$[h]\times [n]$ for a $t=O(\secpar)$. It queries the oracle for the columns $\pi[\cdot,k_u]$ and the prover for vectors $\ewit[\cdot,j_u,k_u]$ for
$u\in [t]$. For $u\in [t]$, let $f_u$ denote the unit vector in $\FF^h$ with $1$ in the position
$j_u$. The prover and the verifier run inner-product arguments to establish the following:
\begin{enumerate}[{\rm 1.}]
\item $\innp{f_u}{P[\cdot,k_u]}=\sum_{i\in [p]}R^i(j_u,k_u)\ewit[i,j_u,k_u]$ for $u\in
[t]$. It is readily checked that for $P$ computed as per the protocol, 
and honest vectors $\ewit[\cdot , j_u,k_u]$, the identity holds. The inner-product
can be checked as in Section ~\ref{sec:matrixcommitment}.

\item $\innp{f_u}{\ewit[i,\cdot,k_u]}=\ewit[i , j_u,k_u]$ for all $u\in [t],i\in
[p]$. %Here $W_{iu}$ denotes the vector $\ewit[i,\cdot,k_u]$. 
Again, 
the inner-products can be verified as in Section ~\ref{sec:matrixcommitment}.
Moreover, the checks for each
$u\in [t]$ can be aggregated, leading to one inner-product check for each $u\in
[t]$.
\end{enumerate}

\noindent{\em Proximity Check for Oracle}: This check forces a prover to commit
to an encoding which is ``close'' to well-formed encoding $\mc{W}$. To check proximity,  %(before sending messages $r,Q$)
the verifier initially  sends a vector $\rho\sample
\FF^p$ and asks the prover to send commitments
$(\tilde{c}_1,\ldots,\tilde{c}_\ell)$ to $\tilde{U}=\sum_{i\in
[p]}\rho_i\ewit[i,\cdot,\cdot]$. It then checks for all $u\in [t]$ that $\prod_{a=1}^\ell(\tilde{c}_a)^{\Lambda^T_{n,\ell}[a,k_u]}=\prod_{i=1}^p(\pi[i,k_u])^{\rho_i}$. It can be
seen that for an honest computation, both the commitments open to the vector
$\sum_{i\in [p]}\rho_iV_{iu}$ where $V_{iu}=(\ewit[i,1,k_u],\ldots,\ewit[i,m,k_u])$.
%The complete linear check protocol is described in Figure \ref{fig:linearcheck}.
%the protocol $\agginnerproduct$ denotes the protocol 
%for veryfying inner products of several commited vectors with a common vector. 
%The completeness of the linear check protocol can be easily verified. We sketch the proof for its
%soundness, leaving the detailed proof to the Appendix.



\commentA{change notation in the protocol and proof}

\begin{lemma}[Soundness]\label{lem:linercheck_sound}
For all polynomially bounded provers $P^\ast$ and all $\pi\in \GG^{p\times n}$,
$A\in \FF^{M\times N}, b\in \FF^M$, there exists an expected polynomial time
extractor $\extr$ with rewinding access to transcript oracle $\mc{O}=\langle
P^\ast(\cdot),\verifier^\pi(\cdot)\rangle$ such that $\extr$ either breaks the 
commitment binding or outputs a witness with overwhelming probability whenever 
$P^\ast$ succeeds, i.e,
{\small
\begin{align*}
\condprob{\begin{array}{c}
\ewit=\open(\pi)\wedge \\
A\wit=b
\end{array}
}{
\begin{array}{c}
\sigma\gets \gen(\secparam) \\
\ewit\gets \extr^{\mc{O}}(\bm{x},\sigma) \\
\wit\gets \dec(\ewit)
\end{array}}\geq
\epsilon(P^\ast)-\kappa_{lc}(\secpar)
\end{align*}
}
where $\epsilon(P^\ast):= \condprob{\langle P^\ast(\bm{x},\sigma),\verifier^\pi(\bm{x},\sigma)\rangle=1}{\sigma\gets \gen(\secparam)}$ denotes the success probability of $P^\ast$, $\kappa_{lc}$ denotes a negligible function, and $\bm{x}$ denotes the tuple $(A,b,M,N)$.
\end{lemma}
\begin{proof}[Proof-Sketch]
Suppose the oracle $\pi$ commits to $\ewit$. In a detailed proof, this $\ewit$
would be ``extracted'' by the appropriate extractor. Note that $\ewit\in
\mc{W}_2$ as a commitment implicitly corresponds to such a matrix. Let $e<(n-\ell)/3$ be a parameter. First we show that 
an adversarial prover succeeds with negligible probability if $d(\ewit,\mc{W}_1)>e$. Second, we
show that for $d(\ewit,\mc{W}_1)\leq e$, the prover succeeds with negligible probability when
$A\wit\neq b$ where $\wit=\dec(\ewit)$. Consider the case when $d(\ewit,\mc{W}_1)>e$. Then for
$\tilde{U}=\sum_{i\in [p]}\rho_i\ewit[i,\cdot,\cdot]$, by Proposition ~\ref{lem:3dcompression},
 we have $d(\tilde{U},\mc{C}_1)>e$ with probability $1-o(1)$. Let $\tilde{\bm{c}}=(\tilde{c}_1,
\ldots,\tilde{c}_\ell)$ be the commitments to $\tilde{U}$ sent by the prover
(Step 3 in Figure ~\ref{fig:linearcheck}). Define 
the vector $\tilde{\bm{C}}=(\tilde{C}_1,\ldots,\tilde{C}_n)$ where
$\tilde{C}_k=\prod_{a=1}^\ell (\tilde{c}_a)^{\Lambda_{n,\ell}^T[a,k]}$ for $k\in [n]$.
 Let $\hat{\bm{C}}=(\hat{C}_1,\ldots,\hat{C}_n)$ where
$\hat{C}_k=\prod_{i=1}^p(\pi[i,k])^{\rho_i}$. Now if
$\Delta(\tilde{\bm{C}},\hat{\bm{C}})>e$, we see that the prover succeeds in the
proximity check (Step 14) with probability at most $(1-e/n)^t$. In the complete proof in
Full Version, we show that when $\Delta(\tilde{\bm{C}},\hat{\bm{C}})\leq e$, 
while $d(\tilde{U},\mc{C}_1)>e$, the prover knows two openings to a commitment.
Thus an adversarial prover succeeds with probability at most $(1-e/n)^t$ when
$d(\ewit,\mc{W}_1)>e$.

We now consider the case when $d(\ewit,\mc{W}_1)\leq e$. From Lemma
~\ref{lem:bicdecoding}, there exists (unique) $\ewit^*\in \mc{W}$
such that $\Delta_1(\ewit,\ewit^*)\leq e$.
Let $\wit=\dec(\ewit)=\dec(\ewit^*)$. We consider the prover's success
probability when $A\wit\neq b$, and thus with overwhelming probability $r^TA \wit \neq
r^Tb$. Let $P^*$ denote the correctly computed $P$ matrix from $\ewit^*$ and let
$\hat{P}$ denote the correctly computed $P$ matrix from $\ewit$. We note that
$\Delta_1(\hat{P},P^*)\leq e$. If the 
prover commits to a matrix $\tilde{P}=P^*$, it fails the inner product check in Step
13(b) as $r^TA\wit\neq r^Tb$. If it commits to a matrix $\tilde{P}\neq P^*$, we have
$\Delta_1(\tilde{P},P^*)\geq n-s-\ell$ by distance property of the code $L_{n, s+\ell}$
(We note that a prover implicitly commits to a matrix in $L_{n,s+\ell}\otimes L_{h,2m}$).
Thus there exists a set $E$ of at least $n-s-\ell-e$ columns, such that for
$k\in E$, $\hat{P}[\cdot,k]=P^*[\cdot,k]\neq \tilde{P}[\cdot,k]$. The check
 13(c) ensures that prover provides vectors $X_u=\ewit[\cdot,j_u,k_u]$ with
overwhelming probability. Then the consistency check succeeds for the 
uniformly sampled query point $(j_u,k_u)$ when:
\[ \tilde{P}[j_u,k_u] = \sum_{i\in [p]}R^i(\alpha_{j_u},\eta_{k_u})X_u[i] =
\hat{P}[j_u,k_u] \]
For $k_u\in E$, the above holds when the codewords $P^*[\cdot,k_u]$ and
$\tilde{P}[\cdot,k_u]$ agree in the position $j_u$, which happens with 
probability $2m/h$. Thus probability $\prob{\mc{E}_u}$ that above query succeeds is bounded by:
{\small
\begin{align*}
\prob{\mc{E}_u}&\leq \frac{s+\ell+e}{n} + \frac{n-s-\ell-e}{n}\cdot\frac{2m}{h}
\\
&= \frac{2m}{h}+\left(1-\frac{2m}{h}\right)\left(\frac{s+\ell+e}{n}\right)
\end{align*}
}
The above probability is smaller than a constant $\epsilon<1$ for suitable
choices of parameters. Hence, the overall probability of prover's success is
$\negl(\secpar)$ for $t=O(\secpar)$.
\end{proof}
%<<<<<<<HEAD
\begin{comment}
\subsection{Quadratic Check Protocol}
We now describe the IPCP which allows a prover to prove knowledge of vectors
$\wit_x$, $\wit_y$ and $\wit_z$ in $\FF^N$, satisfying $\wit_x\circ \wit_y =
\wit_z$. Once again, the protocol requires the prover to construct encodings
$\ewit_x=\enc(\wit_x)$, $\ewit_y=\enc(\wit_y)$ and $\ewit_z=\enc(\wit_z)$ as
described in Section \ref{sec:witencoding}. Thereafter, the prover uses
commitment scheme $\comm$ to commit to these encodings as $\comoracle_x =
\comm(\ewit_x)$, $\comoracle_y = \comm(\ewit_y)$ and $\comoracle_z = \comm(\ewit_z)$. 
The prover forms the oracle $\pi\in \GG^{3p\times n}$ by vertically stacking the
$p\times n$ matrices $\comoracle_x,\comoracle_y$ and $\comoracle_z$. As before,
for a query $Q\subseteq [n]$, the oracle answers with columns $\pi[\cdot,k]$ for
$k\in Q$. The columns returned by the oracle can be parsed into constituent columns 
$\comoracle_x[\cdot,k]$, $\comoracle_y[\cdot,k]$ and $\comoracle_z[\cdot,k]$
canonically. We again discuss the key ingredients of the protocol.

\noindent{\em Probabilistic Reduction}: Let $Q^i_x,Q^i_y$ and $Q^i_z, i\in [p]$ be the
polynomials interpolating the $i^{th}$ slices of $\wit_x$, $\wit_y$ and $\wit_z$
 as in Section \ref{sec:witencoding}. Then for vectors $\wit_x,\wit_y,\wit_z$ satisfying
$\wit_x\circ \wit_y=\wit_z$, the polynomials $Q^i=Q^i_x\cdot Q^i_y - Q^z_i$ interpolate
$\bm{0}^{m\times s}$ on the set $\{(\alpha_j,\zeta_k):j\in [m],k\in [s]\}$ for all $i\in [p]$. This can be probabilistically
checked by checking that the polynomial $F := \sum_{i\in [p]}r_iQ^i$ interpolates
$\bm{0}^{m\times s}$ on the above set for randomly sampled $r\in \FF^p$. Once again, we
ask the prover to ``commit'' to $F$ using a tamper resistant structure, like a codeword,
which enables the verifier to check the aforementioned condition, as well as to
ensure that the commitment is consistent with oracle replies and prior
messages.

\noindent{\em Reduction to Inner Products}: The prover computes 
$h\times n$ matrix $P$ given by $P[j,k]=F(\alpha_j,\eta_k)$. It commits to $P$
using commitments $(c_1,\ldots,c_{2\ell})$ to the first $2\ell$ columns of $P$.
Note that each row of $P$ commits to univariate component polynomials
$F(\alpha_j,\cdot)$ of $F$ via their evaluations of $\bm{\eta}$. To check that
$F$ interpolates $\bm{0}^{m\times s}$ on the points
$\{(\alpha_j,\zeta_k)\}_{j\in [m],k\in [s]}$, the verifier checks that
$p(\cdot) := \sum_{j\in [m]}\gamma_jF(\alpha_j,\cdot)$ interpolates $\bm{0}^s$ on
$\overline{\bm{\zeta}}$ for randomly sampled $\gamma=(\gamma_1,\ldots,\gamma_m)\in \FF^m$.
Again, the verifier checks $p(\overline{\bm{\zeta}})=\bm{0}^s$ via the inner product
check $\innp{\tau}{p(\overline{\bm{\zeta}})}=0$ for a random $\tau\in \FF^s$. As in the
linear check protocol, using $p(\overline{\bm{\zeta}})=\Phi p(\overline{\bm{\eta}})$, we
get the following inner product check
$\innp{(\gamma,0^{h-m})}{\overline{P}\varphi}=0$ where $\varphi=\Phi^T\tau$. 
The commitment to the vector $\overline{P}\varphi$ can be homomorphically computed
from $c_1,\ldots,c_{2\ell}$.

\noindent{\em Checking consistency with Oracle}: As in the linear check, the
verifier uniformly and independently samples $(j_u,k_u)\in [h]\times [n]$ for
$u\in [t]$, and queries the oracle $\pi$ for columns $\pi[\cdot,k_u]$. Let
$\pi_x[\cdot,k_u]$, $\pi_y[\cdot,k_u]$ and $\pi_z[\cdot,k_u]$ denote the parse
of $\pi[\cdot,k_u]$ into commitments corresponding to $\ewit_x,\ewit_y$ and
$\ewit_z$ respectively. Further, the verifier asks prover for vectors
$\ewit_x[\cdot,j_u,k_u]$, $\ewit_y[\cdot,j_u,k_u]$ and $\ewit_z[\cdot,j_u,k_u]$
for $u\in [t]$. The verifier then checks the following:
\begin{enumerate}[{\rm (i)}]
\item For all $u\in [t]$: $P[j_u,k_u]=\sum_{i\in
[p]}r_i(\ewit_x[i,j_u,k_u]\cdot\ewit_y[i,j_u,k_u]-\ewit_z[i,j_u,k_u])$.
\item Checks that vectors $\ewit_x[\cdot,j_u,k_u]$ are consistent with
commitments $\pi_x[\cdot,k_u]$ as in linear check protocol. Similar checks are
made for $\ewit_y[\cdot,j_u,k_u]$ and $\ewit_z[\cdot,j_u,k_u]$.
\end{enumerate}
We present the full protocol in Figure \ref{fig:quadcheck}. The completeness of
the protocol can again be verified by direct calculation. We state the soundness
of the protocol below:
\end{comment}
%=======
%>>>>>>> a570c22d49f6a2b6c53bd2506676e2790bfe3c01
\input{quadratic.tex}

\begin{lemma}[Soundness]\label{lem:quadcheck_sound}
For all polynomially bounded provers $P^\ast$ and all $\pi\in \GG^{3p\times n}$,
there exists an expected polynomial time extractor $\extr$ with rewinding access
to the transcript oracle $\mc{O}=\innp{P^\ast(\cdot)}{\verifier^{\pi}(\cdot)}$
such that either $\extr$ breaks the commitment binding, or it outputs a witness
with overwhelming probability whenever $P^\ast$ succeeds, i.e,
{\small
\begin{align*}
\condprob{
\begin{array}{c}
{[}\ewit_x||\ewit_y||\ewit_z{]}=\open(\pi)\wedge \\
\wit_z=\wit_x\circ\wit_y
\end{array}
}{
\begin{array}{c}
\sigma %\sample \gen(\secparam) \\
\leftarrow \gen(\secparam) \\
{[}\ewit_x||\ewit_y||\ewit_z{]}%\sample \extr^{\mc{O}}(\sigma)\\
\leftarrow \extr^{\mc{O}}(\sigma) \\ 
\wit_a=\dec(\ewit_a), a\in \{x,y,z\}
\end{array}
}\\
\geq \epsilon(P^\ast) - \kappa_{\rm qd}(\secpar)
\end{align*}
}
for some negligible function $\kappa_{qd}$. In the above, $\epsilon(P^\ast)$
denotes the success probability of the prover $P^\ast$ as before.
\end{lemma}
\begin{proof}
The proof is similar to the proof of the linear check protocol. Using similar
arguments, one can show that the above Lemma holds with:
{\small
\begin{equation*}
\kappa_{qd}(\secpar) := \left(1-\frac{e}{n}\right)^t +
\left(\frac{2m}{h}+\left(1-\frac{2m}{h}\right)\left(\frac{2\ell+e}{n}\right)\right)^t
+ \frac{O(|C|)}{|\FF|}
\end{equation*}
}
\end{proof}

\subsection{Zero Knowledge}
%\pnote{Change the simulation according to the new protocol}
We now prove protocols $\linearcheck$ and $\quadcheck$ to be honest verifier
zero knowledge by designing simulators for them. We will extend the verifier's
view with commitment openings (committed vector, randomness) for the inner
product protocols. This has two benefits: (i) the inner product argument need
not be zero knowledge, and (ii) in the distributed setting, the openings can be
shared with the aggregator which can complete the interaction with the verifer
without further involvement of the provers. The verifier's veiw for
the $\linearcheck$ protocol consists of:
\begin{itemize}[\leftmargin=0pt]
\item {\em Verifier Randomness}: The verifier's random messages are $\rho, r, Q = \{(j_u,k_u)\}_{u\in [t]}, \beta, \delta$.
%Vector $\rho \in \FF^p$ for checking proximity, vector $r\in \FF^M$ as part of the reduction $r^TA\wit=r^Tb$, query position $Q= \{(j_u,k_u)\}_{u\in [t]}$ ,$\beta\in \FF^\ast$ used in randomizing the vector $\overline{P}\varphi$ and $\delta \in \FF^p$ to aggregate inner product checks.
%$\tau,\delta$ as part of $\proximityTwoD$ subprotocol in Step 10, $\rho\sample \FF^p$ as the random vector for compressing in subprotocol $\proximityThreeD$ in Step 15, $\tilde{\tau},\tilde{\delta}$ for the second invocation of $\proximityTwoD$ from within $\proximityThreeD$. We do not include the query positions as part of the subprotocol $\proximityThreeD$ as we assume that the same query positions sampled in Step 5 are used there. Summarizing, the verifier randomness consists of $\rho, r, Q, \beta, \delta$. %$\tau,\delta,\rho,\tilde{\tau},\tilde{\delta}$.

\item {\em Commitments}: The prover's commitment messages are $c_0, c_1, \ldots, c_{s+\ell}$ and $\tilde{c}_1, \ldots, \tilde{c}_{\ell}$. And oracle responses are $\pi[\cdot,k_u]$, $k_u$ corresponding to $Q$. In the extended view we will consider the whole of $\pi$.
%Commitment $c_0$ to the first $2m$ entries of the random codeword $P_0$ sampled in Step 6, commitments $c_1,\ldots,c_{s+\ell}$ to commit to matrix $P$, commitments $\tilde{c}_1,\ldots,\tilde{c}_{\ell}$ to the matrix $\tilde{U}$ used for proximity check. Additionally, the view contains commitments $\pi[\cdot,k_u]$ for $u\in [t]$ as part of oracle query response. Summarizing, the view consists of commitments $c_0,c_1,\ldots,c_{s+\ell}$, $\tilde{c}_1, \ldots, \tilde{c}_{\ell} , \pi$, as a part of the extended view of $\verifier$.%$\{\pi[\cdot,k_u]\}_{u\in [t]}$.

\item {\em Commitment Randomness}: We include randomness used to compute certain
commitments as part of the extended view. We include openings $w_u$, $u\in [t]$
corresponding to commitments $\mathsf{cm}_u$ used in the inner product checks in
Step 13(a), the randomness $w$ for the commitment $\mathsf{cm}$ used in inner
product check in Step 13(b) and the randomness $O[\cdot,k_u]$ for $u\in [t]$
used while computing the oracle. Note that the commitment randomness for inner
product checks in Step 13(c) can be derived from $O[\cdot,k_u]$ and hence is
omitted from the view.
 %$\nu=\nu_0+\sum_{a\in [s+\ell]}\mu_a\omega_a$ for the vector $z$ in the subprotocol $\proximityTwoD$ in Step 10 (here $\mu=\mc{T}\tau$), $\omega=\beta\omega_0+\sum_{a\in [s+\ell]}\varphi_ac_a$ for inner product in Step 11, $\chi_u=\sum_{a\in [s+\ell]}T[a,k_u]c_a$ for $u\in [t]$ for the inner products in Step 13, $\{O[\cdot,k_u]\}_{u\in [t]}$ for aggregate inner product arguments in Step 14, $\tilde{\nu}=\tilde{\nu}_0+\sum_{a\in [\ell]}\tilde{\mu}_a\tilde{c}_a$ for the vector $\tilde{z}$ in the $\proximityTwoD$ protocol called as part of $\proximityThreeD$ protocol in Step 15. Summarizing, the view includes $\nu,\omega,\{\chi_u\}_{u\in [t]},\{O[\cdot,k_u]\}_{u\in [t]},\tilde{\nu}$.

\item {\em Vectors}: The view includes vectors
$X_u=\ewit[\cdot,j_u,k_u]$ for $u\in [t]$, the vector $z = \beta P_0 +
\overline{P} \varphi$, witness of step 13(b) and vectors $P[\cdot, k_u]$ for
$u\in [t]$.
We drop $\{X_u\}_{u\in [t]}$ and $\{P[\cdot,k_u]\}_{u\in [t]}$ from the view as these can be derived from $\ewit[\cdot,\cdot,k_u]$ and $r$. Thus, the vectors in the view consist of: $z,\{\ewit[\cdot,\cdot,k_u]\}_{u\in [t]}$.
\end{itemize}
Next, we describe a simulator that outputs a view indistinguishable from the
above view.

\noindent{\bf Simulator}: 
%\begin{itemize}
 $\Sim$ picks $\{\rho, r, Q, \delta, \beta \}$ uniformly at random from their respective domains. Then picks $\ewit[\cdot, \cdot, k_u]$ uniformly such that each plane has columns as codewords in $L_2$ for all $u\in [t]$, and picks $z$ unifromly from $\FF^{2m}$ satisfying $\sum_{j\in[m]} z[j] = r^Tb$. After that, $\Sim$ computes $c_0 \leftarrow \comm(P_0,w_0)$ and $\cm \leftarrow \comm(z,w)$ where $P_0\sample \FF^{2m}$ such that $\innp{1^m}{P_0}=0$ and $w_0,w\sample \FF$. $\Sim$ picks $O\sample \FF^{p\times t}$, and computes $\tilde{U}[\cdot, k_u] = \sum_{i\in[p]} \rho_i U[i,\cdot, k_u]$, $\tilde{O}[k_u]= \sum_{i\in[p]} \rho_i O[i,u]$, and $\tilde{c}_{k_u}  \leftarrow \comm(\tilde{U}'[\cdot, k_u] , \tilde{O}[k_u] ) \, \forall u\in[t]$ and $\pi[i,k_u] = \comm(U'[i,\cdot,k_u], O[i,u])$, where $U'[i,\cdot,k_u]$ consists of first $m$ entries of $U[i,\cdot,k_u]$ and $\tilde{U}'[\cdot,k_u]$ consists of first $m$ entries of $\tilde{U}[\cdot,k_u]$, where $i\in[p]$ and $u\in [t]$. Next, $\Sim$ picks $\tilde{c}_1, \ldots, \tilde{c}_{\ell}$ in such a way that $\tilde{c}_{k_u} = \prod_{a\in[\ell]} (\tilde{c}_a)^{\Lambda^T_{n,\ell}[a,k_u]} \,
\forall u\in [t]$. Picking such $\tilde{c}$ is efficient since the number of unknowns is more than the number of constraints, and the coefficient matrix has full row rank. Now $\Sim$ picks $\pi[\cdot, k]$ such that $c_k = \prod_{i\in[p]}( \pi[i,k])^{\rho_i}$ for all $k=k_u:u\notin [t]$. $\Sim$ picks $w_{k_u}$ uniformly at random for all $u\in [t]$ and computes $c_{k_u} \leftarrow \comm(P'[\cdot,k_u], w_{k_u})$, where $P'[\cdot,k_u]$ is the first $2m$ entries of $P[\cdot,k_u] \, \forall u\in[t]$, and picks ${c}_1, \ldots, {c}_{s+\ell}$ in such a way that ${c}_{k_u} = \prod_{a\in[s+\ell]} ({c}_a)^{\Lambda^T_{n,s+\ell}[a,k_u]} \, \forall u\in [t]$
and $\cm =  c_0^{\beta} \cdot \prod_{a\in[s+\ell]} (c_a)^{\varphi_a}$. Picking such ${c}$ is efficient since the number of unknowns is more than the number of constraints, and the coefficient matrix has full row rank.
%\end{itemize}
\begin{comment}
The simulator outputs $r$, $\{j_u,k_u\}_{u\in [t]}$,
$\beta$, $\tau,\delta$, $\rho$, $\tilde{\tau},\tilde{\delta}$ by uniformly and
independently sampling them from their respective domains, as in the honest
execution of the protocol. Simulator also outputs $z,\tilde{z}$ uniformly from
$L_2$, and $z'$ uniformly from $\dashL_2$ satisfying $\sum_{j\in [m]}z'[j]=0$ . It outputs
$\ewit[\cdot,\cdot,k_u]$ uniformly such that each plane has columns as codewords
in $L_2$. 
Next, the simulator outputs $\omega,\nu,\tilde{\nu}$
and $\chi_1,\ldots,\chi_t$, $\{O[\cdot,k_u]\}_{u\in [t]}$ choosing them randomly and
independently from $\FF$. Finally, the simulator outputs
$c_0,d_0,\ldots,c_{s+\ell}$ and $\tilde{d}_0,\tilde{c}_1,\ldots,\tilde{c}_\ell$ choosing 
them uniformly from $\GG$ subject to the following constraints:
$d_0 + \sum_{a=1}^{s+\ell}\mu_ac_a = \comm(z,\nu)$,
$c_0 + \sum_{a=1}^{s+\ell}\varphi_ac_a = \comm(z',\omega)$,
$\sum_{a=1}^{s+\ell}T[a,k_u]c_a = \comm(P[\cdot,k_u],\chi_u)$ for $u\in [t]$,
$\sum_{a=1}^{\ell}\mc{T}[a,k_u]\tilde{c}_a = \comm\big(\sum_{i\in
[p]}\tilde{U}[\cdot,k_u],\tilde{O}[\cdot, k_u]\big)$ for $u\in [t]$,
$\beta\tilde{d}_0 + \sum_{a=1}^{\ell}\tilde{\mu}_a\tilde{c}_a =
\comm(\tilde{z},\tilde{\nu})$. 
\end{comment}
\begin{lemma}\label{lem:simlincheck}
The output of the above simulator is perfectly indistinguishable
from the extended view of the verifier in honest execution of the protocol
$\linearcheck$ for $t\leq \bi$.
\end{lemma}
We defer the proof of the correctness of the simulation to Appendix.
We omit the simulation for the quadratic check protocol, as it is very similar
to the linear check. 

\begin{lemma}\label{lem:simquadcheck}
There exists an efficient simulator $\simulator$ whose output is perfectly
indistinguishable from the extended view of the verifier in the honest execution
of the protocol $\quadcheck$ for $t\leq \bi$.
\end{lemma}


\section{$\dpname$: Distributed Prover Variant}\label{sec:dpgraphen}
We now describe distributed protocol to produce a $\name$ proof for a
statement, when the witness is shared between several provers. We assume that
there are $\Num$ provers $\prover_1,\ldots,\prover_{\Num}$. For $\xi\in [\Num]$,
let $\shr{\wit}$ denote the prover $\distprover$'s share of the witness $\wit$.
We assume that the sharing is additive, i.e, $\sum_{\xi\in [\Num]} \shr{\wit} =
\wit$. When denoting witnesses to certain protocols, we use the %$[[\cdot]]$
$\langle {\cdot} \rangle$ to denote that the witness is additively
shared among the provers, i.e, notation $\langle x \rangle$ in the witness list
of the protocol denotes that the provers have shares $\shr{x}$ of $x$. Recall
from Section ~\ref{sec:security model}, that there is an algorithm $\Ag$ 
which aggregates the messages received
from provers $\prover_1,\ldots,\prover_\Num$ and constructs the message to be
sent to the verifier $\verifier$. We assume one of the provers executes $\Ag$.
The verifier's  messages and messages used by the aggregator algorithm $\Ag$ 
are assumed to be available on an authenticated broadcast
channel. We specify the algorithm $\Ag$ implicity by describing the construction
of message to the verifier from the provers' messages for each round.
We first discuss a protocol which is secure when the provers are semi-honest, then 
briefly discuss how to ensure the privacy of the honest provers when the corrupt 
provers are malicious. 

\noindent{\bf Distributed Oracle Setup}: In distributed setting, each prover
$\distprover$ encodes his share $\shr{\wit}$ as $\shr{\ewit}=\enc(\shr{\wit})$
and computes the commitment $\shr{\comoracle}=\comm(\shr{\ewit})$. The provers
then share $\shr{\comoracle}$ with the aggregator $\Ag$ which sets the oracle
$\pi$ as $\pi := \combine(\shr{\comoracle})$.  

\begin{comment}
\noindent{\bf Distributed Proximity Test}: The provers jointly prove that 
the oracle is well formed as follows: On receiving the verifier’s challenge 
$r\in\FF^p$ on broadcast channel, the prover $\distprover$ locally computes
$\shr{\tilde{U}}=\sum_{i\in [p]}r_i\shr{\ewit}[i,\cdot,\cdot]$, commitments
$\shr{\tilde{c}_k}=\sum_{i\in [p]}\shr{\comoracle}[i,k]$ for $k\in [\ell]$. They
send the shares of the commitments to the aggregator, who computes
$(\tilde{c}_1,\ldots,\tilde{c}_\ell)=\combine(\shr{\tilde{c}_1},\ldots,\shr{\tilde{c}_\ell})$
and forwards these to the verifier. Next, the provers jointly prove that
$\tilde{c}_1,\ldots,\tilde{c}_\ell$ corresponds to a matrix $\overline{U}$ such
that $\overline{U}\mc{T}\in \mc{C}_2$. This is done via distributed variant of
membership protocol that we describe next.  

\noindent{\bf Distributed Membership Test}: 
This protocol reduces to each prover responding to verifier’s
challenge on their share, as in the single    prover setting. The prover
responses are aggregated to compute the response to    the verifier. The
complete protocol appears in Figure \ref{fig:distprox2d}. In Figure
\ref{fig:distprox2d}, we note that the aggregator obtains the witness to the
inner product protocol in step 7, and hence does not need to interact further
with the provers. 
\end{comment}

\begin{comment}
<<<<<<< HEAD
\noindent{\bf Distributed Linear Test}: We provide the complete distributed protocol in Figure \ref{fig:distlincheck}. Here we highlight key adaptations from the single prover variant. First we assume that the provers have a share of the vector $0^{2m}$ (as part of obtaining shares of the extended witness). In response to verifier’s challenge $r\in \FF^M$, each prover locally computes $R=r^TA$ and the associated polynomials $R^i$, $i\in [p]$. Since, the computation of the $P$ matrix is a linear operation, each prover obtains a share $\shr{P}$ of the $P$ matrix by locally computing on their share of the witness. The provers compute commitments to the columns of their share of the $P$ matrix, following the similar procedure as in the single prover linear check protocol, and send those to the aggregator. The aggregator combines the shares to obtain the commitment to the $P$ matrix. %Provers can jointly prove that $P$ matrix is a codeword in the product code using the distributed membership protocol as above. 
Similarly, the provers can send their shares of vectors $\shr{X_u}=\shr{\ewit}[\cdot,j_u,k_u]$ for $u\in [t]$, required for consitency checks. Due to the bounded independence property, for $t\leq \bi$, these shares do not leak. The only point of departure is the shares of $z=\beta P_0 + \overline{P}\varphi$. The randomization by a random codeword $P_0\in L_2$ such that $\sum_{j\in [m]}
P_0[j]=0$ was based on the distribution of the vector $\overline{P}\varphi$ in an honest execution of the protocol for a witness satisfying $r^TA\wit=r^Tb$. However, individual shares may not satisfy the preceeding condition, and hence the provers further add a share of $0^h$ to the share $\beta\shr{P}_0 + \shr{\overline{P}}\varphi$. This allows the aggregator $\Ag$ to obtain witnesses for all the inner product protocols that it runs with the verifier.  

\begin{figure}[t!]
	{\small
		%\centering
		\begin{framed}
			\noindent{$\distlinearcheck(\mathsf{pp}, \, A\in \mc{M}_{M,N}, \, b\in \FF^M, \, [\pi];\, \shr{\ewit}, \, \shr{0^{2m}})$}:
			%\pnote{why $\wit$ is not part of the witness of linear check protocol?}
			
			\noindent{\bf Relation}: $\ewit=\open(\pi)\wedge A\wit=b$ for $\shr{\wit}=\dec(\shr{\ewit})$ for all $\xi \in [\Num]$ and $\sum_{\xi\in[\Num]} \shr{\wit} = \wit$.
			
			\begin{enumerate}[{\rm 1.}]
				\item $\verifier\rightarrow\distprover$: $\rho\sample \FF^p$.
				\item $\distprover$ computes: $\shr{\tilde{U}} = \sum_{i\in [p]}\rho_i\shr{\ewit}[i,\cdot,\cdot]$, 
				commitments $\shr{\tilde{c}_1},\ldots,\shr{\tilde{c}_\ell}$ as in Section ~\ref{sec:matrixcommitment}.
				\item $\distprover\rightarrow\Ag$: $\shr{\tilde{\bm{c}}} = (\shr{\tilde{c}_1},\ldots,\shr{\tilde{c}_\ell})$.
				\item \textcolor{red}{$\Ag\rightarrow\verifier$: $\tilde{\bm{c}}=\combine(\shr{\tilde{\bm{c}}})$.} %(\tilde{c}_1,\ldots,\tilde{c}_\ell)$.
				\item $\verifier\rightarrow\distprover$: $r\sample \FF^M$.
				\item $\distprover\leftrightarrow\verifier$ compute: Polynomials $R^i$, $i\in [p]$ interpolating $R=r^TA$
				as in Section ~\ref{sec:lincheck}. 
				\item $\distprover$ computes: Matrix $\shr{P}$ from $R$ and $\shr{\ewit}$ as described in Section ~\ref{sec:lincheck}. Samples $\shr{P_0}\sample \FF^m$, $\shr{\omega_0} \sample \FF$ and $\shr{c_0}\gets \comm(\shr{P_0},\shr{\omega_0})$,  and $\shr{d_0}\gets \comm(\shr{0^2m},\shr{o})$ where $\shr{o} \sample \FF$.
				Computes commitments $\shr{c_1},\ldots,\shr{c_{s+\ell}}$ from $\shr{P}$.
				\item $\distprover\rightarrow\Ag$: $\shr{c_0} ,\shr{c_1} ,\ldots, \shr{c_{s+\ell}}, d_0$.
				\item \textcolor{red}{$\Ag\rightarrow\verifier$: $c_k = \combine(\shr{c_k}) \, \forall k\in [s+\ell]$ and sends $c_0,c_1,\ldots,c_{s+\ell}$.}
				\item $\verifier\rightarrow\distprover$: $Q=\{(j_u,k_u):u\in [t]\}$ for $(j_u,k_u)\sample [h]\times [n]$ for $u\in [t]$.
				\item $\verifier\rightarrow\pi$: $\{k_u:u\in [t]\}$.
				\item $\distprover\rightarrow\Ag$: $\shr{X_u}=\shr{\ewit}[\cdot,j_u,k_u],  \shr{P_u} = \shr{P}[\cdot,k_u]$ for $u\in [t]$.
				\item \textcolor{red}{$\Ag\rightarrow\verifier$: $X_u = \sum_{\xi\in[\Num]} \shr{X_u}, P_u= \sum_{\xi\in [\Num]} \shr{P_u}$ and sends ${X_u}$ for $u\in [t]$.}
				\item $\pi\rightarrow\verifier$: $\pi[\cdot,k_u]$ for $u\in [t]$.
				\item $\verifier\rightarrow\distprover$: $\delta\sample \FF^p$, $\beta\sample \FF\backslash \{0\}$. 
				\item $\distprover\rightarrow\Ag$: $\shr{z} = \beta\shr{P_0} + \shr{\overline{P}}\varphi + \shr{0^{2m}}$ and sends $\shr{z}$.
				\item \textcolor{red}{$\Ag$ computes $z=\sum_{\xi\in[\Num]} \shr{z}$}.
				\item $\Ag$ and $\verifier$ run inner product arguments to check:
				\begin{enumerate}
					\item $\innerproduct(\mathsf{pp},f_u^T\Lambda_5,\mathsf{cm}_u,v_u;\overline{P}[\cdot,k_u])$ 
					\pnote{We can use $P[1:2m, k_u]$ to denote the first 2m entries of $P[\cdot,k_u]$ as $\overline{P}$ is the submatrix after cropping both the directions, $h$ and $n$.}
					for $u\in [t]$ where $\mathsf{cm}_u=\sum_{a=1}^{s+\ell}\Lambda_3[a,k_u]c_a$, 
					$v_u=\sum_{i=1}^pR^i(\alpha_{j_u},\eta_{k_u})X_u[i]$ (check consistency of $P$ with $\pi$).
					\item $\innerproduct(\mathsf{pp},1^m,\mathsf{cm},r^Tb;z)$ where $z=\beta P_0+\overline{P}\varphi$ and $\mathsf{cm}=\beta c_0+\sum_{a=1}^{s+\ell}\varphi_ac_a$ (check the condition $r^TAw=r^Tb$).
					\item $\innerproduct(\mathsf{pp},f_u^T\Lambda_2,C_u,\innp{\delta}{X_u})$ for $u\in [t]$ 
					where $C_u=\sum_{i=1}^p\delta_i\pi[i,k_u]$ (consistency of $X_u$ with $\pi$). 
				\end{enumerate}
				\item $\verifier$ checks: $\sum_{a=1}^\ell\Lambda_1[a,k_u]\tilde{c}_a=\sum_{i=1}^p\rho_i\pi[i,k_u]$ for $u\in [t]$ (check proximity of $\ewit$ to $\mc{W}_1$).
			\end{enumerate}
		\end{framed}
		\caption{Distributed Linear Check Protocol}
		\label{fig:distlincheck}
	}
\end{figure}
%=======
\end{comment}

\noindent{\bf Distributed Linear Check}: The messages sent by the prover to the
verifier in the linear check protocol include:
\begin{itemize}
\item Commitments $\tilde{c}_1,\ldots,\tilde{c}_\ell$ to the matrix $\tilde{U}=
\sum_{i\in [p]}\rho_i\ewit[i,\cdot,\cdot]$ for verifier's challenge $\rho\sample
\FF^p$.
\item Commitments $c_0,\ldots,c_{s+\ell}$ where $c_0$ is a commitment to random
vector $P_0\in \FF^{2m}$ satisfying $\innp{1^m}{P_0}=0$ and
$c_1,\ldots,c_{s+\ell}$ are commitments to the $h\times n$ matrix $P$.
\item The vectors $X_u=\ewit[\cdot,j_u,k_u]$ for $u\in [t]$, for verifier's
query $Q=\{(j_u,k_u):u\in [t]\}$.
\end{itemize}
We see that given verifier's challenges, each of the messages is linear function
of the encoding (which itself is a linear function of the witness). Hence, the
provers compute the respective messages on their shares, which can be trivially
combined by $\Ag$. In addition to above messages, we also want $\Ag$ to receive
witnesses to the inner product protocols namely, the vectors
$\overline{P}[\cdot,k_u]$, $W_u=\sum_{i\in [p]}\delta_i\ewit[i,\cdot,k_u]$ for
$u\in [t]$, $z=\beta P_0+\overline{P}\varphi$ and the randomness used to commit
the vectors. Each of these can again be obtained by combining the respective shares.
Note that the share $\shr{z}$ leaks $r^TA\shr{\wit}=\innp{1^m||0^m}{\shr{z}}$, which
is non-trivial knowledge about an individual witness share. Thus provers use a
random share $\shr{0^{2m}}$ to randomize their share of $z$, and send
$\shr{z}=\beta\shr{P_0}+\shr{\overline{P}}\varphi+\shr{0^{2m}}$. 
We provide the complete distributed linear protocol in Figure
\ref{fig:distlincheck} in Appendix.  

%Here we highlight key adaptations
%from the single prover variant. First we assume that the provers have a share of
%the vector $0^{2m}$ (as part of obtaining shares of the extended
%witness). In response
%to verifier’s challenge $r\in \FF^M$, each prover locally computes $R=r^TA$ and
%the associated polynomials $R^i$, $i\in [p]$. Since, the computation of the $P$
%matrix is a linear operation, each prover obtains a share $\shr{P}$ of the $P$
%matrix by locally computing on their share of the witness. The provers compute
%commitments to the columns of their share of the $P$ matrix, following the
%similar procedure as in the single prover linear check protocol, and broadcast
%their commitments. The aggregator combines the commitments to obtain the commitment
%to the $P$ matrix. %Provers can jointly prove that $P$ matrix is a codeword in
%Similarly,
%the provers can send their shares of vectors
%%$\shr{X_u}=\shr{\ewit}[\cdot,j_u,k_u]$ for $u\in [t]$, required for consitency
%checks. Due to the bounded independence property, for $t\leq \bi$, these shares
%do not leak. The only point of departure is the shares of $z=\beta P_0 +
%\overline{P}\varphi$. The randomization by a random codeword $P_0\in L_2$ such
%that $\sum_{j\in [m]}P_0[j]=0$ was based on the distribution of the vector
%$\overline{P}\varphi$ in an honest execution of the protocol for a witness
%satisfying $r^TA\wit=r^Tb$. However, individual shares may not satisfy the
%preceeding condition, and hence the provers further add a share of $0^{2m}$ to the
%share $\beta\shr{P}_0 + \shr{\overline{P}}\varphi$. This allows the aggregator
%algorithm
%$\Ag$ to obtain witnesses for all the inner product protocols that it runs with
%the verifier.  
%>>>>>>> bee3adb42b709af2d55598b10d73189cef6372ba

\noindent{\bf Distributed Quadratic Check}: This is the only protocol whose distributed variant requires an additional interaction among the provers. Recall that in response to the verifier’s challenge $r\in \FF^p$, the provers need to compute the matrix $P$ given by:
\begin{align*}
P[j,k] & =\sum_{i\in
[p]}r_i\big(Q_x^i(\alpha_j,\eta_k).Q_y^i(\alpha_j,\eta_k)-Q_z^i(\alpha_j,\eta_k)\big)\\    
& = \sum_{i\in [p]}r_i(\ewit_x[i,j,k].\ewit_y[i,j,k] - \ewit_z[i,j,k])
\end{align*}    
In the above, the provers have additive shares of $\ewit_x$ and
$\ewit_y$, and so they peformn an MPC with $O(N)$ multiplication gates and
depth $1$, to obtain additive shares of $\ewit_x[i,j,k].\ewit_y[i,j,k]$.
Concretely, we assume an MPC $\mathsf{Mult}$ with following input/output for
a prover $\distprover$:     
%\begin{align*}    
$\shr{\ewit_x.\ewit_y}\leftarrow \mathsf{Mult}(\shr{\ewit_x},\shr{\ewit_y})$    
%\end{align*}    
Thereafter, each prover obtains a share of matrix $P$, and the remaining protocol proceeds as
the distributed linear check protocol. Let $\wit_{sh}$ denote the subvector of
$\wit$ whose value depends on inputs of more than one prover. Let $S\subseteq
[p]$ denote the slices which contain elements of $\wit_{sh}$. We note that in
this case the multiplication MPC is only required for slices in $S$. This
optimization is particularly useful for $|S|\ll p$. 
The complete protocol for
distributed quadratic check appears in Figure \ref{fig:distquadcheck} in
Appendix.
\begin{comment}
\begin{figure}[h!]
\centering
\begin{framed}
\begin{itemize}
\item $\distproxTwoD(\FF,\GG,\ell,L_1,L_2,\bm{c};[[\overline{U}]],[[\bm{\omega}]])$:
\item {\bf Relation}: $(\overline{U},\bm{\omega})=\open(\bm{c})$ and
$\overline{U}\mc{T}\in \mc{C}_2$.
\begin{enumerate}[{\rm 1.}]
\item $\distprover\rightarrow \Ag$: Samples random codeword $u^\xi_0\in L_2$ and
computes $d^\xi_0=\comm(u^\xi_0,\nu^\xi_0)$ for randomly sampled $\nu^\xi_0$.
Sends $d^\xi_0$ to $\Ag$.
\item {\color{red} $\Ag\rightarrow \verifier$: $\Ag$ computes $d_0=\sum_{\xi\in
K}d^\xi_0$ and sends $d_0$ to $\verifier$ }.
\item $\verifier\rightarrow \distprover$: Verifier samples $\tau\sample \FF^m$,
$\delta\sample \FF^{h-m}$ and sends them to $\distprover$.
\item $\Ag\leftrightarrow \verifier$ compute: $\mu=\mc{T}\tau$,
$\mathsf{cm}=d_0+\sum_{i\in [\ell]}\mu_ic_i$, $x=\mc{H}_2\delta$.
\item $\distprover$ computes: $[[z]]^\xi=u^\xi_0+[[\overline{U}]]^\xi\mu$,
$[[\nu]]_\xi=\nu^\xi_0+\sum_{i\in [\ell]}\mu_i[[\omega_i]]^\xi$.
\item {\color{red} $\Ag$ computes: $z=\combine([[z]]^\xi)$,
$\nu=\combine([[\nu]]^\xi)$}.
\item $\Ag$ and $\verifier$ run the subprotocol:
	\begin{itemize}
	\item $b=\innerproduct(\FF,\GG,\bm{g},x,\mathsf{cm},0;z,\nu)$.
	\end{itemize}
\item $\verifier$ accepts if the subprotocol accepts.
\end{enumerate}
\end{itemize}
\end{framed}
\caption{Distributed Membership Test}
\label{fig:distprox2d}
\end{figure}


\begin{figure}[h!]
\centering
\begin{framed}
\begin{itemize}
\item {$\distproxThreeD(\FF,\GG,L_1,L_2,[\pi];[[\ewit]])$}:
\item {\bf Relation}: $\ewit=\open(\pi)$, $\ewit\in \mc{W}$.
\item {\bf Oracle Setup}: 
	\begin{itemize}
	\item $\distprover\rightarrow \Ag$: Each prover computes shares $[[\comoracle]]^\xi$ from $[[\ewit]]^\xi$ as $[[\comoracle]]^\xi=\comm([[\ewit]]^\xi)$ as in Section \ref{sec:construct_oracle}. 
	\item {\color{red} $\Ag$ computes: $\comoracle :=
\combine([[\comoracle]]^\xi)$ and sets $\pi := \comoracle$ as the oracle}.
	\end{itemize}
\begin{enumerate}[{\rm 1.}]
\item $\verifier\rightarrow\distprover$: Verifier samples $r\sample \FF^p$ and
sends $r$ to $\distprover$.
\item $\distprover$ computes:
	\begin{itemize}
	\item $\shr{\tilde{U}} := \sum_{i\in [p]}r_i\shr{\ewit}[i,\cdot,\cdot]$.
	\item $\shr{\tilde{c}_k} := \sum_{i\in [p]}\shr{\comoracle}[i,k]$ for
$k\in [\ell]$.
	\item $\shr{\tilde{\omega}_k} := \sum_{i\in [p]}O^\xi[i,k]$ for
$k\in [\ell]$.
	\end{itemize}
\item $\distprover\rightarrow\Ag$: The provers send $\shr{\tilde{\bm{c}}} :=
(\shr{\tilde{c}_1},\ldots,\shr{\tilde{c}_\ell})$ to $\Ag$.
\item {\color{red} $\Ag\rightarrow\verifier$: $\Ag$ computes $\tilde{\bm{c}} :=
\combine(\shr{\tilde{\bm{c}}})$ and sends
$\tilde{\bm{c}}=(\tilde{c}_1,\ldots,\tilde{c}_\ell)$ to $\verifier$}.
\item $\Ag$ and $\verifier$ run the subprotocol:
	\begin{itemize}
	\item
$b=\distproxTwoD(\FF,\GG,\ell,L_1,L_2,\tilde{\bm{c}};\shr{\tilde{U}})$.
	\end{itemize}
\item $\verifier$ queries: $\verifier$ samples $Q\subseteq [n]$ of size $t$ and
makes oracle queries for positions in $Q$.
\item Oracle Answers: The oracle responds with columns $\pi[\cdot,k]$ for $k\in
Q$.
\item $\verifier$ checks: The verifier checks $\sum_{i\in
[p]}r_i\pi[i,k]=\sum_{i\in [\ell]}\mc{T}[i,k]\tilde{c}_i$ for $k\in Q$.
\item $\verifier$ accepts if the above check succeeds and $b=1$. 
\end{enumerate}
\end{itemize}
\end{framed}
\caption{Distributed 3D Proximity Protocol}
\label{fig:distprox3d}
\end{figure}
\end{comment} 

Our next lemma captures the fact that the aggregator $\Ag$ gains no additional
knowledge from the messages sent by the provers in the distributed protocol
$\distlinearcheck$. Note that in $\distlinearcheck$, provers are interacting only with the aggregator and in $\distquadcheck$ provers are interacting among themselves but to execute a secure MPC protocol. So, it is enough to argue that the aggregator is not learning any additional information.

\begin{lemma}\label{lem:distlincheckzk}
	For $\xi\in [\Num]$, let $\View^\xi_\Ag$ denote the view of the aggregator $\Ag$ in the distributed
	linear check protocol consisting of messages from the prover $\distprover$ with
	witness $\shr{\wit}$. Then there exists an efficient simulator $\simulator$
	which outputs a view indistinguishalbe from the joint view $\langle
	\View^1_\Ag,\ldots,\View^{\Num}_\Ag\rangle$ 
	%whenever $A\wit=b$ for
	%$\wit=\combine(\shr{\wit})$.
\end{lemma}
\begin{proof}
	
	Let  $\rho, \, r, \, \{(j_u,k_u)\}_{u\in[t]}, \, \delta, \, \beta$ denote the verifier
	randomness, which is shared by each view. Consider other messages in
	$\View^\xi_\Ag$. Let $\shr{\comoracle}, \shr{\bm{\tilde{c}}}, \shr{c_0}, \shr{c_1}, \ldots, \shr{c_{s+\ell}}, \shr{d_0}$ 
	%$c_0^\xi,c_1^\xi,\ldots,c_{s+\ell}^\xi,d_0^\xi,\tilde{c}_1^\xi,\ldots,\tilde{c}_\ell^\xi,\tilde{d}_0^\xi,\{\pi^\xi[\cdot,k_u]\}_{u\in[t]}$ 
	denote the commitments sent by $\distprover$. Also, $\Ag$ gets the opening of $\shr{\comoracle[\cdot,k_u]}, \shr{c_{k_u}}$.
	Finally, $\Ag$ gets the vector $\shr{z} = \beta\shr{P_0} + \shr{\overline{P}}\varphi + \shr{0^{2m}}$.
	%Similarly let $\nu^\xi,\omega^\xi,\{\chi^\xi_u\}_{u\in [t]},\{O^\xi[\cdot,k_u]\}_{u\in [t]})$ denote the commitment randomness sent by the $\distprover$. 
	%Finally, the (extended) view also includes $\ewit^\xi[\cdot,\cdot,k_u]$ for $u\in [t]$, $z^\xi=u_0^\xi+P^\xi\mu$, $\tilde{z}^\xi=\tilde{u}^\xi_0+\tilde{U}^\xi\tilde{\mu}$ and ${z'}^\xi=\beta P^\xi_0 + \overline{P}^\xi\varphi + \shr{0^h}$. 
	Note that other than $\shr{\comoracle}$ and $\shr{z}$, remaining
	included in our definition of the extended view of the single prover case. As in the simulation of the
	single prover case, given the verifier randomness. The joint distribution of ${z}^1,\ldots,{z}^{\Num}$ can be seen to be:
	%({\color{red} Protik to finish the argument}).
	%\begin{itemize}
		%\item Uniform distribution on $\FF^h$ if one of the provers sends an incorrect
		%share of $0^h$.
		Uniform distribution on the set codewords $P_0$ in $\dashL_2$ satisfying
		$\sum_{j\in [m]}P_0[j]=0$ 
		jointly have the correct witness and individually it does not reveal any information due to padding with $0^{2m}$.
		%\item Provers follow the protocol but with incorrect inputs: in lemma ~\ref{lem:privacy}, we described a simulator which can simulate the messages of the honest parties. 
	%\end{itemize}
	
	Thus, the view of the aggregator $\Ag$ can be perfectly simulated.
\end{proof}

\subsection{Semi-honest to Malicious Security: }\label{sec:semi-honesttomalicious} 
To achieve security against malicious provers, every prover $\distprover$ sends Zero-Knowledge Argument of Knowledge (ZKAoK) along with every committed value sent to $\Ag$, which says $\distprover$ knows what value is committed. A malicious secure MPC executes multiplication. Furthermore, all messages are sent via broadcast.
%In our protocol, when a prover $\distprover$ sends any commitment output to the aggregator, in the maliciously secure protocol $\distprover$ with that commitment output, he additionally sends a Zero-Knowledge Argument of Knowledge (ZKAoK) which says $\distprover$ knows which value is committed. This additional step is required in the simulation so that the simulator can extract the committed value and maliciously secure MPC to compute $\mathsf{Mult}$. Furthermore, all the messages to $\Ag$ are via broadcast. 
\pnote{Give the overview of the proof. and explain why the does not go through when the aggregator is corrupt.}

\noindent{\bf Overview of the proof:} 
With the above modification, our protocol can withstand malicious provers. We can design a simulator for the protocol, which uses $\extrac$, the extractor of ZKAoK, Simulator $\Sim_{Mult}$ for the secure MPC for multiplication, and zero-knowledge simulator $\Sim_{ZK}$ of Graphene. \pnote{I think a little detailing is required here.}
%We will describe, in high-level, how to construct a simulator $\Sim$ which can generate a view which is indistinguishable from a real execution, where $\Sim$ has access to the extractor $\extrac$ for the ZKAoK and the simulator $\Sim_{Mult}$ for the secure multiplication and the zero-knowledge simulator $\Sim_{ZK}$ of \name. $\Sim$ receives the messages from the corrupt parties to $\Ag$ due to broadcast. Then, $\Sim$ calls $\extrac$ and obtains opening of $\shr{\comoracle}$ for all corrupt $\xi$. Using $\Sim_{ZK}$, $\Sim$ prepares the messages of the honest parties, and if there is any interaction required between the provers, that interaction is generated via $\Sim_{Mult}$. $\Sim$ checks if all the messages are consistent or not with all the previous messages if not $\Sim$ sends consistent but random values on behalf of honest parties, otherwise, just before sending $\shr{z_{lc}}$ and $\shr{z_{qd}}$, $\Sim$ invokes the $\Func_{\DPZK}$ if it outputs 1, then $\Sim$ generates messages for honest parties using $\Sim_{ZK}$ otherwise it picks messages uniformly at random.

\noindent{\bf Difficulty in simulation while aggregator is malicious:} The above proof works against malicious provers but semi-honest aggregator. To generate the messages on behalf of honest parties, $\Sim$ needs the output of $\Func_{DPZK}$. If $\Ag$ deviates from the protocol at the last round, then the simulator cannot generate an appropriate message, which gives the same output as real-execution. Though we don't have a simulator when $\Ag$ is malicious, we do not have an attack either. Also, it is possible to identify the malicious behavoiur of $\Ag$ since messages to $\Ag$ are via broadcast, and $\Ag$ does not require any internal randomness to generate messages to the $\verifier$. 
 %In the above construction, it considered that provers might deviate while interacting among themselves. However, we do not consider the scenario if $\Ag$ deviates. We do not have a simulator that can simulate a view when all the provers (corrupt and honest) behaving correctly, but $\Ag$ sends a wrong combined value to $\verifier$. To resolve, one can try to invoke the $\Func_{\DPZK}$ after $\Ag$ sending the final value to $\verifier$. However, $\Ag$ needs honest parties messages to generate the message for $\verifier$, and the simulator needs the output of the $\Func_{\DPZK}$. So, it is difficult to get a simulator, which can generate a view indistinguishable from the real world execution. Though we can not construct a simulator for our protocol, we do not have an attack in this corruption model. Also, it is possible to identify the malicious behavoiur of $\Ag$ since messages to $\Ag$ are via broadcast, and $\Ag$ does not require any internal randomness to generate messages to the $\verifier$. 

\begin{comment}
\begin{lemma}\label{lem:privacy}
	For $\xi\in [K]$, let $\View^{\xi}_{\adv}$\pnote{fix notation} denotes the view of the adversary $\adv$ corrupting parties in $I$, where $I$ is a proper subset of the sets of the provers. Then there is a $\ppt$ simulator $\Sim$ which can generate a view of $\adv$ which is indistinguishable from the real execution of the protocol given there is a secure MPC for multiplication which withstand against the corrupt parties in $I$. \pnote{(Maybe a broadcast required)}
\end{lemma}

\begin{proof}
	To prove the above theorem we will design a simulator $\Sim$ which has access to the ideal functionality $\Func_{\DPZK}$ (~\ref{func:DPZK}). Provers in $I$ encode and commit to their inputs and $\Sim$ gets $\shr{\comoracle}, \shr{\comoracle}_a$ $\forall a\in\{x,y,z\}$ and $\forall \xi \in I$. Since the provers are giving ZKAoK in addition to the commitment, by the extractor of argument of knowledge, $\Sim$ extracts and gets shares of $\wit, \wit_x,\wit_y,\wit_z$ of the parties in $I$. 
	%Now $\Sim$ checks if all the steps of Linear and Quadratic checks are done using the same inputs extracted by $\Sim$. If yes, then $\Sim$ calls the functionality $\Func_{\DPZK}$ if it outputs 1, then $\Sim$ uses the same approach of  
	%$\Sim$ picks random challenges for both linear and quadratic checks on behalf of the verifier (picking random challenges on behalf of the verifier is fine since the verifier is semi-honest). 
	Then $\Sim$ uses the simulator for \name and gets an accepting transcript. Using that computes $\shr{\comoracle}, \shr{\comoracle}_a$ $\forall \xi\notin I$, using homomorphic property of the commitment scheme.
	$\Sim$ gets $\shr{c_0}, \shr{c_1}, \ldots, \shr{c_{s+\ell}}$ and $\shr{Z}$ for all $\xi\in I$ in the linear check, as all the provers are sending this values to $\Ag$, which is done using broadcast. Using extraction of commitment $\Sim$ gets the decommitted values. If there is some inconsistency in the values extracted from $\shr{\comoracle}, \shr{\comoracle}_a$ $\forall a\in\{x,y,z\}$ and $\forall \xi \in I$ and these decommitted values then
	%output $\abort$, else continue.
	sets $\mathsf{state}$ as fail, otherwise pass.
	For quadratic check, $\Sim$ calls the simulator, $\Sim_M$, of the secure multiplication protocol and extracts the input of the MPC, and does the consistency check, as in the linear case. If there is some inconsistency then 
	%output $\abort$, else continue.
	sets $\mathsf{state}$ as fail, otherwise, pass.
	Similarly, $\mathsf{state}$ is set for the remaining part of the protocol.
	(This part is informally stated here, details will be provided in the full version.)
	Now for both the protocols if $\mathsf{state}$ remains pass till the provers send the final messages to $\Ag$, which are the witnesses of the inner product arguments, 
	\pnote{Should we add the details such as what are the consistency checks? From the protocol that is quite evident.}
	then $\Sim$ calls the functionality $\Func_{\DPZK}$ with the input $\wit, x,y,z$ of the parties in $I$. If it outputs 1, then $\Sim$ generates the final messages of the honest parties using the accepting transcript produced by the zero-knowledge simulator, else $\Sim$ picks any random message which is consistent with the transcript so far.
	
	If there is some inconsistency in some intermediate step, i.e., the $\mathsf{state}$ is fail, then $\Sim$ calls the functionality $\Func_{\DPZK}$ on some random value and proceeds accordingly with the simulation.
	
	This proves that the view of the corrupt provers can be simulated, which ensures privacy of the honest provers.
\end{proof}

%\end{enumerate}
\end{comment}