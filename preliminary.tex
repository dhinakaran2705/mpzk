\section{Preliminaries}\label{sec:prelims}
\subsection{Linear Codes}
\begin{definition}\label{defn:lincode}
\commentA{this looks like general definition for code. why is it called linear? change}
For positive integers $n,k$ and a finite field $\bbF$, a $k$-dimensional linear subspace $L$ of $\bbF^n$ is called an $[n,k]$ linear code. Elements of $L$ are conventionally called {\em codewords}. 
\end{definition}

For codewords $x,y\in L$ where $x=(x_1,\ldots,x_n)$ and $y=(y_1,\ldots,y_n)$ we define the hamming distance $\dham(x,y)=|\{i\in [n]: x_i\neq y_i\}|$. It is easily checked that $\dham$ defines a metric on $L$. The minimum distance of the code $L$, denoted by $\delta(L)$ is defined as $\min\{\dham(x,y):x,y\in L, x\neq y\}$.

For an $[n,k]$ code $L$, a $n\times k$ matrix $\calG$ is called a {\em generator matrix} iff (i) $\calG x\in L$ for all $x \in \bbF^k$ and (ii) $\calG x\neq \calG y$ for $x\neq y$. Clearly, such a matrix $\calG$ has rank $k$. Similarly an $n\times (n-k)$ matrix $\calH$ such that $y^T \calH = 0$ for all $y\in L$ is called a {\em parity check} matrix for $L$. It is easily seen that the above two matrices exist for any $[n,k]$ linear code $L$. We will assume that description of the linear code $L$ includes a generator matrix $\calG$ and a parity check matrix $\calH$.

\begin{definition}[Interleaved Code]\label{defn:interleavedcode}
For an $[n,k]$-linear code $L$ and a positive integer $m$, we define a {\em row interleaved code} $\ric{L}{m}$ to be the set of $m\times n$ matrices $A$ such that each row of $A$ is a codeword in $L$. Similarly, we define a {\em column interleaved code} $\cic{L}{m}$ to be the set of $n\times m$ matrices $B$ such that each column of $B$ is a codeword in $L$.
\end{definition}

For a linear $[n,k]$-code $L$ over the field $\bbF$, we observe that $\ric{L}{m}$ forms an $[n,k]$-code over the field $\bbF^m$ by viewing each column of the codeword $A\in \ric{L}{m}$ as an element in the field $\bbF^m$ \commentA{this isn't clear at all }. Similarly, $\cic{L}{m}$ forms an $[n,k]$ code over $\bbF^m$ by viewing each row of the codeword $B\in \cic{L}{m}$ as an element in $\bbF^m$. For $A,A'\in \ric{L}{m}$, we define the distance $\dham(A,A')=|\{i\in [n]: A[\cdot,i]\neq A'[\cdot,i]\}|$ where the notation $X[\cdot,i]$ denotes the $i^{th}$ column of the matrix $X$. Similarly for $B,B'\in \cic{L}{m}$ we define $\dham(B,B')=|\{i\in [n]: B[i,\cdot]\neq B'[i,\cdot]\}|$.
 
\begin{definition}[Product Code]\label{defn:productcode}
Let $L_i$ be an $[n_i,k_i]$-linear code for $i=1,2$. We define the product code $L_1\oplus L_2$ \commentA{notation is very bad; } to be the code consisting of $n_2\times n_1$ matrices $A$ such that each row of $A$ is a codeword in $L_1$ and each column of $A$ is a codeword in $L_2$. 
\end{definition}

Note that by definition, the product code $L_1\oplus L_2$ is a row interleaved code of $L_1$ and a column interleaved code of $L_2$, i.e $L_1\oplus L_2 =
\ric{L_1}{n_2}\cap \cic{L_2}{n_1}$. For $A,A'\in L_1\oplus L_2$, we define $\dham_1(A,A')=|\{i\in [n_1]: A[\cdot,i]\neq A'[\cdot,i]\}|$ and $\dham_2(A,A')=|\{i\in
[n_2]: A[i,\cdot]\neq A'[i,\cdot]\}|$. The distance $\dham_1$ corresponds to distance function of the code $\ric{L}{n_2}$, where we view $A,A'$ as codewords in $\ric{L}{n_2}$. Similarly, the distance $\dham_2$ corresponds to the distance function of the code $\cic{L}{n_1}$.

\begin{definition}[Reed Solomon Code]\label{defn:rscode}
An $[n,k]$-Reed Solomon Code $L\subseteq \bbF^n$ consists of vectors $(p(\eta_1),\ldots,p(\eta_n))$ for polynomials $p\in \bbF[x]$ of degree less than $k$ where $\eta_1,\ldots,\eta_n$ are distinct points in $\bbF$. We will use $\rsc{\eta}{k}$ to denote the Reed Solomon code with $\bm{\eta}=(\eta_1,\ldots,\eta_n)$ and $deg(p)<k$.
\end{definition}

\begin{lemma}\label{lem:bicdecoding}
Let $L_1 := \rsc{\eta}{\ell}$ and $L_2 := \rsc{\alpha}{m}$ be $[n,\ell]$ and
$[h,m]$ Reed Solomon codes respectively. Let $\mc{C}_1 := \ric{L_1}{h}$ and
$\mc{C}_2 := \cic{L_2}{n}$ be the interleaved codes of $L_1$ and $L_2$. Suppose
a matrix $U^\ast\in \FF^{h\times n}$ satisfies $d_1(U^\ast,\mc{C}_1)<e_1$ and
$d_2(U^\ast,\mc{C}_2)<e_2$ for $e_1\leq n-\ell$ and $e_2\leq h-m$. Then, there exists $U\in L_1\oplus L_2$ and sets
$S_1\subseteq [n]$, $S_2\subseteq [h]$ with $|S_1|>n-e_1$ and $S_2>h-e_2$ such
that $U^\ast[i,j]=U[i,j]$ for $(i,j)\in S_1\times S_2$.
\end{lemma}
 

\subsection{Geometric Results on Linear Codes}
In this section, we prove some ``geometric'' results about subspaces spanned by linear codes. 
These results will play a key role in protocols for checking that the witness has been correctly encoded with small query complexity. Some variants of results presented in this section appear as lemmas and conjectures in \cite[Section 4]{ligero}.

Throughout this section, let $L$ be a linear $[n,k,d]$ code over field a finite $\HH$ and let $\FF\subseteq \HH$ be a subfield.  Let $m\geq 1$ be an integer, and let $\mc{C}$ denote the row interleaved code $\ric{L}{m}$. For a matrix $U\in \HH^{m\times n}$ and a vector $u_0\in \HH^n$, let $\aff{u_0}{U}$ denote the affine space given by:
\begin{equation}\label{eq:affspace}
\aff{u_0}{U} := \{u_0+r^TU: r\in \FF^m\}
\end{equation}
Note that in the above, the scalars in the linear combination come from $\FF$.

\begin{lemma}\label{lem:farpoint}
Let $L$ and $\mc{C}$ be codes as defined, and let $e$ be a positive integer such that $e+2\leq |\FF|$. Then for any $u_0\in \HH^n$ and any $U^\ast\in \HH^{m\times n}$ such that $d(U^\ast,\mc{C})>e$, there exists $v\in \aff{u_0}{U^\ast}$ such that $d(v,L)>e$.
\end{lemma} 
\begin{proof}
For sake of contradiction, assume that $d(u,L)\leq e$ for all $u\in
\aff{u_0}{U^\ast}$. Let $x$ be the point in $\aff{u_0}{U^\ast}$ such that
$d(x,L)$ is maximum. By assumption $d(x,L)\leq e$. Let $v\in L$ be such that
$\Delta(x,v)=d(x,L)$. Let $E\subseteq [n]$ be the set of positions where $x$ and
$v$ differ. Since $d(U^\ast,\mc{C})>e$, there exists row $R$ of $U^\ast$ and
position $j\in [n]$ 
such that $R_j\neq 0$. Let
$\alpha_1,\ldots,\alpha_{e+1}$ be distinct non zero elements in $\FF$. Let $E_k$
for $k=1,\ldots,e+1$ denote the set of positions where $x+\alpha_kR$ and $v$
differ. Fix a position $i\in E$. Then there exists at most one $k\in [e+1]$ such
that $i\not\in E_k$. By pegion hole principle, there exists $k\in [e+1]$ such
that $E\subseteq E_k$. We also observe that since $\alpha_k\neq 0$, $j\in E_k$.
Thus $d(x+\alpha_k,v)>d(x,v)$, contradicting the choice of $x$. This proves the
lemma.   
\end{proof}


\begin{lemma}[Affine Line]\label{lem:affineline}
Let $L$ be the linear code as defined. Define an affine line $\ell_{u,v}$ in $\HH^n$ as $\ell_{u,v} := \{u + \alpha v:\alpha\in \FF\}$ for $u,v\in \HH^n$. Then for $e < d/3$ and any affine line $\ell_{u,v}$ we have:
\begin{enumerate}[{\rm (i)}]
\item $d(x,L)\leq e$ for all $x\in \ell_{u,v}$, or
\item $d(x,L)> e$ for at most $d$ points in $\ell_{u,v}$.
\end{enumerate}
\end{lemma}
We defer the proof of the above lemma to Appendix.
The next result underlies our key proximity protocols. Intuitively the result states that if a matrix is far away from the code $\mc{C}$, a random linear combination of its rows is far away from a codeword in $L$, and thus the proximity of the matrix to $\mc{C}$ may be tested by testing the proximity of a random linear combination of its rows to $L$.

\begin{lemma}[Affine Subspace]\label{lem:affinesubspace}
Let the codes $L$ and $\mc{C}$ be as defined and $e<d/3$ be an integer. Let $U\in \HH^{m\times n}$ be a matrix such that $d(U,\mc{C})>e$. Then for any $u_0\in \HH^n$, $\prob{d(u_0+r^TU,L)\leq e}\leq d/|\FF|$ for uniformly sampled $r\sample \FF^m$.
\end{lemma}
\begin{proof}
From Lemma \ref{lem:farpoint}, there exists $u\in \aff{u_0}{U}$ such that $d(u,L)>e$. Now we can write $\aff{u_0}{U}$ as union of affine lines passing through $u$. Applying lemma \ref{lem:affineline} to each line, we see that at most $d$ points $x$ on each affine line satisfy $d(x,L)\leq e$. Thus, a randomly sampled point $x$ in $\aff{u_0}{U}$, equivalently obtained as $u_0+r^TU$ for a randomly sampled vector $r\in \FF^m$ satisfies $d(x,L)$ with probability at most $d/|\FF|$.
\end{proof}

\noindent{\bf Three dimensional interleaving}: Consider a $p\times m\times n$ three dimensional
matrix $U[\cdot,\cdot,\cdot]$. We refer to $U[i,\cdot,\cdot]$ as $i^{th}$ {\em
slice}
of $U$, while we refer to $U[\cdot,\cdot,k]$ as the $k^{th}$ {\em plane} of $U$. Let $\mc{C}^p$ denote the set of $p\times m\times n$ matrices $U$ such that
each slice of $U$ is a codeword in $\mc{C}$. Let $\Delta_1$ denote the distance
metric on the set of $p\times m\times n$ matrices denoting the number of planes
where they differ, i.e, $\Delta_1(U,U')=|\{k\in [n]:
U[\cdot,\cdot,k]=U'[\cdot,\cdot,k]\}|$. As before, define $d(U^\ast,\mc{C}^p)$ to be
the minimum value $\Delta_1(U^\ast,U)$ for $U\in \mc{C}^p$. The next result
states that if a three dimensional matrix $U$ is far away from the code
$\mc{C}^p$, a random linear combination of slices of $U$ is far away from $C$.

\begin{lemma}[3D Compression]\label{lem:3dcompression}
Let the codes $L,\mc{C}$ and $\mc{C}^p$ be as defined. Let $U^\ast$ be a
$p\times m\times n$ matrix such that $d(U^\ast,\mc{C}^p)>e$. Then for any
$m\times n$ matrix $u_0$, we have 
\[ \prob{d\left(u_0 + \sum_{i=1}^p r_iU^\ast[i,\cdot,\cdot], \mc{C}\right)\leq e}\leq
\frac{d}{|\FF|}\]  
for a uniformly sampled $(r_1,\ldots,r_p)\sample \FF^p$. 
\end{lemma}
\begin{proof}
Let $\HH$ denote the field $\FF^m$. Then $u_0$ can be viewed as a point in
$\HH^n$. Similarly $U$ can be viewed as $p\times n$ matrix over $\HH$.
We consider $\mc{C}$ as $[n,k,d]$ code over $\HH$ and $\mc{C}^p$ as the
interleaved code of $\mc{C}$ over the field $\HH$. Then by applying Lemma \ref{lem:affinesubspace}
with $\HH=\FF^m$ and codes $\mc{C}$ and $\mc{C}^p$ in place of codes $L$ and
$\mc{C}$, we have the desired bound.
\end{proof}


\subsection{Inner Product Arguments}
We define an interactive protocol that allows proving inner product relation over committed values. 
\begin{definition}\label{defn:commscheme}
 A pair of $\ppt$ algorithms $(\gen,\com)$ constitute a non-interactive commitment scheme if $\sigma\sample \gen(\secparam)$ consists of description of sets $\calM_\sigma$ (message space), $\calR_\sigma$ (randomness space), $\calC_\sigma$ (commitment space) and an efficiently computable function $\com_\sigma: \calM_\sigma\times \calR_\sigma\rightarrow \calC_\sigma$ which is {\em hiding} and {\em binding} as defined later.
\end{definition}

For $x\in \calM_p$, we generate a {\em commitment} of $x$ as $\com_\sigma(x,r)$ where $r\sample \calR_p$ is drawn uniformly at random. For ease of notaion, we simply use $\com$ instead of $\com_\sigma$ and use $\com(x)$ to denote the random variable corresponding to commitment of $x$. 

\begin{definition}[Hiding Commitment]\label{defn:hidingcomm}
A commitment scheme $(\gen,\com)$ is called {\em hiding} (perfectly) if for all $\ppt$ adversaries $\adv$, the following probability is negligibly close to $1/2$:
\begin{align*}
\condprob{b=b'}{
\begin{array}{l}
\sigma\sample \gen(\secparam); \\
(x_0,x_1)\in \calM^2_p\sample \adv(\sigma); \\
b\sample \bitset; c\sample \com(x_b);\\
b'\sample \adv(\sigma,c)
\end{array}
}
\end{align*}
\end{definition}

\begin{definition}[Binding Commitment]\label{defn:bindingcomm}
A commitment scheme $(\gen,\com)$ is called {\em binding} if for all $\ppt$ adversaries $\adv$, 
\begin{align*}
\condprob{\com_p(x_0,r_0)=\com_p(x_1,r_1)\wedge x_0\neq x_1}{
\begin{array}{l}
\sigma\sample \gen(\secparam) \\
x_0,x_1,r_0,r_1 \sample \adv(\sigma)
\end{array}
} < \negl
\end{align*}

\end{definition}

We will assume that all the message spaces $\calM_\sigma$ output by the $\gen$ algorithm come equipped with an inner product operator $\innp{.}{.}:\calM_\sigma\times \calM_\sigma\rightarrow Z$. We define the language $\calL_\sigma\subseteq \calC_\sigma\times \calC_\sigma\times Z$ as:
\begin{equation*}
\calL_\sigma = \{(c_1,c_2,v):\exists x_1,x_2,r_1,r_2 \text{ s.t. }
c_1=\com(x_1,r_1), c_2=\com(x_2,r_2) \text{ and } \innp{x_1}{x_2}=v\} 
\end{equation*}

The $\NP$ relation $\calR_\sigma$ for the language $\calL_\sigma$ consists of pairs $(\stmt,\wit)$ with $\stmt=(c_1,c_2,v)$ and $\wit=(x_1,x_2,r_1,r_2)$ such that $c_1=\com(x_1,r_1)$, $c_2=\com(x_2,r_2)$ and $\innp{x_1}{x_2}=v$ 

\begin{definition}[Inner Product Argument]\label{defn:innerproductarg}
We call an interactive protocol $(\pip,\vip)$ consisting of $\ppt$ interactive algorithms $\pip$ and $\vip$ an inner product argument for commitment scheme $(\gen,\com)$ if it recognizes the language $\calL_\sigma$ as defined previously. Namely, $innp{\pip}{\vip}$ satisfies the following:
\begin{enumerate}[{\rm (i)}]
\item {\bf Completeness}: For all adversaries $\adv$,
\begin{align*}
\condprob{(\stmt,\wit)\in \calR_\sigma \vee \langle \pip(\sigma,\stmt,\wit),\vip(\sigma,\stmt)\rangle={\tt accept}}{
\begin{array}{l}
\sigma\sample \gen(\secparam);\\
(\stmt,\wit)\sample \adv(\sigma)
\end{array}
}=1
\end{align*}

\item{\bf Soundness}: For all deterministic polynomial time $\prover^*$ and$\ppt$ adversaries $\adv$:
\begin{align*}
\condprob{\stmt\not\in \calL_\sigma \wedge \langle
\prover^*(\sigma,\stmt,s),\vip(\sigma,u)\rangle={\tt accept}}{
\begin{array}{l}
\sigma\sample \gen(\secparam);\\
(\stmt,s)\sample \adv(\sigma)
\end{array}
} = \negl(\lambda)
\end{align*}
\end{enumerate}
\end{definition}

We mention some concrete instantiations of commitment schemes and corresponding inner product arguments that we use in our protocol.

\noindent{\em Logarithmic Inner Product Argument}: In this setting we have $\calM_\sigma=\bbZ^n_p$, $\calR_\sigma=\bbZ_p$, $\calC_\sigma=\bbG$ where $\bbG$ is group of prime order $p$. The algorithm $\gen$ samples generators $g_1,\ldots,g_n$, $h$ $\sample \bbG$. The commitment is a pederson vector commitment given by $\com({\bf x } ,r ) = h^r \cdot \prod_{i=1}^n {g_i}^{x_i}$ where ${\bf x}=(x_1,\ldots,x_n)$. We use the inner product argument $(\piplog,\viplog)$ from Bootle et.al in \cite{Bulletproofs} for the commitment scheme $(\gen,\com)$. The interactive protocol $(\piplog,\viplog)$ is a $O(\log n)$ round protocol with argument size $O(\log n)$. Time complexity of the verifier $\viplog$ is given by $t(\viplog)=O(n).\bbZ_p + O(n).\bbG$.\smallskip


\noindent{\em Square Root Inner Product Argument}: In this setting we use the same commitment scheme as above. For the inner product argument we use the interactive protocol $(\pipsq,\vipsq)$ from \cite{InnerProductDLS} or \cite{Groth09b}. The construction in \cite{InnerProductDLS} gives a $5$-move protocol with total communication complexity $O(\sqrt{n})$. The construction in \cite{Groth09b} gives a 7-move protocol with $O(\sqrt{n})$ communication complexity. In both the constructions $t(\vipsq) = O(n).\bbZ_p +
O(\sqrt{n}).\bbG$.

\subsection{Forking Lemma and Knowledge Soundness}
We use the Forking Lemma from \cite{InnerProductDLS,bulletproofs} to describe 
expected polynomial time knowledge extractors for our protocols. Let
$(\prover,\verifier)$ be a public coin $(2\mu+1)$-move interactive protocol with
challenges $x_1,\ldots,x_\mu$ in sequence. Let $n_i\geq 1$ for $1\leq i\leq
\mu$. We call a collection of $n=\prod_{i=1}^\mu n_i$ accepting transcripts to be
$(n_1,\ldots,n_\mu)$-tree of accepting transcripts, if the challenges are
organized in the tree format that we describe now: The root of the tree is
labelled with the statement being proved. Each node of depth $i<\mu$ has exactly
$n_i$ children, each labelled with a distinct value of the $i^{th}$ challenge
$x_i$. We call a $\ppt$ algorithm $\chi$ to be a {\em witness extraction
algorithm} if $\chi$ can extract a witness $w$ to the statement, given an
appropriate tree of accepting transcripts. This can be seen as a generalization
of the notion of special soundness for Sigma protocols with $n=2$ and $\mu=1$.

\begin{definition}[Argument of Knowledge]\label{def:argofknowledge}
Let $(\setup,\prover,\verifier)$ be an public coin interactive protocol. We say that
$(\setup,\prover,\verifier)$ is an {\em argument of knowledge} for the language
$\mc{L}$ if for every $\ppt$
prover $P^\ast$, there exists an expected polynomial time extractor $\extr$ such that for all $x$:
\begin{align}
\condprob{(x,w)\in \mc{L}}{
\begin{array}{c}
\sigma\sample\setup(\secparam) \\
w\sample \extr^{\mc{O}}(x,\sigma)
\end{array}
}
\geq \condprob{\langle
P^\ast(x,\sigma),\verifier(x,\sigma)\rangle=1}{\sigma\sample \setup(\secparam)} -
\kappa(\lambda)
\end{align}
for some negligible function $\kappa$. In the above $\mc{O}$ denotes the
transcript oracle $\langle P^\ast(x,\sigma),\verifier(x,\sigma)\rangle$ which
can be rewound to any previous state, and resumed with fresh randomness for the
verifier $\verifier$.
\end{definition}

Our argument of knowledge proofs rely on the following result from \cite{bulletproofs}.
While the result is originally stated and proved for showing witness-extended
emulation, we restate it for the case of argument of knowledge. The proof in
\cite{InnerProductDLS} also applies to this restricted case.

\begin{lemma}[Forking Lemma,\cite{bulletproofs}]\label{lem:forkinglemma}
Let $(\setup,\prover,\verifier)$ be a $(2\mu+1)$-move public coin interactive
protocol. Let $\chi$ be a $\ppt$ witness extraction algorithm that outputs a
witness with probability $1-\kappa(\secpar)$ from an $(n_1,\ldots,n_\mu)$-tree
of accepting transcripts. Assuming that $\prod_{i=1}^\mu n_i$ is bounded by a
polynomial in security parameter $\secpar$, the protocol
$(\setup,\prover,\verifier)$ is an argument of knowledge.
\end{lemma}

 
%--------------------------------Protik's prelims--------------------------------
%\subsection{Codes}
%\paragraph{\textbf{Reed-Solomon Code:}} For positive integers $n,k$, finite field $\mathbb{F}$, and a vector $\eta = (\eta_1,\cdots ,\eta_n) \in \mathbb{F}_n$ of distinct field elements, the code $RS_{\mathbb{F},n,k,\eta}$ is the $[n,k,n-k+1]$ linear code over $\mathbb{F}$ that consists of all $n$-tuples $(p(\eta_1),...,p(\eta_n))$ where $p(\cdot)$ is a polynomial of degree $< k$ over $\mathbb{F}$.
%\dnote{Comments for Protik:\\
%1. Have spaces between paragraphs even in Latex. The PDF generated by Latex is usually beautiful and readable. The tex file can't be beautiful but should atleast be as readable as the PDF.\\
%2. I have already told you many times, and I am not going to leave till you use it :) Use macro for any notation that you use more than two times throughout the paper. (You will realize the use of it the day we decide change some notation in the middle of writing a paper.. But it is also a good practice to do it in general).\\
%3.Use \ ldots for ...\\
%4. You could use subsection* or subsubsection* instead of having a paragraph and using a textbf inside. (try both, what looks better depends on the cls file you use).}
%\paragraph{\textbf{Interleaved Code:}} Let $L\subset \mathbb{F}_n$ be an $[n,k,d]$ linear code over $\mathbb{F}$. We let $L^m$ denote the $[n,mk,d]$ (interleaved) code over $\mathbb{F}^m$ whose code words are all $m\times n$ matrices $U$ such that every row $U_i$ of $U$ satisfies $U_i\in L$. For $U\in L^m$ and $j\in[n]$, we denote by $U[j]$ the $j^{th}$ symbol (column) of $U$.
%\dnote{Have "th" in jth outside math mode.}
%
%\subsection{Interactive Oracle Proofs} The Interactive Oracle Proofs is the notion which combine both Interactive Proofs and Probabilistically Checkable Proofs, and also generalize the notion of the Interactive PCPs.
%\paragraph{} A $k$-round public-coin IOP has $k$ rounds of interaction. In the $i^{th}$ round of interaction, the verifier sends a uniformly random message $m_i$ to the prover; then the prover replies with a message $\pi_i$ to the verifier. After $k$ rounds of interaction, the verifier makes some queries to the oracles it received and either accepts or rejects.
%\dnote{Have at least the main definitions in the Definition environment.}
%\paragraph{} An IOP system for a relation $\mathcal{R}$ with round complexity $k$ and soundness error $\epsilon$ is a pair $(P, V )$, where $P, V$ are probabilistic algorithms, that satisfies the following properties:
%\paragraph{\textit{Completeness:}}  For every instance-witness pair $(x,w)$ in the relation $\mathcal{R}, (P (x, w), V (x))$ is a $k(n)$-round interactive oracle protocol with accepting probability 1.
%\paragraph{\textit{Soundness:}} For every instance $x \notin \mathcal{L(R)}$ and unbounded malicious prover $P^*, (P^*, V (x))$ is a $k(n)$-round interactive oracle protocol with accepting probability at most $\epsilon(n)$.
%\subsection{Zero-Knowledge} 
%\paragraph{\textbf{Interactive Argument Systems:}} A pair of PPT(Probabilistic Polynomial Time) interactive machines $<P, V>$ is called an interactive proof system for a language $\mathcal{L}$ if there exists a negligible function $negl(\cdot)$ such that the following two conditions hold:
%\dnote{1. have space before a ( or any other bracket.\\
%2. Use \ langle and \ rangle instead of $<$ and $>$ when using it as brackets.
%3. You can have negl in mathsf, it would look better (have a macro for this too!)}
%\begin{itemize}
%	\item[(1)] \textit{Completeness:} For every $x\in \mathcal{L}$ there exists a string $w$ such that for every $z \in \{0,1\}^*$,
%$Pr[<P(x,w),V(x,z)>=1] \geq 1-negl(|x|)$.
%	\item[(2)] \textit{Soundness:} For every $x \notin \mathcal{L}$, every interactive PPT machine $P^*$, and every $w,z\in \{0,1\}^*$ $Pr[<P^*(x,w),V(x,z)>=1]\leq negl(|x|)$ 
%\end{itemize}
%\paragraph{\textbf{Zero Knowledge:}} Let$<P,V>$ be an interactive proof system for some language $\mathcal{L}$. We say that $<P,V>$ is computational zero-knowledge with respect to an auxiliary input if for every PPT interactive machine $V^*$ there exists a PPT algorithm $S$, running in time polynomial in the length of its first input, such that $\{<P(x,w),V^*(x,z)>\}_{x\in \mathcal{L},w\in \mathcal{R}_x,z\in \{0,1\}^*}\approx_c \{<S(x,z)>\}_{x\in\mathcal{L},z\in\{0,1\}^*}$
%\subsection{Commitment schemes} 
%\paragraph{\textbf{Commitemnts:}} A non-interactive commitment scheme consists of a pair of probabilistic polynomial time algorithms $(Setup,Com)$. The setup algorithm $pp\leftarrow Setup(1^{\lambda})$ generates public parameters $pp$ for the scheme, for security parameter $\lambda$. The commitment algorithm $Com_{pp}$ defines a function $M_{pp} \times R_{pp} \rightarrow C_{pp}$ for message space $M_{pp}$, randomness space $R_{pp}$ and commitment space $C_{pp}$ determined by $pp$. For a message $x\in M_{pp}$, the algorithm draws $\delta \in_R  R_{pp}$ uniformly at random, and computes commitment $\com = Com_{pp}(x; \delta)$.\\
%For ease of notation we write $Com = Com_{pp}$.
%\dnote{All algorithm names in mathsf (macro for each). Eg. in the above paragraph, Setup, Com, ...}
%\paragraph{\textbf{Homomorphic Commitment:}} A homomorphic commitment scheme is a non-interactive commitment scheme such that $M_{pp},R_{pp}$ and $C_{pp}$ are all abelian groups, and for all $x_1,x_2 \in M_{pp}, \delta_1,\delta_2 \in R_{pp}$, we have $Com(x_1; \delta_1) + Com(x_2; \delta_2) = Com(x_1 + x_2; \delta_1 + \delta_2)$
%\dnote{Hiding and binding are the core properties of a commitment scheme, i.e., a definition of a commitment scheme includes the properties of hiding and binding. So bring them first. And just mention ``Hiding'' and ``Binding''.}
%\paragraph{\textbf{Hiding Commitment:}} A commitment scheme is said to be hiding if for all PPT adversaries $\Adv$ there exists a negligible function $\mu(\lambda)$ such that
%$$ |Pr[b=b'|pp\leftarrow Setup(1^{\lambda}); (x_0,x_1)\in M^2_{pp}\leftarrow \Adv(pp), b\in_R\{0,1\}, \delta \in_R R_{pp}, \com=Com(x_b;\delta), b'\leftarrow \Adv(pp,com)]-\frac{1}{2}|\leq \mu(\lambda)$$
%\paragraph{\textbf{Binding Commitment:}} A commitment scheme is said to be binding if for all PPT adversaries $\Adv$ there exists a negligible function $\mu$ such that 
%$$Pr[Com(x_0;\delta_0)=Com(x_1,\delta_1) \wedge x_0\neq x_1| pp\leftarrow Setup(1^{\lambda})x_0,x_1,\delta_0, \delta_1\leftarrow \Adv(pp)]\leq \mu(\lambda)$$
%\dnote{Have a subsection* with Pedersen commitment, and have the vector commitment also in the same part.}
%\paragraph{\textbf{Pedersen Commitment:}} $M_{pp}, R_{pp} = \mathbb{Z}_p, C_{pp} = \mathbb{G}$ of order $p$.\\
%$Setup : g, h \in_R \mathbb{G}$\\
%$Com(x,\delta)=(g^xh^{\delta})$
%\paragraph{\textbf{Pedersen Vector Commitment:}} $M_{pp}= \mathbb{Z}^n_p , R_{pp} = \mathbb{Z}_p, C_{pp}= \mathbb{G}$ with G of order p.\\ 
%$Setup: \vc{g}=(g_1,\cdots,g_n),h \in_R \mathbb{G}$\\
%$Com(\vc{x} = (x_1,\cdots,x_n);\delta) = h^r\vc{g}^{\vc{x}} = h^r \prod\limits_i g_i^{x_i} \in \mathbb{G}$
