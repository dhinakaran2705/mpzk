\section{Notation and Preliminaries}\label{sec:prelims}
\subsection{Basic Notation}\label{sec:basicnotation}
In this section, we describe the notation that will be followed in rest of the
paper. We will often recall the notation in appropriate places. 
We use $[n]$ to
denote the set $\{1,\ldots,n\}$. All the arithmetic circuits and constraint
systems are assumed to be defined over finite field denoted by $\FF$. For a
vector $x\in \FF^m$, we implicitly assume $x_i$ represents the $i^{th}$
component of $x$ for $i\in [m]$. Whenever suitable, we will also use $x[i]$ to
denote $i^{th}$ entry of a vector $x$. We naturally extend the notation $[]$ to
index matrices and three dimensional matrices. For an $m\times n$ matrix over
$\FF$, we use $A[i,j]$ to denote $(i,j)^{th}$ entry of $A$. We use $A[i,\cdot]$
to denote the $i^{th}$ row of $A$ and $A[\cdot,j]$ to denote $j^{th}$ column of
$A$. Similarly, for an $m\times n\times p$ matrix $X$, we use $X[i,j,k]$ to
denote the $(i,j,k)^{th}$ entry of $X$. The notation $X[i,\cdot,\cdot]$ denotes
the $n\times p$ matrix $Y$ given by $Y[j,k] := X[i,j,k]$. 
Analogously,
$X[\cdot,j,\cdot]$ and $X[\cdot,\cdot,k]$ denote $m\times p$ and $m\times n$
matrices respectively. For three dimensional
matrix $X$, we will refer to submatrices $X[i,\cdot,\cdot]$ as {\em slices} of
$X$, and submatrices $X[\cdot,\cdot,k]$ as the {\em planes} of $X$. \commentA{don't need a name for the other dimension?}

Throughout the paper we use several distance metrics on vectors and matrices,
which we now introduce. For two vectors $x,y$ of length $n$, we define
$\Delta(x,y)=|\{i\in [n]: x_i\neq y_i\}|$. For two $m\times n$ matrices $A,B$ we
define $\Delta_1(A,B)=|\{j\in [n]: A[\cdot,j]\neq B[\cdot,j]\}|$ and
$\Delta_2(A,B)=|\{i\in [m]: A[i,\cdot]\neq B[i,\cdot]\}|$. For $p\times m\times
n$ matrices $X$ and $Y$, we define $\Delta_1(X,Y)=\{k\in
[n]:X[\cdot,\cdot,k]\neq Y[\cdot,\cdot,k]\}$. Similarly we define $\Delta_2(X,Y)
= |\{j\in [m]: X[\cdot,j,\cdot]\neq Y[\cdot,j,\cdot]\}|$. Let $u$ be a vector
and $S$ be a set of vectors. We use $d(u,S) := \min\{\Delta(u,v):v\in S\}$ to
denote distance between $u$ and $S$. Similarly for a matrix (two or three
dimenional) $U$ and a set of matrices $\mc{M}$, we define $d_i(U,\mc{M}) :=
\min\{\Delta_i(U,M): M\in \mc{M}\}$ for $i=1,2$.

\subsection{Notation for Protocols}
We describe some of the notation that we use to describe our protocols. We
notate our interactive protocols with a name and a list of parameters. The
parameters to the left of semicolon ($;$) are assumed to be {\em public}, while those to the
right of the semicolon are considered {\em private} inputs of the prover. For example
$\innerproduct(\FF,\GG,\bm{g},x,\mathsf{cm},v;z)$ denotes a protocol with $6$
public parameters and a private parameter $z$. We enclose a public parameter in
brackets ($[\,]$) to denote ``oracle'' access to the parameter (i.e, it can only
be partially queried by the verifier). As an illustration
$\proximityThreeD(\FF,\GG,L_1,L_2,[\pi];\ewit)$ denotes a protocol, where the
verifier only has oracle access to $\pi$. Finally, in the distributed setting,
some of the private inputs may be secret shared  across multiple provers \commentA{be specific about the form of secret sharing}. We notate it
by enclosing those private inputs in $[[\,]]$ \commentA{better to use $\langle \rangle$ notation}. For example, in the protocol
$\proximityTwoD(\FF,\GG,\ell,L_1,L_2,\bm{c};[[\bar{U}]])$, each prover
$\distprover$ only has a share $\shr{\bar{U}}$ of the matrix $\bar{U}$.

\subsection{Linear Codes}
\begin{definition}\label{defn:lincode}
Let $n,k,d$ be positive integers with $n\geq k$ and $\FF$ be a finite field. We
call $L\subseteq \FF^n$ to be an $[n,k,d]$ linear code if $L$ is a $k$-dimnensional
subspace of $\FF^n$ and $d$ is the minimum value of $\Delta(x,y)$ for distinct
$x,y\in L$. Elements of $L$ are conventionally called {\em codewords}.
\end{definition}

For an $[n,k,d]$ code $L$, an $n\times k$ matrix $\calG$ is called a {\em
generator matrix} for $L$ iff (i) $\calG x\in L$ for all $x \in \bbF^k$ and (ii)
$\calG x\neq \calG y$ for $x\neq y$. Clearly, such a matrix $\calG$ has rank
$k$. Similarly an $n\times (n-k)$ matrix $\calH$ such that $y^T \calH = 0$ for
all $y\in L$ is called a {\em parity check} matrix for $L$. It is easily seen
that the above two matrices exist for any $[n,k,d]$ linear code $L$. We will
assume that description of the linear code $L$ includes a generator matrix
$\calG$ and a parity check matrix $\calH$.

\begin{definition}[Interleaved Code]\label{defn:interleavedcode}
For an $[n,k,d]$ linear code $L$ and a positive integer $m$, we define a {\em row interleaved code} $\ric{L}{m}$ to be the set of $m\times n$ matrices $A$ such that each row of $A$ is a codeword in $L$. Similarly, we define a {\em column interleaved code} $\cic{L}{m}$ to be the set of $n\times m$ matrices $B$ such that each column of $B$ is a codeword in $L$.
\end{definition}

Let $L$ be a linear $[n,k,d]$ code over the field $\FF$ and let
$\mc{C}=\ric{L}{m}$ be the row interleaved code of $L$. One can view $\mc{C}$ as
a $[mn,mk,d]$ linear code over the field $\FF$. Alternatively, we can view
$\mc{C}$ as an $[n,k,d]$ code over the field $\HH$, where $\HH\cong \FF^m$. We
elaborate on this correspondence next.

\subsection{Interleaved Codes: Alternative interpretation}
For concreteness, let $\FF$ be the finite field $\FF_q$, consisting of $q$
elements. Let $\HH$ denote the finite field consisting of $q^m$ elements. From
standard algebra, the field $\HH$ is specified by an irreducible polynomial $h(x)\in \FF[x]$ of
degree $m$ as follows:
\begin{itemize}
\item The elements of $\HH$ are polynomials $a(x)$ over $\FF$ of degree at most
$m-1$.
\item The field operations $(+,\cdot)$ on $\HH$ are defined as corresponding
polynomial operations modulo $h(x)$, i.e, on $\HH$ we define $a(x)+b(x) =
(a(x)+b(x)) \text{ mod } h(x)$ and $a(x)\cdot b(x) = (a(x)\cdot b(x)) \text{ mod
} h(x)$. 
\end{itemize}
We can identify the elements of field $\HH$ naturally with the set $\FF^m$ via
the bijection $f:\FF^m\leftrightarrow \HH$ given by
$f(a_0,\ldots,a_{m-1})=\sum_{i=0}a_ix^i$ \commentA{seems to map $\FF^m$ to $\FF$ and NOT what you stated }. For any $a,b\in \FF^m$ and $\alpha\in
\FF$, it is seen that $f(a+b)=f(a)+f(b)$ and $f(\alpha a)=\alpha f(a)$. We can
extend $f$ to the bijection $\tilde{f}$ between the set $\mc{M}_{m,n}$ of $m\times n$
matrices over $\FF$ and $\HH^n$ by defining \commentA{same  issue here}:
\begin{equation}\label{lem:bijection}
\tilde{f}(A) = \big(f(A[\cdot,1]),\ldots,f(A[\cdot,n])\big)
\end{equation}
We have the following:
\begin{lemma}[Correspondence Lemma]\label{lem:correspondence}
Let $L$ be an $[n,k,d]$ code over the field $\FF=\FF_q$, and let $\mc{C} :=
\ric{L}{m}$ be the row interleaved code of $L$. Then $\tilde{f}(\mc{C})$ is an
$[n,k,d]$ code over the field $\HH$, where $\HH=\FF_{q^m}\cong \FF^m$. Moreover,
the generator matrix $\mc{G}$ of $L$ is also the generator matrix of
$\tilde{f}(\mc{C})$. We note that in the latter case $\mc{G}$ is viewed as a
matrix over the field $\HH$.
\end{lemma}
\begin{proof}
Let $\tilde{\mc{C}}$ denote the linear code over $\HH$ given by $\tilde{\mc{C}} :=
\{\mc{G}x: x\in \HH^k\}$. Clearly, $\tilde{\mc{C}}$ is a linear code over $\HH$.
Further it can be seen that $\tilde{\mc{C}}$ is an $[n,k,\ast]$ linear code because
$\mathsf{rank}_{\FF}(\calG)=\mathsf{rank}_{\HH}(\calG)$. This is because $\calG$
contains entries from the subfield $\FF$, and so the determinant of a submatrix of $\calG$
vanishes over $\HH$ if and only if it vanishes over the subfield $\FF$. Now,
consider $A\in \mc{C}$. Then there exists $m\times k$ matrix $P$ such that for all $j\in [n]$, 
we have $A[\cdot,j]=\sum_{i\in [m]}\calG[i,j]P[\cdot,i]$. Therefore, for all
$j\in [m]$,  we have
\begin{align*}
f(A[\cdot,j]) &= f\big(\sum_{i=1}^m \calG[i,j]P[\cdot,i]\big) \\
	&= \sum_{i=1}^k \calG[i,j]f(P[\cdot,i]) \\
	&= \sum_{i=1}^k \calG[i,j]p_i(x) \text{ where } p_i(x)=f(P[\cdot,i]).
\end{align*}
From the above, it can be seen that:
\[ \tilde{f}(A)=(f(A[\cdot,1]),\ldots,f(A[\cdot,n]))=\calG
[p_1(x),\ldots,p_k(x)]^T \]
 where we view $\tilde{f}(A)$ as a column vector. Thus
$\tilde{f}(\mc{C})\subseteq \tilde{\mc{C}}$. Similarly, one can show that
$\tilde{\mc{C}}\subseteq \tilde{f}(\mc{C})$ and thus conclude
$\tilde{f}(\mc{C})=\tilde{\mc{C}}$. Using similar arguments we can also show
that the code $\tilde{f}(\mc{C})=\tilde{\mc{C}}$ also has the same parity check
matrix as the code $L$, and hence has the same minimum distance $d$. We skip the
proof as it is standard. 
\end{proof}

Analogous to the row interleaved code, a column interleaved code $\cic{L}{m}$ 
can be viewed as an $[n,k,d]$ code over $\bbF^m$ by viewing each row of the
codeword $B\in \cic{L}{m}$ as an element in $\bbF^m$. Here the distance metric
for $\ric{L}{m}$ is given by the matrix distance $\Delta_1$, while the distance
metric for $\cic{L}{m}$ is given by the matrix distance $\Delta_2$ as defined in
Section \ref{sec:basicnotation}.  
 
\begin{definition}[Product Code]\label{defn:productcode}
Let $L_i$ be an $[n_i,k_i,d_i]$-linear code for $i=1,2$. We define the product
code $L_1\otimes L_2$ to be the code consisting of $n_2\times n_1$ matrices $A$
such that each row of $A$ is a codeword in $L_1$ and each column of $A$ is a
codeword in $L_2$.
\end{definition}

Note that by definition, the product code $L_1\otimes L_2$ is a row interleaved
code of $L_1$ and a column interleaved code of $L_2$, i.e $L_1\otimes L_2 =
\ric{L_1}{n_2}\cap \cic{L_2}{n_1}$. For $A,A'\in L_1\otimes L_2$, we define
$\dham_1(A,A')=|\{i\in [n_1]: A[\cdot,i]\neq A'[\cdot,i]\}|$ and
$\dham_2(A,A')=|\{i\in [n_2]: A[i,\cdot]\neq A'[i,\cdot]\}|$. The distance
$\dham_1$ corresponds to distance function of the code $\ric{L}{n_2}$, where we
view $A,A'$ as codewords in $\ric{L}{n_2}$. Similarly, the distance $\dham_2$
corresponds to the distance function of the code $\cic{L}{n_1}$.

\begin{definition}[Reed Solomon Code]\label{defn:rscode}
An $[n,k]$-Reed Solomon Code $L\subseteq \bbF^n$ consists of vectors
$(p(\eta_1),\ldots,p(\eta_n))$ for polynomials $p\in \bbF[x]$ of degree less
than $k$ where $\eta_1,\ldots,\eta_n$ are distinct points in $\bbF$. We will use
$\rsc{\eta}{k}$ to denote the Reed Solomon code with
$\bm{\eta}=(\eta_1,\ldots,\eta_n)$ and $deg(p)<k$.
\end{definition}

\begin{lemma}\label{lem:bicdecoding}
Let $L_1 := \rsc{\eta}{\ell}$ and $L_2 := \rsc{\alpha}{m}$ be $[n,\ell]$ and
$[h,m]$ Reed Solomon codes respectively. Let $\mc{C}_1 := \ric{L_1}{h}$ and
$\mc{C}_2 := \cic{L_2}{n}$ be the interleaved codes of $L_1$ and $L_2$. Suppose
a matrix $U^\ast\in \FF^{h\times n}$ satisfies $d_1(U^\ast,\mc{C}_1)<e_1$ and
$d_2(U^\ast,\mc{C}_2)<e_2$ for $e_1\leq n-\ell$ and $e_2\leq h-m$.
Then, there exists $U\in L_1\otimes L_2$ and sets
$S_1\subseteq [n]$, $S_2\subseteq [h]$ with $|S_1|>n-e_1$ and $|S_2|>h-e_2$ such
that $U^\ast[i,j]=U[i,j]$ for $(i,j)\in S_1\times S_2$.
\end{lemma}

\begin{proof}
	Let $U_1 \in \mc{C}_1$  such that $\dham_1(U^\ast, U_1) \leq \dham(U^\ast, U) \forall U\in \mc{C}_1$ and let $\dham_1(U^\ast, U_1) = \beta_1$ with $\beta_1<e_1$
	Therefore $U^\ast$ differs from $U_1$ in $\beta_1$ many columns, i.e. in the remaining $n-\beta_1$ columns $U^\ast$ and $U_1$ are identical.
	So, there are columns $S_1 = \{j_{s} : s\in [n - \beta_1] \}$ for which $U^\ast[i,j] = U_1[i,j]$ $\forall j\in S$.
	Let $U_2 \in \mc{C}_2$  such that $\dham_2(U^\ast, U_2) \leq \dham(U^\ast, U) \forall U\in \mc{C}_2$ and let $\dham_1(U^\ast, U_2) = \beta_2$ with $\beta_2<e_2$
	Therefore $U^\ast$ differs from $U_2$ in $\beta_2$ many rows, i.e. in the remaining $n-\beta_1$ rows $U^\ast$ and $U_2$ are identical.
	So, there are rows $S_2 = \{i_{t} : t\in [h - \beta_2] \}$ for which $U^\ast[i,j] = U_2[i,j]$ $\forall i\in T$.
	
	$U_1\in \mc{C}_1 \implies \exists$ polynomials $p_1(\cdot), \ldots, p_h(\cdot)$ of degree $< \ell$
	
	$U_2\in \mc{C}_2 \implies \exists$ polynomials $q_1(\cdot), \ldots, q_n(\cdot)$ of degree $< m$
	
	Such that $p_i(\eta_j) = U_1[i,j] $ and $ q_i(\alpha_j) = U_2[j,i]$
	
	Therefore, $U_1[i,j] = U_2[i,j]$ $\forall i\in S_1, j\in S_2$. Choose a bivariate polynomial $Q(x,y)$ such that $deg_x(Q)<\ell $ and $deg_y(Q)<m$ and $Q(\eta_{i_s}, \alpha_{j_t}) = U_1[i_s,j_t] = U_2[i_s,j_t] \forall (i, j)\in S_1\times S_2$. 
	
	Defne $U = Q(\eta_i,\alpha_j)$ $\forall i\in[h], j\in[n]$, Then $U \in L_1\otimes L_2$ and $U^\ast[i,j] = U[i,j]$ $\forall (i,j) \in S_1 \times S_2$.
	
	And $|S_1|= n-\beta_1 > n-e_1$ and $|S_2|= h-\beta_2 > h-e_2$.
	This proves the above claim.	
\end{proof}
 

\subsection{Coding Theoretic Results for our Constructions}
In this section we prove the coding theoretic result (Proposition \ref{lem:3dcompression}), which underlies
the construction of our new ``MPC-friendly'' zero knowledge protocol. The result
extends the approach in \cite{ligero}  of interleaving the witness and checking proximity (to well-formed encoding) by
compressing along a dimension using a random linear combination. 
While in \cite{ligero}, the witness was interleaved as a matrix,
we interleave the witness as a three dimensional matrix to obtain smaller
argument size. The following result is the key to testing proximity of a three
dimensional matrix to a well formed encoding.
\begin{proposition}[3D Compression]\label{lem:3dcompression}
Let $\HH$ be a finite field and $\FF\subseteq \HH$ be a subfield of $\HH$. Let $L$ be an $[n,k,d]$ code over $\FF$, and $\mc{C} :=
\ric{L}{m}$ be a row interleaved code of $L$. Let $\mc{C}^p$ denote the set of
$p\times m\times n$ matrices $U$ over $\FF$ such that $U[i,\cdot,\cdot]\in
\mc{C}$ for all $i\in [p]$. Let $U^\ast$ be a
$p\times m\times n$ matrix such that $d_1(U^\ast,\mc{C}^p)>e$. Then for any
$m\times n$ matrix $u_0$, we have 
\[ \prob{d_1\left(u_0 + \sum_{i=1}^p r_iU^\ast[i,\cdot,\cdot], \mc{C}\right)\leq e}\leq
\frac{d}{|\FF|}\]  
for a uniformly sampled $(r_1,\ldots,r_p)\sample \FF^p$. 
\end{proposition}

We need several observations to prove the above, which we present next.
Throughout this section, let $L$ be a linear $[n,k,d]$ code over a field $\HH$ and let $\FF\subseteq \HH$ be a subfield.  Let $m\geq 1$ be an integer, and let $\mc{C}$ denote the row interleaved code $\ric{L}{m}$. For a matrix $U\in \HH^{m\times n}$ and a vector $u_0\in \HH^n$, let $\aff{u_0}{U}$ denote the affine space given by:
\begin{equation}\label{eq:affspace}
\aff{u_0}{U} := \{u_0+r^TU: r\in \FF^m\}
\end{equation}
Note that in the above, the scalars in the linear combination come from $\FF$.

The following result was proved in \cite{ligero} for the setting $\HH=\FF$.
For completeness, we present an adaptation of the proof to the setting where
$\FF$ is a subfield of $\HH$.
\begin{lemma}\label{lem:farpoint}
Let $L$ and $\mc{C}$ be codes as defined, and let $e$ be a positive integer such that $e+2\leq |\FF|$. Then for any $u_0\in \HH^n$ and any $U^\ast\in \HH^{m\times n}$ such that $d(U^\ast,\mc{C})>e$, there exists $v\in \aff{u_0}{U^\ast}$ such that $d(v,L)>e$.
\end{lemma} 
\begin{proof}
For sake of contradiction, assume that $d(u,L)\leq e$ for all $u\in
\aff{u_0}{U^\ast}$. Let $x$ be the point in $\aff{u_0}{U^\ast}$ such that
$d(x,L)$ is maximum. By assumption $d(x,L)\leq e$. Let $v\in L$ be such that
$\Delta(x,v)=d(x,L)$. Let $E\subseteq [n]$ be the set of positions where $x$ and
$v$ differ. Since $d(U^\ast,\mc{C})>e$, there exists row $R$ of $U^\ast$ and there is some 
position $j\in [n]\backslash E$ 
such that $R_j\neq 0$. Let
$\alpha_1,\ldots,\alpha_{e+1}$ be distinct non zero elements in $\FF$. Let $E_k$
for $k=1,\ldots,e+1$ denote the set of positions where $x+\alpha_kR$ and $v$
differ. Fix a position $i\in E$. Then there exists at most one $k\in [e+1]$ such
that $i\not\in E_k$. By pegion hole principle, there exists $k\in [e+1]$ such
that $E\subseteq E_k$. We also observe that since $\alpha_k\neq 0$, $j\in E_k$.
Thus $d(x+\alpha_k,v)>d(x,v)$, contradicting the choice of $x$. This proves the
lemma.   
\end{proof}

Next we prove a result about lines with respect to linear codes. The result was
proved in \cite{ligero} for the case $\HH=\FF$ and $e<d/4$. The authors in
\cite{ligero} conjectured the result for $e<d/3$. Here we prove the result for
$e<d/3$ when $\FF$ can be an arbitrary subfield of $\HH$.

\begin{lemma}[Affine Line]\label{lem:affineline}
Let $L$ be the linear code as defined. Define an affine line $\ell_{u,v}$ in $\HH^n$ as $\ell_{u,v} := \{u + \alpha v:\alpha\in \FF\}$ for $u,v\in \HH^n$. Then for $e < d/3$ and any affine line $\ell_{u,v}$ we have:
\begin{enumerate}[{\rm (i)}]
\item $d(x,L)\leq e$ for all $x\in \ell_{u,v}$, or
\item $d(x,L)> e$ for at most $d$ points in $\ell_{u,v}$.
\end{enumerate}
\end{lemma}
\begin{proof}
%\nnote{Candidate for Appendix: Putting it here just to have all content for the moment}
The above is equivalent to
proving that if there exist $d+1$ distinct points $X =
\{x_1,\ldots,x_{d+1}\}\subseteq \ell_{u,v}$ such
that $d(x_i,L)\leq e$ for all $i\in [d+1]$, then $d(x,L)\leq e$ for all $x\in
\ell_{u,v}$. Assume that there exists such a set $X$ of $d+1$ points. Let $\ell_i$ denote the point in $L$ closest to $x_i$ for $i\in
[d+1]$. Set $\eta_i=x_i-l_i$ for all $i$ and let
$\bm{\eta}=\{\eta_1,\ldots,\eta_{d+1}\}$. By assumption we have
$\wt{\eta_i}\leq e$ for all $i\in [d+1]$. Since $\ell_{u,v}$ is contained in affine
span of any two distinct points on it, we have tuples
$\{(\alpha_i,\beta_i)\}_{i\in [d+1]}d$ such that $\alpha_i + \beta_i=1$ and $x_i=\alpha_ix_1 +
\beta_ix_2$ for $i\in [d+1]$. Note that $(\alpha_1,\beta_1)=(1,0)$ and
$(\alpha_2,\beta_2)=(0,1)$. Observe that $\alpha_i$'s and $\beta_i$'s must be
distinct for all $i\in [d+1]$. We call $X$ as {\em degenerate} if there exist
$i\neq j$ satisfying $\alpha_j=\gamma\alpha_i$ and $\beta_j=\gamma\beta_j$ for
some $\gamma\in \FF\backslash {0}$. We consider two cases:

\noindent{\em $X$ is degenerate}: Degeneracy implies there exist $i\neq j$ such that $x_i=\gamma x_j$ for
some $\gamma\in \FF\backslash\{0\}$. In this case we have $0 =
\frac{1}{1-\gamma}x_i -\frac{\gamma}{1-\gamma}x_j\in \ell_{u,v}$. This implies
$\ell_{u,v}=\{\alpha x_i: \alpha\in \FF\}$
and hence $d(x,L)=d(x_i,L)\leq e$ for all $x\in \ell_{u,v}$. Thus the statement
of the Lemma holds in this case.\smallskip 

\noindent{\em X is not degenerate}: We first prove
that $\ell_i=\alpha_i\ell_1+\beta_i\ell_2$ and
$\eta_i=\alpha_i\eta_1+\beta_i\eta_2$ for
all $i\in [d+1]$. We have
\begin{align*}
\ell_i+\eta_i = x_i &= \alpha_ix_1 + \beta_ix_2 \\
    &= \alpha_i(\ell_1+\eta_1) + \beta_i(\ell_2 + \eta_2) \\
    &= (\alpha_i\ell_1 + \beta_i\ell_2) + (\alpha_i\eta_1 + \beta_i\eta_2)
\end{align*}
Rearranging we have,
\begin{equation*}
\ell_i - (\alpha_i\ell_1 + \beta_i\ell_2) = \alpha_i\eta_1 +
\beta_i\eta_2 - \eta_i
\end{equation*}
In the above equation we see that LHS is a vector in $L$. Further
$\wt{\alpha_i\eta_1 + \beta_i\eta_2 - \eta_i}\leq
\wt{\eta_1}+\wt{\eta_2}+\wt{\eta_i}\leq 3e < d$. Thus the LHS must be equal to
$0$ and hence $\ell_i = \alpha_i\ell_1 + \beta_i\ell_2$ and
$\eta_i=\alpha_i\eta_1+\beta_i\eta_2$. Observe that any $x^\ast\in \ell_{u,v}$ can
be written as $x^\ast=\alpha^\ast x_1+\beta^\ast x_2$. Thus
$d(x^\ast,L)=d(\alpha^\ast x_1+\beta^\ast x_2,L)\leq \wt{\alpha^\ast
\eta_1+\beta^\ast \eta_2}\leq |E|$ where $E$ denotes the set
$\Delta(\eta_1,0)\cup \Delta(\eta_2,0)$. Our final effort will be to show that
$|E|\leq e$.

\noindent{\it Claim}: $|E|\leq e$ where $E = \Delta(\eta_1,0)\cup
\Delta(\eta_2,0)$. We consider the partition of $E$ into sets
$E_0=\Delta(\eta_1,0)\backslash \Delta(\eta_2,0)$,
$E_1=\Delta(\eta_2,0)\backslash \Delta(\eta_1,0)$ and
$E_{01}=\Delta(\eta_1,0)\cap \Delta(\eta_2,0)$. Let $t=|E|$. Consider a $t\times (d+1)$ matrix $M=(m_{ij})$
where $m_{ij}=0$ if $j^{th}$ coordinate ($\eta_i^j$) of $\eta_i$ is zero, and $m_{ij}=1$
otherwise. We will show that each row of $M$ has at most one $0$. Assume that
there exists $i$ such that $m_{ip}=0$ and $m_{iq}=0$ for $p\neq q$. We consider
three cases:
\begin{itemize}
\item If $i\in E_0$, the above condition implies $\alpha_p\eta_1^i =
\alpha_q\eta_1^i=0$, or $\alpha_p=\alpha_q=0$ as $\eta_1^i\neq 0$ for $i\in E_0$.
This contradicts the fact that $X$ is not degenerate.
\item The case $i\in E_1$ is similar to above.
\item If $i\in E_{01}$ we have
$\alpha_p\eta_0^i+\beta_p\eta_1^i=\alpha_q\eta_0^i+\beta_q\eta_1^i=0$ which
implies $\alpha_p/\beta_p=\alpha_q/\beta_q=-\eta_1^i/\eta_0^i$, or
$\alpha_p/\alpha_q = \beta_p/\beta_q$ which contradicts the fact that $X$ is
not degenerate. Note that all denominators can be argued to be non-zero for $i\in E_{01}$. 
\end{itemize}

From the above, we conclude that each row has at least $d$ entries as $1$.
Counting by columns, we have that each column has at most $e$ entries as $1$
(since $\wt{\eta_i}\leq e$ for all $i\in [d]$. Comparing the lower and upper
bounds on the number of $1$ entries in the matrix we have $td \leq e(d+1)$
which implies $t \leq e + e/d < e + 1$. Thus $t\leq e$, as we wanted to show.
This completes the proof.

\end{proof}

The following result underlies proximity protocols in our work and in
\cite{ligero}. Intuitively the result states that if a matrix is far away from the code $\mc{C}$, a random linear combination of its rows is far away from a codeword in $L$, and thus the proximity of the matrix to $\mc{C}$ may be tested by testing the proximity of a random linear combination of its rows to $L$.

\begin{lemma}[Affine Subspace]\label{lem:affinesubspace}
Let the codes $L$ and $\mc{C}$ be as defined and $e<d/3$ be an integer. Let $U\in \HH^{m\times n}$ be a matrix such that $d(U,\mc{C})>e$. Then for any $u_0\in \HH^n$, $\prob{d(u_0+r^TU,L)\leq e}\leq d/|\FF|$ for uniformly sampled $r\sample \FF^m$.
\end{lemma}
\begin{proof}
From Lemma \ref{lem:farpoint}, there exists $u\in \aff{u_0}{U}$ such that $d(u,L)>e$. Now we can write $\aff{u_0}{U}$ as union of affine lines passing through $u$. Applying lemma \ref{lem:affineline} to each line, we see that at most $d$ points $x$ on each affine line satisfy $d(x,L)\leq e$. Thus, a randomly sampled point $x$ in $\aff{u_0}{U}$, equivalently obtained as $u_0+r^TU$ for a randomly sampled vector $r\in \FF^m$ satisfies $d(x,L)$ with probability at most $d/|\FF|$.
\end{proof}

We are now in a position to prove Proposition \ref{lem:3dcompression}.
\begin{proof}[Proof of Proposition \ref{lem:3dcompression}]
Let $\HH$ denote the field $\FF^m$. Then $u_0$ can be viewed as a point in
$\HH^n$. Similarly $U$ can be viewed as $p\times n$ matrix over $\HH$.
We consider $\mc{C}$ as $[n,k,d]$ code over $\HH$ and $\mc{C}^p$ as the
interleaved code of $\mc{C}$ over the field $\HH$. Then by applying Lemma \ref{lem:affinesubspace}
with $\HH=\FF^m$ and codes $\mc{C}$ and $\mc{C}^p$ in place of codes $L$ and
$\mc{C}$, we have the desired bound.
\end{proof}

\subsection{Inner-product Arguments}
We define an interactive protocol that allows proving inner-product relation over committed values, where the commitment corresponds to a non-interactive perfectly hiding and computationally binding commitment scheme. We start with a definition of such a commitment scheme. 
\begin{definition}\label{defn:commscheme}
 A pair of $\ppt$ algorithms $(\gen,\com)$ constitute a non-interactive commitment scheme if $\sigma\sample \gen(\secparam)$ consists of description of sets $\calM_\sigma$ (message space), $\calR_\sigma$ (randomness space), $\calC_\sigma$ (commitment space) and an efficiently computable function $\com_\sigma: \calM_\sigma\times \calR_\sigma\rightarrow \calC_\sigma$ which is {\em hiding} and {\em binding} as defined later.
\end{definition}

For $x\in \calM_p$ \commentA{p or $\sigma?$}, we generate a {\em commitment} of $x$ as $\com_\sigma(x,r)$ where $r\sample \calR_p$ \commentA{p or $\sigma?$; there are occurrences of $p$ below} is drawn uniformly at random. For ease of notaion, we simply use $\com$ instead of $\com_\sigma$ and use $\com(x)$ to denote the random variable corresponding to commitment of $x$. 

\begin{definition}[Hiding Commitment]\label{defn:hidingcomm}
A commitment scheme $(\gen,\com)$ is called perfectly {\em hiding}  if for all $\ppt$ \commentA{for perfect hiding, the adversary is no longer $\ppt$} adversaries $\adv$, the following probability is negligibly close to $1/2$ \commentA{for perfect, it is not negligible close to 1/2, it is 1/2; also not that if you want define this as statistical, the statistical sec parameter needs to be distinguished from computational parameter $\lambda$ }:
\begin{align*}
\condprob{b=b'}{
\begin{array}{l}
\sigma\sample \gen(\secparam); \\
(x_0,x_1)\in \calM^2_p\sample \adv(\sigma); \\
b\sample \bitset; c\sample \com(x_b);\\
b'\sample \adv(\secparam,\sigma,c)
\end{array}
}
\end{align*}
\end{definition}

\begin{definition}[Binding Commitment]\label{defn:bindingcomm}
A commitment scheme $(\gen,\com)$ is called {\em binding} if for all $\ppt$ adversaries $\adv$, 
\begin{align*}
\condprob{\com_p(x_0,r_0)=\com_p(x_1,r_1)\wedge x_0\neq x_1}{
\begin{array}{l}
\sigma\sample \gen(\secparam) \\
x_0,x_1,r_0,r_1 \sample \adv(\secparam,\sigma)
\end{array}
} < \negl(\lambda)
\end{align*}

\end{definition}

We will assume that all the message spaces $\calM_\sigma$ output by the $\gen$ algorithm come equipped with an inner-product operator $\innp{.}{.}:\calM_\sigma\times \calM_\sigma\rightarrow Z$. We define the language $\calL_\sigma\subseteq \calC_\sigma\times \calC_\sigma\times Z$ as:
\begin{equation*}
\calL_\sigma = \{(c_1,c_2,v):\exists x_1,x_2,r_1,r_2 \text{ s.t. }
c_1=\com(x_1,r_1), c_2=\com(x_2,r_2) \text{ and } \innp{x_1}{x_2}=v\} 
\end{equation*}

The $\NP$ relation $\calR_\sigma$ for the language $\calL_\sigma$ consists of pairs $(\stmt,\wit)$ with $\stmt=(c_1,c_2,v)$ and $\wit=(x_1,x_2,r_1,r_2)$ such that $c_1=\com(x_1,r_1)$, $c_2=\com(x_2,r_2)$ and $\innp{x_1}{x_2}=v$ 

\begin{definition}[Inner-product Argument]\label{defn:innerproductarg}
We call an interactive protocol $\innp{\pip}{\vip}$ consisting of $\ppt$ interactive algorithms $\pip$ and $\vip$ an inner-product argument for commitment scheme $(\gen,\com)$ if it recognizes the language $\calL_\sigma$ as defined previously. Namely, $\innp{\pip}{\vip}$ satisfies the following: \commentA{what do  you mean by adversary picking an instance uniformly at random? what does $\sample$ mean? adversary may have arbitrary stategy}
\begin{enumerate}[{\rm (i)}]
\item {\bf Completeness}: For all adversaries $\adv$,
\begin{align*}
\condprob{(\stmt,\wit)\in \calR_\sigma \vee \langle \pip(\sigma,\stmt,\wit),\vip(\sigma,\stmt)\rangle=1}{
\begin{array}{l}
\sigma\sample \gen(\secparam);\\
(\stmt,\wit)\sample \adv(\secparam,\sigma)
\end{array}
}=1
\end{align*}

\item{\bf Soundness}: For all deterministic \commentA{why deterministic? any reason to be so?}polynomial time $\prover^*$ and $\ppt$ adversaries $\adv$:
\begin{align*}
\condprob{\stmt\not\in \calL_\sigma \wedge \langle
\prover^*(\sigma,\stmt,s),\vip(\sigma,u)\rangle=1}{
\begin{array}{l}
\sigma\sample \gen(\secparam);\\
(\stmt,s)\sample \adv(\secparam,\sigma)
\end{array}
} = \negl(\lambda)
\end{align*}
\commentA{no zero-knowledge  property needed for the above definition?}
\end{enumerate}

\end{definition}

We mention some concrete instantiations of  commitment schemes and corresponding inner-product arguments that we use in our protocol.

\noindent{\em Logarithmic Inner-product Argument}: In this setting, we have $\calM_\sigma=\bbZ^n_p$, $\calR_\sigma=\bbZ_p$, $\calC_\sigma=\bbG$ where $\bbG$ is group of prime order $p$. The algorithm $\gen$ samples generators $g_1,\ldots,g_n$, $h$ $\sample \bbG$. The commitment is a Pederson vector commitment given by $\com({\bf x } ,r ) = h^r \cdot \prod_{i=1}^n {g_i}^{x_i}$ where ${\bf x}=(x_1,\ldots,x_n)$. We use the inner-product argument $(\piplog,\viplog)$ from Bootle et. al in \cite{bulletproofs} for the commitment scheme $(\gen,\com)$. The interactive protocol $(\piplog,\viplog)$ is a $O(\log n)$ round protocol with argument size $O(\log n)$. Time complexity of the verifier $\viplog$ is given by $t(\viplog)=O(n).\bbZ_p + O(n).\bbG$.\smallskip


\noindent{\em Square-root Inner-product Argument}: In this setting we use the same commitment scheme as above. For the inner product argument we use the interactive protocol $(\pipsq,\vipsq)$ from \cite{InnerProductDLS} or \cite{Groth09b}. The construction in \cite{InnerProductDLS} gives a $5$-move \commentA{move=round? better to stick to one term; i prefer round} protocol with total communication complexity $O(\sqrt{n})$. The construction in \cite{Groth09b} gives a 7-move protocol with $O(\sqrt{n})$ communication complexity. In both the constructions $t(\vipsq) = O(n).\bbZ_p +O(\sqrt{n}).\bbG$ \commentA{something wrong in the expression.  you mean size of the group/field? same for the earlier argument. secondly, why would anyone be using the latter which has more round complexity? Just say that the second paper proposes a construction same argument size and verifier complexity, yet with 2 more number of rounds}.

\subsection{Forking Lemma and Knowledge Soundness}
We use the Forking Lemma from \cite{InnerProductDLS,bulletproofs} to describe 
expected polynomial time knowledge extractors for our protocols. Let
$(\prover,\verifier)$ be a public coin $(2\mu+1)$-move interactive protocol with
challenges $x_1,\ldots,x_\mu$ in sequence. Let $n_i\geq 1$ for $1\leq i\leq
\mu$. We call a collection of $n=\prod_{i=1}^\mu n_i$ accepting transcripts to be
$(n_1,\ldots,n_\mu)$-tree of accepting transcripts, if the challenges are
organized in the tree format that we describe now: \commentA{the following didn't make sense. what does a path from root to a leaf node signify? one sequence of challenges for an accepting execution? at any rate this portion needs a clean write-up} The root of the tree is
labelled with the statement being proved. Each node of depth $i<\mu$ has exactly
$n_i$ children, each labelled with a distinct value of the $i^{th}$ challenge
$x_i$. We call a $\ppt$ algorithm $\chi$ to be a {\em witness extraction
algorithm} if $\chi$ can extract a witness $w$ to the statement, given an
appropriate tree of accepting transcripts. This can be seen as a generalization
of the notion of special soundness for Sigma protocols with $n=2$ and $\mu=1$.

\begin{definition}[Argument of Knowledge]\label{def:argofknowledge}
Let $(\setup,\prover,\verifier)$ be an public coin interactive protocol. We say that
$(\setup,\prover,\verifier)$ is an {\em argument of knowledge} for the language
$\mc{L}$ if for every $\ppt$
prover $P^\ast$, there exists an expected polynomial time extractor $\extr$ such that for all $x$:
%\begin{comment}
\small
\begin{align}
\condprob{(x,w)\in \mc{L}}{
\begin{array}{c}
\sigma\sample\setup(\secparam) \\
w\sample \extr^{\mc{O}}(x,\sigma)
\end{array}
}
\geq \condprob{\langle
P^\ast(x,\sigma),\verifier(x,\sigma)\rangle=1}{\sigma\sample \setup(\secparam)} -
\negl(\lambda)
\end{align}
%\end{comment}
%\begin{align}
%&\prob{(x,w)\in\mc{L}| \sigma \sample \setup(\secparam); w\sample \extr^{\mc{O}}(x,\sigma)}\\
%\geq & \prob{\innp{P^*(x,\sigma)}{\verifier(x, \sigma)} = 1|\sigma\sample \setup(\secparam)} - \kappa(\lambda)
%\end{align}
for some negligible function $\negl$. In the above $\mc{O}$ denotes the transcript oracle  $\langle P^\ast(x,\sigma)$ ,$\verifier(x,\sigma)\rangle$which can be rewound to any previous state, and resumed with fresh randomness for the
verifier $\verifier$.
\end{definition}

Our argument of knowledge proofs rely on the following result from \cite{bulletproofs}.
While the result is originally stated and proved for showing witness-extended
emulation, we restate it for the case of argument of knowledge. The proof in
\cite{InnerProductDLS} also applies to this restricted case.

\begin{lemma}[Forking Lemma,\cite{bulletproofs}]\label{lem:forkinglemma}
Let $(\setup,\prover,\verifier)$ be a $(2\mu+1)$-move public coin interactive
protocol. Let $\chi$ be a $\ppt$ witness extraction algorithm that outputs a
witness with probability $1-\negl(\secpar)$ from an $(n_1,\ldots,n_\mu)$-tree
of accepting transcripts. Assuming that $\prod_{i=1}^\mu n_i$ is bounded by a
polynomial in security parameter $\secpar$, the protocol
$(\setup,\prover,\verifier)$ is an argument of knowledge.
\end{lemma}

.\commentA{change 'move' to 'round' in all the above text.}
 
%--------------------------------Protik's prelims--------------------------------
%\subsection{Codes}
%\paragraph{\textbf{Reed-Solomon Code:}} For positive integers $n,k$, finite field $\mathbb{F}$, and a vector $\eta = (\eta_1,\cdots ,\eta_n) \in \mathbb{F}_n$ of distinct field elements, the code $RS_{\mathbb{F},n,k,\eta}$ is the $[n,k,n-k+1]$ linear code over $\mathbb{F}$ that consists of all $n$-tuples $(p(\eta_1),...,p(\eta_n))$ where $p(\cdot)$ is a polynomial of degree $< k$ over $\mathbb{F}$.
%\dnote{Comments for Protik:\\
%1. Have spaces between paragraphs even in Latex. The PDF generated by Latex is usually beautiful and readable. The tex file can't be beautiful but should atleast be as readable as the PDF.\\
%2. I have already told you many times, and I am not going to leave till you use it :) Use macro for any notation that you use more than two times throughout the paper. (You will realize the use of it the day we decide change some notation in the middle of writing a paper.. But it is also a good practice to do it in general).\\
%3.Use \ ldots for ...\\
%4. You could use subsection* or subsubsection* instead of having a paragraph and using a textbf inside. (try both, what looks better depends on the cls file you use).}
%\paragraph{\textbf{Interleaved Code:}} Let $L\subset \mathbb{F}_n$ be an $[n,k,d]$ linear code over $\mathbb{F}$. We let $L^m$ denote the $[n,mk,d]$ (interleaved) code over $\mathbb{F}^m$ whose code words are all $m\times n$ matrices $U$ such that every row $U_i$ of $U$ satisfies $U_i\in L$. For $U\in L^m$ and $j\in[n]$, we denote by $U[j]$ the $j^{th}$ symbol (column) of $U$.
%\dnote{Have "th" in jth outside math mode.}
%
%\subsection{Interactive Oracle Proofs} The Interactive Oracle Proofs is the notion which combine both Interactive Proofs and Probabilistically Checkable Proofs, and also generalize the notion of the Interactive PCPs.
%\paragraph{} A $k$-round public-coin IOP has $k$ rounds of interaction. In the $i^{th}$ round of interaction, the verifier sends a uniformly random message $m_i$ to the prover; then the prover replies with a message $\pi_i$ to the verifier. After $k$ rounds of interaction, the verifier makes some queries to the oracles it received and either accepts or rejects.
%\dnote{Have at least the main definitions in the Definition environment.}
%\paragraph{} An IOP system for a relation $\mathcal{R}$ with round complexity $k$ and soundness error $\epsilon$ is a pair $(P, V )$, where $P, V$ are probabilistic algorithms, that satisfies the following properties:
%\paragraph{\textit{Completeness:}}  For every instance-witness pair $(x,w)$ in the relation $\mathcal{R}, (P (x, w), V (x))$ is a $k(n)$-round interactive oracle protocol with accepting probability 1.
%\paragraph{\textit{Soundness:}} For every instance $x \notin \mathcal{L(R)}$ and unbounded malicious prover $P^*, (P^*, V (x))$ is a $k(n)$-round interactive oracle protocol with accepting probability at most $\epsilon(n)$.
%\subsection{Zero-Knowledge} 
%\paragraph{\textbf{Interactive Argument Systems:}} A pair of PPT(Probabilistic Polynomial Time) interactive machines $<P, V>$ is called an interactive proof system for a language $\mathcal{L}$ if there exists a negligible function $negl(\cdot)$ such that the following two conditions hold:
%\dnote{1. have space before a ( or any other bracket.\\
%2. Use \ langle and \ rangle instead of $<$ and $>$ when using it as brackets.
%3. You can have negl in mathsf, it would look better (have a macro for this too!)}
%\begin{itemize}
%	\item[(1)] \textit{Completeness:} For every $x\in \mathcal{L}$ there exists a string $w$ such that for every $z \in \{0,1\}^*$,
%$Pr[<P(x,w),V(x,z)>=1] \geq 1-negl(|x|)$.
%	\item[(2)] \textit{Soundness:} For every $x \notin \mathcal{L}$, every interactive PPT machine $P^*$, and every $w,z\in \{0,1\}^*$ $Pr[<P^*(x,w),V(x,z)>=1]\leq negl(|x|)$ 
%\end{itemize}
%\paragraph{\textbf{Zero Knowledge:}} Let$<P,V>$ be an interactive proof system for some language $\mathcal{L}$. We say that $<P,V>$ is computational zero-knowledge with respect to an auxiliary input if for every PPT interactive machine $V^*$ there exists a PPT algorithm $S$, running in time polynomial in the length of its first input, such that $\{<P(x,w),V^*(x,z)>\}_{x\in \mathcal{L},w\in \mathcal{R}_x,z\in \{0,1\}^*}\approx_c \{<S(x,z)>\}_{x\in\mathcal{L},z\in\{0,1\}^*}$
%\subsection{Commitment schemes} 
%\paragraph{\textbf{Commitemnts:}} A non-interactive commitment scheme consists of a pair of probabilistic polynomial time algorithms $(Setup,Com)$. The setup algorithm $pp\leftarrow Setup(1^{\lambda})$ generates public parameters $pp$ for the scheme, for security parameter $\lambda$. The commitment algorithm $Com_{pp}$ defines a function $M_{pp} \times R_{pp} \rightarrow C_{pp}$ for message space $M_{pp}$, randomness space $R_{pp}$ and commitment space $C_{pp}$ determined by $pp$. For a message $x\in M_{pp}$, the algorithm draws $\delta \in_R  R_{pp}$ uniformly at random, and computes commitment $\com = Com_{pp}(x; \delta)$.\\
%For ease of notation we write $Com = Com_{pp}$.
%\dnote{All algorithm names in mathsf (macro for each). Eg. in the above paragraph, Setup, Com, ...}
%\paragraph{\textbf{Homomorphic Commitment:}} A homomorphic commitment scheme is a non-interactive commitment scheme such that $M_{pp},R_{pp}$ and $C_{pp}$ are all abelian groups, and for all $x_1,x_2 \in M_{pp}, \delta_1,\delta_2 \in R_{pp}$, we have $Com(x_1; \delta_1) + Com(x_2; \delta_2) = Com(x_1 + x_2; \delta_1 + \delta_2)$
%\dnote{Hiding and binding are the core properties of a commitment scheme, i.e., a definition of a commitment scheme includes the properties of hiding and binding. So bring them first. And just mention ``Hiding'' and ``Binding''.}
%\paragraph{\textbf{Hiding Commitment:}} A commitment scheme is said to be hiding if for all PPT adversaries $\Adv$ there exists a negligible function $\mu(\lambda)$ such that
%$$ |Pr[b=b'|pp\leftarrow Setup(1^{\lambda}); (x_0,x_1)\in M^2_{pp}\leftarrow \Adv(pp), b\in_R\{0,1\}, \delta \in_R R_{pp}, \com=Com(x_b;\delta), b'\leftarrow \Adv(pp,com)]-\frac{1}{2}|\leq \mu(\lambda)$$
%\paragraph{\textbf{Binding Commitment:}} A commitment scheme is said to be binding if for all PPT adversaries $\Adv$ there exists a negligible function $\mu$ such that 
%$$Pr[Com(x_0;\delta_0)=Com(x_1,\delta_1) \wedge x_0\neq x_1| pp\leftarrow Setup(1^{\lambda})x_0,x_1,\delta_0, \delta_1\leftarrow \Adv(pp)]\leq \mu(\lambda)$$
%\dnote{Have a subsection* with Pedersen commitment, and have the vector commitment also in the same part.}
%\paragraph{\textbf{Pedersen Commitment:}} $M_{pp}, R_{pp} = \mathbb{Z}_p, C_{pp} = \mathbb{G}$ of order $p$.\\
%$Setup : g, h \in_R \mathbb{G}$\\
%$Com(x,\delta)=(g^xh^{\delta})$
%\paragraph{\textbf{Pedersen Vector Commitment:}} $M_{pp}= \mathbb{Z}^n_p , R_{pp} = \mathbb{Z}_p, C_{pp}= \mathbb{G}$ with G of order p.\\ 
%$Setup: \vc{g}=(g_1,\cdots,g_n),h \in_R \mathbb{G}$\\
%$Com(\vc{x} = (x_1,\cdots,x_n);\delta) = h^r\vc{g}^{\vc{x}} = h^r \prod\limits_i g_i^{x_i} \in \mathbb{G}$
