\section{Preliminaries}
\subsection{Linear Codes}
\begin{definition}\label{defn:lincode}
For positive integers $n,k$ and a finite field $\FF$, a $k$-dimensional
subspace $L$ of $\FF^n$ is called an $[n,k]$ linear code. Elements of $L$ are
conventionally called {\em codewords}. 
\end{definition}

For codewords $x,y\in L$ where $x=(x_1,\ldots,x_n)$ and $y=(y_1,\ldots,y_n)$ we
define the hamming distance $\dham(x,y)=|\{i\in [n]: x_i\neq y_i\}|$. It is
easily checked that $\dham$ defines a metric on $L$. The minimum distance of
the code $L$, denoted by $\delta(L)$ is defined as $\min\{\dham(x,y):x,y\in L,
x\neq y\}$.

For an $[n,k]$ code $L$, a $n\times k$ matrix $\mc{G}$ is called a {\em generator
matrix} iff (i) $\mc{G} x\in L$ for all $x\in\FF^k$ and (ii) $\mc{G} x\neq
\mc{G} y$ for $x\neq
y$. Clearly, such a matrix $\mc{G}$ has rank $k$. Similarly an $n\times (n-k)$
matrix $\mc{H}$ such that $y^T\mc{H}=0$ for all $y\in L$ is called a {\em parity check}
matrix for $L$. It is easily seen that the above two matrices exist for any
$[n,k]$ linear code $L$. We will assume that description of the linear code $L$
includes a generator matrix $\mc{G}$ and a parity check matrix $\mc{H}$.

\begin{definition}[Interleaved Code]\label{defn:interleavedcode}
For an $[n,k]$-linear code $L$ and a positive integer $m$, we define a {\em row
interleaved code} $\ric{L}{m}$ to be the set of $m\times n$ matrices $A$ such that
each row of $A$ is a codeword in $L$. Similarly, we define a {\em column
interleaved code} $\cic{L}{m}$ to be the set of $n\times m$ matrices $B$ such
that each column of $B$ is a codeword in $L$.
\end{definition}

For a linear $[n,k]$-code $L$ over the field $\FF$, we observe that
$\ric{L}{m}$ forms an $[n,k]$-code over the field $\FF^m$ by viewing each
column of the codeword $A\in \ric{L}{m}$ as a symbol in the field $\FF^m$.
Similarly, $\cic{L}{m}$ forms an $[n,k]$ code over $\FF^m$ by viewing each row
of the codeword $B\in \cic{L}{m}$ as a symbol in $\FF^m$. For $A,A'\in
\ric{L}{m}$, we define the distance $\dham(A,A')=|\{i\in [n]: A[.,i]\neq
A'[.,i]\}|$ where the notation $X[.,i]$ denotes the $i^{th}$ column of the
matrix $X$. Similarly for $B,B'\in \cic{L}{m}$ we define $\dham(B,B')=|\{i\in
[n]: B[i,.]\neq B'[i,.]\}|$.
 
\begin{definition}[Product Code]\label{defn:productcode}
Let $L_i$ be an $[n_i,k_i]$-linear code for $i=1,2$. We define the product code
$L_1\oplus L_2$ to be the code consisting of $n_2\times n_1$ matrices $A$ such
that each row of $A$ is a codeword in $L_1$ and each column of $A$ is a
codeword in $L_2$. 
\end{definition}

Note that by definition, the product code $L_1\oplus L_2$ is a row interleaved
code of $L_1$ and a column interleaved code of $L_2$, i.e $L_1\oplus L_2 =
\ric{L_1}{n_2}\cap \cic{L_2}{n_1}$. For $A,A'\in L_1\oplus L_2$, we define
$\dham_1(A,A')=|\{i\in [n_1]: A[.,i]\neq A'[.,i]\}|$ and $\dham_2(A,A')=|\{i\in
[n_2]: A[i,.]\neq A'[i,.]\}|$. The distance $\dham_1$ corresponds to distance
function of the code $\ric{L}{n_2}$, where we view $A,A'$ as codewords in
$\ric{L}{n_2}$. Similarly, the distance $\dham_2$ corresponds to the distance
function of the code $\cic{L}{n_1}$.

\begin{definition}[Reed Solomon Code]\label{defn:rscode}
An $[n,k]$-Reed Solomon Code $L\subseteq \FF^n$ consists of vectors
$(p(\eta_1),\ldots,p(\eta_n))$ for polynomials $p\in \FF[x]$ of degree less
than $k$ where $\eta_1,\ldots,\eta_n$ are distinct points in $\FF$. We will use
$\rsc{\eta}{k}$ to denote the Reed Solomon code with
$\bm{\eta}=(\eta_1,\ldots,\eta_n)$ and $deg(p)<k$.
\end{definition}
   
The following lemma will be useful to us later. Intuitively it states that for
a code $L$, if a
matrix is ``far'' from an interleaved code of $L$, then a random linear combination of its
rows is also likely to be far from the code $L$. 

\begin{lemma}\label{lem:proximitytest}
Let $L$ be an $[n,k]$ linear code and let $m$ be a positive integer. Let
$U^\ast\in \FF^{m\times n}$ be such that $\dham(U^\ast,\ric{L}{m})>e$ for
$e<\delta(L)/3$. Then $\prob{\dham(r^TU^\ast,L)\leq e}<\delta(L)/|\FF|$ where
$r$ is sampled uniformly from $\FF^m$. 
\end{lemma}
The above lemma is proved in \cite{Ligero2017} for $e=\delta(L)/4$. We provide
a self-contained proof of the above for $e=\delta(L)/3$ in Appendix.


\subsection{Inner Product Arguments}
We define an interactive protocol that allows proving inner product relation
over committed values. 
\begin{definition}\label{defn:commscheme}
 A pair of $\ppt$ algorithms
$(\csetup,\comm)$ constitute a non-interactive commitment scheme if
$\sigma\sample \csetup(\secparam)$ consists of description of sets
$\mc{M}_\sigma$ (message space), $\mc{R}_\sigma$ (randomness space),
$\mc{C}_\sigma$ (commitment space) and an efficiently computable function 
$\comm_\sigma: \mc{M}_\sigma\times \mc{R}_\sigma\rightarrow \mc{C}_\sigma$ which is {\em hiding} and {\em binding} as defined later.
\end{definition}

For $x\in \mc{M}_p$, we generate a {\em commitment} of $x$
as $\comm_\sigma(x,r)$ where $r\sample \mc{R}_p$ is drawn uniformly at random. For ease
of notaion, we simply use $\comm$ instead of $\comm_\sigma$ and use $\comm(x)$ to denote the random variable corresponding to commitment of $x$. 

\begin{definition}[Hiding Commitment]\label{defn:hidingcomm}
A commitment scheme $(\csetup,\comm)$ is called {\em hiding} (perfectly) if for
all $\ppt$ adversaries $\adv$, the following probability is negligibly close to
$1/2$:
\begin{align*}
\condprob{b=b'}{
\begin{array}{l}
\sigma\sample \csetup(\secparam); \\
(x_0,x_1)\in \mc{M}^2_p\sample \adv(\sigma); \\
b\sample \{0,1\}; c\sample \comm(x_b);\\
b'\sample \adv(\sigma,c)
\end{array}
}
\end{align*}
\end{definition}

\begin{definition}[Binding Commitment]\label{defn:bindingcomm}
A commitment scheme $(\csetup,\comm)$ is called {\em binding} if for all $\ppt$
adversaries $\adv$, 
\begin{align*}
\condprob{\comm_p(x_0,r_0)=\comm_p(x_1,r_1)\wedge x_0\neq x_1}{
\begin{array}{l}
\sigma\sample \csetup(\secparam) \\
x_0,x_1,r_0,r_1 \sample \adv(\sigma)
\end{array}
} < \negl
\end{align*}

\end{definition}

We will assume that all the message spaces $\mc{M}_\sigma$ output by the
$\csetup$ algorithm come equipped with an inner product operator
$\innp{.}{.}:\mc{M}_\sigma\times \mc{M}_\sigma\rightarrow \ZZ$. We define
the language $\mc{L}_\sigma\subseteq \mc{C}_\sigma\times \mc{C}_\sigma\times
\ZZ$ as:
\begin{equation*}
\mc{L}_\sigma = \{(c_1,c_2,v):\exists x_1,x_2,r_1,r_2 \text{ s.t. }
c_1=\comm(x_1,r_1), c_2=\comm(x_2,r_2) \text{ and } \innp{x_1}{x_2}=v\} 
\end{equation*}

The $\npol$ relation $\mc{R}_\sigma$ for the language $\mc{L}_\sigma$ consists
of pairs $(u,w)$ with $u=(c_1,c_2,v)$ and $w=(x_1,x_2,r_1,r_2)$ such that
$c_1=\comm(x_1,r_1)$, $c_2=\comm(x_2,r_2)$ and $\innp{x_1}{x_2}=v$ 

\begin{definition}[Inner Product Argument]\label{defn:innerproductarg}
We call an interactive protocol $(\pip,\vip)$ consisting of $\ppt$ interactive algorithms
$\pip$ and $\vip$ an inner product argument for commitment scheme
$(\csetup,\comm)$ if it recognizes the language $\mc{L}_\sigma$ as defined
previously. Namely, $(\pip,\vip)$ satisfies the following:
\begin{enumerate}[{\rm (i)}]
\item {\bf Completeness}: For all adversaries $\adv$,
\begin{align*}
\condprob{(u,w)\not\in \mc{R}_\sigma \vee \langle \pip(\sigma,u,w),\vip(\sigma,u)\rangle={\tt accept}}{
\begin{array}{l}
\sigma\sample \csetup(\secparam);\\
(u,w)\sample \adv(\sigma)
\end{array}
}=1
\end{align*}

\item{\bf Soundness}: For all deterministic polynomial time $\mc{P}^\ast$ and
$\ppt$ adversaries $\adv$:
\begin{align*}
\condprob{u\not\in \mc{L}_\sigma \wedge \langle
\mc{P}^\ast(\sigma,u,s),\vip(\sigma,u)\rangle={\tt accept}}{
\begin{array}{l}
\sigma\sample \csetup(\secparam);\\
(u,s)\sample \adv(\sigma)
\end{array}
} = \negl
\end{align*}
\end{enumerate}
\end{definition}

We mention some concrete instantiations of commitment schemes and corresponding
inner product arguments that we use in our protocol.

\noindent{\em Logarithmic Inner Product Argument}: In this setting we have
$\mc{M}_\sigma=\ZZ^n_p$, $\mc{R}_\sigma=\ZZ_p$, $\mc{C}_\sigma=\GG$ where $\GG$
is group of prime order $p$. The algorithm $\csetup$ samples generators
$g_1,\ldots,g_n$, $h$ $\sample \GG$. The commitment is a pederson vector
commitment given by $\comm({\bf
x},r)=h^r.\prod_{i=1}^n {g_i}^{x_i}$ where ${\bf x}=(x_1,\ldots,x_n)$. We use
the inner product argument $(\piplog,\viplog)$ from Bootle et.al
in \cite{Bulletproofs} for the commitment scheme $(\csetup,\comm)$. The
interactive protocol $(\piplog,\viplog)$ is a $O(\log n)$ round protocol with
argument size $O(\log n)$. Time complexity of the verifier $\viplog$ is given
by $t(\viplog)=O(n).\ZZ_p + O(n).\GG$.\smallskip


\noindent{\em Square Root Inner Product Argument}: In this setting we use the
same commitment scheme as above. For the inner product argument we use the
interactive protocol $(\pipsq,\vipsq)$ from \cite{InnerProductDLS} or
\cite{Groth09b}. The construction in \cite{InnerProductDLS} gives a $5$-move
protocol with total communication complexity $O(\sqrt{n})$. The construction in
\cite{Groth09b} gives a 7-move protocol with $O(\sqrt{n})$ communication
complexity. In both the constructions $t(\vipsq) = O(n).\ZZ_p +
O(\sqrt{n}).\GG$.



%--------------------------------Protik's prelims--------------------------------
%\subsection{Codes}
%\paragraph{\textbf{Reed-Solomon Code:}} For positive integers $n,k$, finite field $\mathbb{F}$, and a vector $\eta = (\eta_1,\cdots ,\eta_n) \in \mathbb{F}_n$ of distinct field elements, the code $RS_{\mathbb{F},n,k,\eta}$ is the $[n,k,n-k+1]$ linear code over $\mathbb{F}$ that consists of all $n$-tuples $(p(\eta_1),...,p(\eta_n))$ where $p(\cdot)$ is a polynomial of degree $< k$ over $\mathbb{F}$.
%\dnote{Comments for Protik:\\
%1. Have spaces between paragraphs even in Latex. The PDF generated by Latex is usually beautiful and readable. The tex file can't be beautiful but should atleast be as readable as the PDF.\\
%2. I have already told you many times, and I am not going to leave till you use it :) Use macro for any notation that you use more than two times throughout the paper. (You will realize the use of it the day we decide change some notation in the middle of writing a paper.. But it is also a good practice to do it in general).\\
%3.Use \ ldots for ...\\
%4. You could use subsection* or subsubsection* instead of having a paragraph and using a textbf inside. (try both, what looks better depends on the cls file you use).}
%\paragraph{\textbf{Interleaved Code:}} Let $L\subset \mathbb{F}_n$ be an $[n,k,d]$ linear code over $\mathbb{F}$. We let $L^m$ denote the $[n,mk,d]$ (interleaved) code over $\mathbb{F}^m$ whose code words are all $m\times n$ matrices $U$ such that every row $U_i$ of $U$ satisfies $U_i\in L$. For $U\in L^m$ and $j\in[n]$, we denote by $U[j]$ the $j^{th}$ symbol (column) of $U$.
%\dnote{Have "th" in jth outside math mode.}
%
%\subsection{Interactive Oracle Proofs} The Interactive Oracle Proofs is the notion which combine both Interactive Proofs and Probabilistically Checkable Proofs, and also generalize the notion of the Interactive PCPs.
%\paragraph{} A $k$-round public-coin IOP has $k$ rounds of interaction. In the $i^{th}$ round of interaction, the verifier sends a uniformly random message $m_i$ to the prover; then the prover replies with a message $\pi_i$ to the verifier. After $k$ rounds of interaction, the verifier makes some queries to the oracles it received and either accepts or rejects.
%\dnote{Have at least the main definitions in the Definition environment.}
%\paragraph{} An IOP system for a relation $\mathcal{R}$ with round complexity $k$ and soundness error $\epsilon$ is a pair $(P, V )$, where $P, V$ are probabilistic algorithms, that satisfies the following properties:
%\paragraph{\textit{Completeness:}}  For every instance-witness pair $(x,w)$ in the relation $\mathcal{R}, (P (x, w), V (x))$ is a $k(n)$-round interactive oracle protocol with accepting probability 1.
%\paragraph{\textit{Soundness:}} For every instance $x \notin \mathcal{L(R)}$ and unbounded malicious prover $P^*, (P^*, V (x))$ is a $k(n)$-round interactive oracle protocol with accepting probability at most $\epsilon(n)$.
%\subsection{Zero-Knowledge} 
%\paragraph{\textbf{Interactive Argument Systems:}} A pair of PPT(Probabilistic Polynomial Time) interactive machines $<P, V>$ is called an interactive proof system for a language $\mathcal{L}$ if there exists a negligible function $negl(\cdot)$ such that the following two conditions hold:
%\dnote{1. have space before a ( or any other bracket.\\
%2. Use \ langle and \ rangle instead of $<$ and $>$ when using it as brackets.
%3. You can have negl in mathsf, it would look better (have a macro for this too!)}
%\begin{itemize}
%	\item[(1)] \textit{Completeness:} For every $x\in \mathcal{L}$ there exists a string $w$ such that for every $z \in \{0,1\}^*$,
%$Pr[<P(x,w),V(x,z)>=1] \geq 1-negl(|x|)$.
%	\item[(2)] \textit{Soundness:} For every $x \notin \mathcal{L}$, every interactive PPT machine $P^*$, and every $w,z\in \{0,1\}^*$ $Pr[<P^*(x,w),V(x,z)>=1]\leq negl(|x|)$ 
%\end{itemize}
%\paragraph{\textbf{Zero Knowledge:}} Let$<P,V>$ be an interactive proof system for some language $\mathcal{L}$. We say that $<P,V>$ is computational zero-knowledge with respect to an auxiliary input if for every PPT interactive machine $V^*$ there exists a PPT algorithm $S$, running in time polynomial in the length of its first input, such that $\{<P(x,w),V^*(x,z)>\}_{x\in \mathcal{L},w\in \mathcal{R}_x,z\in \{0,1\}^*}\approx_c \{<S(x,z)>\}_{x\in\mathcal{L},z\in\{0,1\}^*}$
%\subsection{Commitment schemes} 
%\paragraph{\textbf{Commitemnts:}} A non-interactive commitment scheme consists of a pair of probabilistic polynomial time algorithms $(Setup,Com)$. The setup algorithm $pp\leftarrow Setup(1^{\lambda})$ generates public parameters $pp$ for the scheme, for security parameter $\lambda$. The commitment algorithm $Com_{pp}$ defines a function $M_{pp} \times R_{pp} \rightarrow C_{pp}$ for message space $M_{pp}$, randomness space $R_{pp}$ and commitment space $C_{pp}$ determined by $pp$. For a message $x\in M_{pp}$, the algorithm draws $\delta \in_R  R_{pp}$ uniformly at random, and computes commitment $\com = Com_{pp}(x; \delta)$.\\
%For ease of notation we write $Com = Com_{pp}$.
%\dnote{All algorithm names in mathsf (macro for each). Eg. in the above paragraph, Setup, Com, ...}
%\paragraph{\textbf{Homomorphic Commitment:}} A homomorphic commitment scheme is a non-interactive commitment scheme such that $M_{pp},R_{pp}$ and $C_{pp}$ are all abelian groups, and for all $x_1,x_2 \in M_{pp}, \delta_1,\delta_2 \in R_{pp}$, we have $Com(x_1; \delta_1) + Com(x_2; \delta_2) = Com(x_1 + x_2; \delta_1 + \delta_2)$
%\dnote{Hiding and binding are the core properties of a commitment scheme, i.e., a definition of a commitment scheme includes the properties of hiding and binding. So bring them first. And just mention ``Hiding'' and ``Binding''.}
%\paragraph{\textbf{Hiding Commitment:}} A commitment scheme is said to be hiding if for all PPT adversaries $\Adv$ there exists a negligible function $\mu(\lambda)$ such that
%$$ |Pr[b=b'|pp\leftarrow Setup(1^{\lambda}); (x_0,x_1)\in M^2_{pp}\leftarrow \Adv(pp), b\in_R\{0,1\}, \delta \in_R R_{pp}, \com=Com(x_b;\delta), b'\leftarrow \Adv(pp,com)]-\frac{1}{2}|\leq \mu(\lambda)$$
%\paragraph{\textbf{Binding Commitment:}} A commitment scheme is said to be binding if for all PPT adversaries $\Adv$ there exists a negligible function $\mu$ such that 
%$$Pr[Com(x_0;\delta_0)=Com(x_1,\delta_1) \wedge x_0\neq x_1| pp\leftarrow Setup(1^{\lambda})x_0,x_1,\delta_0, \delta_1\leftarrow \Adv(pp)]\leq \mu(\lambda)$$
%\dnote{Have a subsection* with Pedersen commitment, and have the vector commitment also in the same part.}
%\paragraph{\textbf{Pedersen Commitment:}} $M_{pp}, R_{pp} = \mathbb{Z}_p, C_{pp} = \mathbb{G}$ of order $p$.\\
%$Setup : g, h \in_R \mathbb{G}$\\
%$Com(x,\delta)=(g^xh^{\delta})$
%\paragraph{\textbf{Pedersen Vector Commitment:}} $M_{pp}= \mathbb{Z}^n_p , R_{pp} = \mathbb{Z}_p, C_{pp}= \mathbb{G}$ with G of order p.\\ 
%$Setup: \vc{g}=(g_1,\cdots,g_n),h \in_R \mathbb{G}$\\
%$Com(\vc{x} = (x_1,\cdots,x_n);\delta) = h^r\vc{g}^{\vc{x}} = h^r \prod\limits_i g_i^{x_i} \in \mathbb{G}$