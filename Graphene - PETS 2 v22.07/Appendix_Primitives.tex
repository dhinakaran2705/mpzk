%----------------------------------------------------------------
\subsection{Basic Notations}\label{sec:basicnotation}
%------
In this section, we describe the notation that will be followed in rest of the
paper. We will often recall the notation in appropriate places. 
We use $[n]$ to
denote the set $\{1,\ldots,n\}$. All the arithmetic circuits and constraint
systems are assumed to be defined over finite field denoted by $\FF$. For a
vector $x\in \FF^m$,  $x_i$ and $x[i]$ represent the $i^{th}$
component of $x$ for $i\in [m]$. % and $x[1\mbox{:}m']$ stands for the the first $m'$ elements of $x$, where $m' \leq m$.
For an $m\times n$ matrix over
$\FF$, we use (a) $A[i,j]$ to denote $(i,j)^{th}$ entry, (b) $A[i,\cdot]$
to denote the $i^{th}$ row of %, (c) $A[\cdot,j]$ to denote $j^{th}$ column, (d) $A[1\mbox{:}m',1\mbox{:}n']$ to denote the sub-matrix consisting of first $m'$ rows and $n'$ columns
$A$. Similarly, for an $p\times m\times n$  matrix $X$, we use (a) $X[i,j,k]$ to
denote the $(i,j,k)^{th}$ entry, (b)  $X[i,\cdot,\cdot]$ to denote
the $m\times n$ matrix $Y$ given by $Y[j,k] := X[i,j,k]$, 
(c) and analogously $X[\cdot,j,\cdot]$ and $X[\cdot,\cdot,k]$ to denote $n\times p$ and $p\times m$
matrices respectively.  We will refer to sub-matrices $X[i,\cdot,\cdot]$ as {\em slices} of
$X$, sub-matrices $X[\cdot,j,\cdot]$ as {\em slabs}  of $X$ and sub-matrices $X[\cdot,\cdot,k]$ as the {\em planes} of $X$. 
$X^T$ denotes transpose of a  two dimensional matrix $X$. Inner-product of two vectors $X,Y$ is denoted as $\langle X,Y\rangle$.
Inner-product of two three or two dimensional  matrices $X$ and $Y$, denoted as 
$\langle X,Y\rangle$,  implies the inner-product of the corresponding one-dimensional vectors obtained from unrolling  the matrices.


Throughout the paper we use several distance metrics on vectors and matrices,
which we now introduce. For two vectors $x,y$ of length $n$, we define
$\Delta(x,y)=|\{i\in [n]: x_i\neq y_i\}|$. For two $m\times n$ matrices $A,B$ we
define $\Delta_1(A,B)=|\{j\in [n]: A[\cdot,j]\neq B[\cdot,j]\}|$ and
$\Delta_2(A,B)=|\{i\in [m]: A[i,\cdot]\neq B[i,\cdot]\}|$. For $p\times m\times
n$ matrices $X$ and $Y$, we define $\Delta_1(X,Y)=\{k\in
[n]:X[\cdot,\cdot,k]\neq Y[\cdot,\cdot,k]\}$ (no of planes that are different between the two matrices). Similarly we define $\Delta_2(X,Y)
= |\{j\in [m]: X[\cdot,j,\cdot]\neq Y[\cdot,j,\cdot]\}|$  (no of slabs that are different between the two matrices). Let $u$ be a vector
and $S$ be a set of vectors. We use $d(u,S) := \min\{\Delta(u,v):v\in S\}$ to
denote distance between $u$ and $S$. Similarly for a matrix (two or three
dimensional) $U$ and a set of matrices $\mc{M}$, we define $d_i(U,\mc{M}) :=
\min\{\Delta_i(U,M): M\in \mc{M}\}$ for $i=1,2$.

We use $\sample$ to denote drawing from a distribution uniformly at random.
%---------
\subsection{Coding Theory primitives}\label{subsec:coding_theory}
%\subsection{Linear Codes}
\begin{definition}[Linear Codes]\label{defn:lincode}
	Let $n,k,d$ be positive integers with $n\geq k$ and $\FF$ be a finite field. We
	call $L\subseteq \FF^n$ to be an $[n,k,d]$ linear code if $L$ is a $k$-dimensional
	subspace of $\FF^n$ and $d$ is the minimum value of $\Delta(x,y)$ for distinct
	$x,y\in L$. Elements of $L$ are conventionally called {\em codewords}.
\end{definition}
%-------------------------

For an $[n,k,d]$ code $L$, an $n\times k$ matrix $\calG$ is called a {\em
	generator matrix} for $L$ iff (i) $\calG x\in L$ for all $x \in \bbF^k$ and (ii)
$\calG x\neq \calG y$ for $x\neq y$. Clearly, such a matrix $\calG$ has rank
$k$. Similarly an $n\times (n-k)$ matrix $\calH$ such that $y^T \calH = 0$ for
all $y\in L$ is called a {\em parity check} matrix for $L$. It is known
that the above two matrices exist for any $[n,k,d]$ linear code $L$. We will
assume that description of the linear code $L$ includes a generator matrix
$\calG$ and a parity check matrix $\calH$.
%-------------------------

\begin{definition}[Interleaved Code]\label{defn:interleavedcode}
	For an $[n,k,d]$ linear code $L$ and a positive integer $m$, we define a {\em row interleaved code} $\ric{L}{m}$ to be the set of $m\times n$ matrices $A$ such that each row of $A$ is a codeword in $L$. Similarly, we define a {\em column interleaved code} $\cic{L}{m}$ to be the set of $n\times m$ matrices $B$ such that each column of $B$ is a codeword in $L$.
\end{definition}
%-------------------------

For  an  $[n,k,d]$ linear code $L$ over $\FF$, one can view $\ric{L}{m}$ as
an $[mn,mk,d]$ linear code over  $\FF$ or alternatively,  as an $[n,k,d]$ code over $\HH$, where $\HH\cong \FF^m$. 

Analogous to the row interleaved code, a column interleaved code $\cic{L}{m}$ 
can be viewed as an $[n,k,d]$ code over $\bbF^m$ by viewing each row of the
codeword $B\in \cic{L}{m}$ as an element in $\bbF^m$. Here the distance metric
for $\ric{L}{m}$ is given by the matrix distance $\Delta_1$, while the distance
metric for $\cic{L}{m}$ is given by the matrix distance $\Delta_2$ as defined in
Section \ref{sec:basicnotation}.  
%-----------------------

\begin{definition}[Product Code]\label{defn:productcode}
	Let $L_i$ be an $[n_i,k_i,d_i]$-linear code for $i=1,2$. We define the product
	code $L_1\otimes L_2$ to be the code consisting of $n_2\times n_1$ matrices $A$
	such that each row of $A$ is a codeword in $L_1$ and each column of $A$ is a
	codeword in $L_2$.
\end{definition}
%-----------------------

Note that by definition, the product code $L_1\otimes L_2$ is a row interleaved
code of $L_1$ and a column interleaved code of $L_2$, i.e $L_1\otimes L_2 =
\ric{L_1}{n_2}\cap \cic{L_2}{n_1}$. For $A,A'\in L_1\otimes L_2$, we define
$\dham_1(A,A')=|\{i\in [n_1]: A[\cdot,i]\neq A'[\cdot,i]\}|$ and
$\dham_2(A,A')=|\{i\in [n_2]: A[i,\cdot]\neq A'[i,\cdot]\}|$. The distance
$\dham_1$ corresponds to distance function of the code $\ric{L}{n_2}$, where we
view $A,A'$ as codewords in $\ric{L}{n_2}$. Similarly, the distance $\dham_2$
corresponds to the distance function of the code $\cic{L}{n_1}$.
%----------------------

\begin{definition}[Reed-Solomon Code]\label{defn:rscode}
	An $[n,k]$-Reed Solomon Code $L\subseteq \bbF^n$ consists of vectors $(p(\eta_1),\ldots,p(\eta_n))$ for polynomials $p\in \bbF[x]$ of degree less than $k$ where $\eta_1,\ldots,\eta_n$ are distinct points in $\FF$. We will use
	$\rsc{\eta}{n,k}$ to denote the Reed Solomon code with $\bm{\eta}=(\eta_1,\ldots,\eta_n)$ and $deg(p)<k$. 
\end{definition}

\begin{lemma}[\cite{CodingTheory}]\label{lem:bicdecoding}
	Suppose	a matrix $U^\ast\in \FF^{h\times n}$ satisfies $d_1(U^\ast, \ric{\rsc{\eta}{n,\ell}}{h})<e_1 \leq n-\ell$ and $d_2(U^\ast, \cic{ \rsc{\alpha}{h,m}}{n})<e_2 \leq h-m$. Then, there exists $U\in \rsc{\eta}{n,\ell} \otimes \rsc{\alpha}{h,m}$ and sets $S_1\subseteq [n]$, $S_2\subseteq [h]$ with $|S_1|>n-e_1$ and $|S_2|>h-e_2$ such that $U^\ast[i,j]=U[i,j]$ for $(i,j)\in S_1\times S_2$.
\end{lemma}
%----------------------------------------

\subsection{Coding Theory Results for our Constructions}\label{new_result_coding_theory}
%--------------
The following coding theoretic result is the key to testing proximity of a three-dimensional matrix to a well-formed encoding. 
\begin{proposition}[3D Compression]\label{lem:3dcompression}
	Let $L$ be an $[n,k,d]$ code over $\FF$, and $\mc{C} :=
	\ric{L}{m}$ be a row interleaved code of $L$. Let $\mc{C}^p$ denote the set of
	$p\times m\times n$ matrices $U$ over $\FF$ such that $U[i,\cdot,\cdot]\in
	\mc{C}$ for all $i\in [p]$. Let $U^\ast$ be a
	$p\times m\times n$ matrix such that $d_1(U^\ast,\mc{C}^p)>e$.  Then for any
	$m\times n$ matrix $u_0$ over $\FF$, we have 
	\[ \prob{d_1\left(u_0 + \sum_{i=1}^p r_iU^\ast[i,\cdot,\cdot], \mc{C}\right)\leq e}\leq
	\frac{d}{|\FF|}\]  
	for a uniformly sampled $(r_1,\ldots,r_p)\sample \FF^p$. 
\end{proposition}
%-------------
%------------
%\paragraph*{Proof of Proposition \ref{lem:3dcompression}} \label{app:ProofofLem3dcompression}
%-------
We need several observations to prove the proposition, which we present next.
Throughout this section, let $L$ be a linear $[n,k,d]$ code over a field $\HH$ and let $\FF\subseteq \HH$ be a subfield.  Let $m\geq 1$ be an integer, and let $\mc{C}$ denote the row interleaved code $\ric{L}{m}$. For a matrix $U\in \HH^{m\times n}$ and a vector $u_0\in \HH^n$, let $\aff{u_0}{U}$ denote the affine space as:
{
	\begin{equation}\label{eq:affspace}
	\aff{u_0}{U} := \{u_0+r^TU: r\in \FF^m\}
	\end{equation}}
Note that in the above, the scalars in the linear combination come from $\FF$.

The following result was proved in \cite{ligero} for the setting $\HH=\FF$.
For completeness, we present an adaptation of the proof to the setting where
$\FF$ is a subfield of $\HH$.
\begin{lemma}\label{lem:farpoint}
	Let $L$ and $\mc{C}$ be codes as defined, and let $e$ be a positive integer such that $e+2\leq |\FF|$. Then for any $u_0\in \HH^n$ and any $U^\ast\in \HH^{m\times n}$ such that $d(U^\ast,\mc{C})>e$, there exists $v\in \aff{u_0}{U^\ast}$ such that $d(v,L)>e$.
\end{lemma} 
\begin{proof}
	For sake of contradiction, assume that $d(u,L)\leq e$ for all $u\in
	\aff{u_0}{U^\ast}$. Let $x$ be the point in $\aff{u_0}{U^\ast}$ such that
	$d(x,L)$ is maximum. By assumption $d(x,L)\leq e$. Let $v\in L$ be such that
	$\Delta(x,v)=d(x,L)$. Let $E\subseteq [n]$ be the set of positions where $x$ and
	$v$ differ. Since $d(U^\ast,\mc{C})>e$, there exists row $R$ of $U^\ast$ and there is some 
	position $j\in [n]\backslash E$ 
	such that $R_j\neq 0$. Let
	$\alpha_1,\ldots,\alpha_{e+1}$ be distinct non zero elements in $\FF$. Let $E_k$
	for $k=1,\ldots,e+1$ denote the set of positions where $x+\alpha_kR$ and $v$
	differ. Fix a position $i\in E$. Then there exists at most one $k\in [e+1]$ such
	that $i\not\in E_k$. By pigeon hole principle, there exists $k\in [e+1]$ such
	that $E\subseteq E_k$. We also observe that since $\alpha_k\neq 0$, $j\in E_k$.
	Thus $d(x+\alpha_k,v)>d(x,v)$, contradicting the choice of $x$. This proves the
	lemma.   
\end{proof}
%------------------------------

Next we prove a result about lines with respect to linear codes. The result was proved in \cite{ligero} for the case $\HH=\FF$ and $e<d/4$. The authors in \cite{ligero} conjectured the result for $e<d/3$. Here we prove the result for $e<d/3$ when $\FF$ can be an arbitrary subfield of $\HH$.
%------------------------------
\begin{lemma}[Affine Line]\label{lem:affineline}
	Let $L$ be the linear code as defined. Define an affine line $\ell_{u,v}$ in $\HH^n$ as $\ell_{u,v} := \{u + \alpha v:\alpha\in \FF\}$ for $u,v\in \HH^n$. Then for $e < d/3$ and any affine line $\ell_{u,v}$:
	%-----
	\begin{enumerate}[{\rm (i)}]
		%-----
		\item $d(x,L)\leq e$ for all $x\in \ell_{u,v}$, or
		%-----
		\item $d(x,L)> e$ for at most $d$ points in $\ell_{u,v}$.
		%-----
	\end{enumerate}
	%-----
\end{lemma}
%-----------
\begin{proof}
	%\nnote{Candidate for Appendix: Putting it here just to have all content for the moment}
	In this proof, let $\delta(x,y)$ denote the set of positions where the vectors
	$x$ and $y$ differ, and let $\wt{x}$ denote $|\delta(x,\bm{0})|$. The above is equivalent to
	proving that if there exist $d+1$ distinct points $X =
	\{x_1,\ldots,x_{d+1}\}\subseteq \ell_{u,v}$ such
	that $d(x_i,L)\leq e$ for all $i\in [d+1]$, then $d(x,L)\leq e$ for all $x\in
	\ell_{u,v}$. Assume that there exists such a set $X$ of $d+1$ points. Let $\ell_i$ denote the point in $L$ closest to $x_i$ for $i\in
	[d+1]$. Set $\eta_i=x_i-l_i$ for all $i$ and let
	$\bm{\eta}=\{\eta_1,\ldots,\eta_{d+1}\}$. By assumption we have
	$\wt{\eta_i}\leq e$ for all $i\in [d+1]$. Since $\ell_{u,v}$ is contained in affine
	span of any two distinct points on it, we have tuples
	$\{(\alpha_i,\beta_i)\}_{i\in [d+1]}d$ such that $\alpha_i + \beta_i=1$ and $x_i=\alpha_ix_1 +
	\beta_ix_2$ for $i\in [d+1]$. Note that $(\alpha_1,\beta_1)=(1,0)$ and
	$(\alpha_2,\beta_2)=(0,1)$. Observe that $\alpha_i$'s and $\beta_i$'s must be
	distinct for all $i\in [d+1]$. We call $X$ as {\em degenerate} if there exist
	$i\neq j$ satisfying $\alpha_j=\gamma\alpha_i$ and $\beta_j=\gamma\beta_j$ for
	some $\gamma\in \FF\backslash {0}$. We consider two cases:
	
	\noindent{\em $X$ is degenerate}: Degeneracy implies there exist $i\neq j$ such that $x_i=\gamma x_j$ for
	some $\gamma\in \FF\backslash\{0\}$. In this case we have $0 =
	\frac{1}{1-\gamma}x_i -\frac{\gamma}{1-\gamma}x_j\in \ell_{u,v}$. This implies
	$\ell_{u,v}=\{\alpha x_i: \alpha\in \FF\}$
	and hence $d(x,L)=d(x_i,L)\leq e$ for all $x\in \ell_{u,v}$. Thus the statement
	of the Lemma holds in this case.\smallskip 
	
	\noindent{\em X is not degenerate}: We first prove
	that $\ell_i=\alpha_i\ell_1+\beta_i\ell_2$ and
	$\eta_i=\alpha_i\eta_1+\beta_i\eta_2$ for
	all $i\in [d+1]$. We have
	%--------
	\begin{align*}
		\ell_i+\eta_i& = x_i = \alpha_ix_1 + \beta_ix_2 = \alpha_i(\ell_1+\eta_1) + \beta_i(\ell_2 + \eta_2) \\
					 & = (\alpha_i\ell_1 + \beta_i\ell_2) + (\alpha_i\eta_1 + \beta_i\eta_2)
	\end{align*}
	%--------
	Rearranging we have,
	$
	\ell_i - (\alpha_i\ell_1 + \beta_i\ell_2) = \alpha_i\eta_1 +
	\beta_i\eta_2 - \eta_i
	$.
	%-------
	In the above equation we see that LHS is a vector in $L$. Further $\wt{\alpha_i\eta_1 + \beta_i\eta_2 - \eta_i}\leq \wt{\eta_1}+\wt{\eta_2}+\wt{\eta_i}\leq 3e < d$. Thus the LHS must be equal to $0$ and hence $\ell_i = \alpha_i\ell_1 + \beta_i\ell_2$ and $\eta_i=\alpha_i\eta_1+\beta_i\eta_2$. Observe that any $x^\ast\in \ell_{u,v}$ can
	be written as $x^\ast=\alpha^\ast x_1+\beta^\ast x_2$. Thus $d(x^\ast,L)=d(\alpha^\ast x_1+\beta^\ast x_2,L)\leq \wt{\alpha^\ast \eta_1+\beta^\ast \eta_2}\leq |E|$ where $E$ denotes the set $\delta(\eta_1,0)\cup \delta(\eta_2,0)$. Our final effort will be to show that $|E|\leq e$.
	%-------
	
	\noindent{\it Claim}: $|E|\leq e$ where $E = \delta(\eta_1,0)\cup
	\delta(\eta_2,0)$. We consider the partition of $E$ into sets
	$E_0=\delta(\eta_1,0)\backslash \delta(\eta_2,0)$,
	$E_1=\delta(\eta_2,0)\backslash \delta(\eta_1,0)$ and
	$E_{01}=\delta(\eta_1,0)\cap \delta(\eta_2,0)$. Let $t=|E|$. Consider a $t\times (d+1)$ matrix $M=(m_{ij})$
	where $m_{ij}=0$ if $j^{th}$ coordinate ($\eta_i^j$) of $\eta_i$ is zero, and $m_{ij}=1$
	otherwise. We will show that each row of $M$ has at most one $0$. Assume that
	there exists $i$ such that $m_{ip}=0$ and $m_{iq}=0$ for $p\neq q$. We consider
	three cases:
	%------
	\begin{itemize}
		%-----
		\item[--] If $i\in E_0$, the above condition implies $\alpha_p\eta_1^i = \alpha_q\eta_1^i=0$, or $\alpha_p=\alpha_q=0$ as $\eta_1^i\neq 0$ for $i\in E_0$.
		This contradicts the fact that $X$ is not degenerate.
		%-----
		\item[--] The case $i\in E_1$ is similar to above.
		%-----
		\item[--] If $i\in E_{01}$ we have $\alpha_p\eta_1^i+\beta_p\eta_2^i=\alpha_q\eta_1^i+\beta_q\eta_2^i=0$ which implies $\alpha_p/\beta_p = \alpha_q/\beta_q = -\eta_2^i/\eta_1^i $, or $\alpha_p/\alpha_q = \beta_p/\beta_q$ which contradicts the fact that $X$ is not degenerate. Note that all denominators can be argued to be non-zero for $i\in E_{01}$.
		%-----
	\end{itemize}
	%-----
	
	From the above, we conclude that each row has at least $d$ entries as $1$.
	Counting by columns, we have that each column has at most $e$ entries as $1$
	(since $\wt{\eta_i}\leq e$ for all $i\in [d]$. Comparing the lower and upper
	bounds on the number of $1$ entries in the matrix we have $td \leq e(d+1)$
	which implies $t \leq e + e/d < e + 1$. Thus $t\leq e$, as we wanted to show.
	This completes the proof.
	%-----
\end{proof}
%------------------------------------------------------
The following result underlies proximity protocols in our work and in
\cite{ligero}. Intuitively the result states that if a matrix is far away from the code $\mc{C}$, a random linear combination of its rows is far away from a codeword in $L$, and thus the proximity of the matrix to $\mc{C}$ may be tested by testing the proximity of a random linear combination of its rows to $L$.
%-------
\begin{lemma}[Affine Subspace]\label{lem:affinesubspace}
	Let the codes $L$ and $\mc{C}$ be as defined and $e<d/3$ be an integer. Let $U\in \HH^{m\times n}$ be a matrix such that $d(U,\mc{C})>e$. Then for any $u_0\in \HH^n$, $\prob{d(u_0+r^TU,L)\leq e}\leq d/|\FF|$ for uniformly sampled $r\sample \FF^m$.
\end{lemma}
%-------
\begin{proof}
	From Lemma \ref{lem:farpoint}, there exists $u\in \aff{u_0}{U}$ such that $d(u,L)>e$. Now we can write $\aff{u_0}{U}$ as union of affine lines passing through $u$. Applying lemma \ref{lem:affineline} to each line, we see that at most $d$ points $x$ on each affine line satisfy $d(x,L)\leq e$. Thus, a randomly sampled point $x$ in $\aff{u_0}{U}$, equivalently obtained as $u_0+r^TU$ for a randomly sampled vector $r\in \FF^m$ satisfies $d(x,L)$ with probability at most $d/|\FF|$.
\end{proof}
%---------
We are now in a position to prove Proposition \ref{lem:3dcompression}.
%--------
\begin{proof}[Proof of Proposition \ref{lem:3dcompression}]
	Let $\HH$ denote the field $\FF^m$. Then $u_0$ can be viewed as a point in
	$\HH^n$. Similarly $U$ can be viewed as $p\times n$ matrix over $\HH$.
	We consider $\mc{C}$ as $[n,k,d]$ code over $\HH$ and $\mc{C}^p$ as the
	interleaved code of $\mc{C}$ over the field $\HH$. Then by applying Lemma \ref{lem:affinesubspace}
	with $\HH=\FF^m$ and codes $\mc{C}$ and $\mc{C}^p$ in place of codes $L$ and	$\mc{C}$, we have the desired bound.
\end{proof}
%---------

%\paragraph*{Commitment and Inner-Product Argument}
\subsection{Vector Commitment Schemes and Inner-product Arguments}
%-------------
%-------------
\paragraph*{Functional Commitment for linear functions}

Let $\calD$ be a domain and consider linear functions $\innp{\cdot}{\cdot} : \calD^n \times \calD^n \rightarrow \calD$ defined by $\innp{{\bf x}}{{\bf m}} = \sum_{i=1}^n x_im_i$ for ${\bf x, m} \in  \calD^n$ with ${\bf x} = (x_1, \ldots , x_n)$, ${\bf m} = (m_1, \ldots, m_n)$. 
A functional commitment scheme $\FC$ is a tuple of four probabilistic polynomial-time algorithms - $(\setup, \comm, \open, \verif)$.

\noindent$\setup(1^\secp, 1^n):$ takes in a security parameter $\secp \in \NN$, size of the message $n \in \poly$ and outputs a commitment key $CK$.\\
%----
\noindent$\comm(CK, {\bf m}):$ takes commitment key $CK$ and a $n$ length message ${\bf m}$ as input. It outputs a commitment $C$ for ${\bf m}$ and auxiliary information denotes $\aux$.\\
%----
\noindent$\open(CK, C, \aux, {\bf x}):$ takes as input the commitment key $CK$, a commitment $C$ (to ${\bf m}$), auxiliary information (possibly containing ${\bf m}$) and a vector ${\bf x} \in \calD^n$; computes a witness
$W_y$ for $y = \innp{{\bf x}}{{\bf m}}$ i.e., $W_y$ is a witness for the fact that the linear function defined by ${\bf x}$ when evaluated on ${\bf m}$ gives $y$.\\
%----
\noindent$\verif(CK, C, W_y, {\bf x}, y):$ takes as input the commitment key $CK$, a commitment $C$, a witness $W_y$, a vector ${\bf x} \in \calD^n$ and $y \in \calD$; outputs $W_y$ is a witness for $C$ being a
commitment for some ${\bf m} \in \calD^n$ such that $\innp{{\bf x}}{{\bf m}} = y$ and outputs $0$ otherwise.

{\it Correctness: } For every $CK \leftarrow \setup(1^\secp, 1^n)$, for all ${\bf x, m} \in \calD^n$, if $(C, \aux) \leftarrow \comm(CK, {\bf m})$ and $W_y \leftarrow \open(CK, C, \aux, {\bf x})$, then $\verif(CK, C, W_y, {\bf x}, y) = 1$ with probability 1.


{\it Hiding: } For a key $CK$ generated by an honest setup, for all ${\bf m_1, m_2} \in \calD^n$ with ${\bf m_1} \neq {\bf m_2}$, the two distributions $\{CK, \comm(CK, {\bf m_1})\}$ and $\{CK, \comm(CK, {\bf m_2})\}$ are indistinguishable.

{\it Function binding: } A functional commitment scheme $\FC = (\setup, \comm, \open, \verif)$ for $(\calD, n,\innp{\cdot}{\cdot})$ is said to be computationally binding if any PPT adversary $\Adv$ has negligible advantage in winning the following game.
\begin{enumerate}
	%----
	\item The challenger generates $CK$ by running $\setup(1^\secp, 1^n)$ and gives $CK$ to $\Adv$.
	%----
	\item The adversary $\Adv$ outputs a commitment $C$, a vector ${\bf x} \in \calD^n$, two values $y, y' \in \calD$ and two witnesses $W_y, W_{y'}$ . We say that $\Adv$ wins the game if the following conditions hold.
	\begin{itemize}
		\item $y' = y$;
		%----
		\item $\verif(CK, C, W_y, {\bf x}, y) = \verif(CK, C, W_{y'} , {\bf x}, y') = 1$.
	\end{itemize}
\end{enumerate}
%----------------------
Functional commitment is generalization of vector commitments. In our constructions, vector commitment suffices. We initiate $\FC$ with Pedersen commitment scheme or Pedersen vector commitment scheme accordingly.

We define an interactive protocol that allows proving inner-product relation over committed vectors. For this we are using Pedersen vector commitment scheme $\comm$.
%, where the commitment corresponds to a non-interactive perfectly hiding and computationally binding commitment scheme, with an additional property of homomorphicity.  We use Pedersen vector commitment scheme $\comm$ with message space $\FF^m$, commitment space $\GG$ and randomness space $\FF$. For a set of generators $\bm{g}=(g_1,\ldots,g_m,h)$, the commitment of vector $x\in \FF^m$ with commitment randomness $r\in \FF$ is given by $\comm(x,r)=h^r \prod_{i=1}^m (g_i)^{x_i} $. This commitment is homomorphic with $\calC=\GG$ as the commitment space.
%---------

We now define the language for inner-product w.r.t. the above  as $\calL \subseteq \FF^m \times \calC \times \FF$ given by:
%------
\begin{equation*}
%\begin{aligned}[c]
\begin{array}{l}
\calL = \{(x,c,z):\exists \; y,r \text{ s.t. } 
c=\com(y,r)  \text{ and } \innp{x}{y}=z\} 
%\end{aligned}
\end{array}
\end{equation*}
%-------
%The $\NP$ relation $\calR_\sigma$ for the language $\calL_\sigma$ consists of pairs $(\stmt,\wit)$ with $\stmt=(c_1,c_2,v)$ and $\wit=(x_1,x_2,r_1,r_2)$ such that $c_1=\com(x_1,r_1)$, $c_2=\com(x_2,r_2)$ and $\innp{x_1}{x_2}=v$.
%-------
%Interestingly, the inner-product argument is used to save on communication. Proving an inner-product statement requires sending $y$ on clear which costs more than the argument-size. Our language for inner-product is a simpler version of the one defined in ~\cite{bulletproofs}, where the first argument is also hidden in a commitment. 
Our language for inner-product is a simpler version of the one defined in ~\cite{bulletproofs}, where the first argument is also hidden in a commitment. The inner product argument is used to reduce the communication.
%---------------------------

%We mention some instantiations inner-product arguments in Appendix~\ref{inner-product-instan}.  
We use inner-product argument from Bulletproofs~\cite{bulletproofs}. Notably, we use a variant of inner-product where one of the vector is public and the other one is private. $\innerproduct(\mathsf{pp}, x, \cm, v ; z)$ is the notation we will be using through out the paper, where the public parameter $\mathsf{pp}$  consists of group description and generator, $x$ is a public vector and $\cm$ is the commitment of the private vector $z$ such that $\innp{x}{z} = v$.




In our work, we are using Pedersen vector commitment scheme, $\comm$ with message space $\FF^m$, commitment space $\GG$ and randomness space $\FF$. For a set of generators $\bm{g}=(g_1,\ldots,g_m,h)$, the commitment of vector $x\in \FF^m$ with commitment randomness $r\in \FF$ is given by $\comm(x,r)=h^r \prod_{i=1}^m (g_i)^{x_i} $. This commitment is homomorphic with $\calC=\GG$ as the commitment space. This is a non-interactive perfectly hiding and computationally binding commitment scheme.

\begin{definition}[Inner-product Argument \cite{InnerProductDLS,bulletproofs} ]\label{defn:innerproductarg} 
	We call an interactive protocol $\innp{\pip}{\vip}$ consisting of $\ppt$ interactive algorithms $\pip$ and $\vip$ an inner-product argument for commitment scheme $(\gen,\com)$ if it recognizes the language $\calL$ as defined previously. Namely, $\innp{\pip}{\vip}$ satisfies the following:
	\begin{enumerate}[{\rm (i)}]
		%--------
		\item {\bf Completeness}: For all $\adv$, the following should be $1$
		
		\footnotesize %\\~
		\begin{align*}
		\condprob{
			\begin{array}{l}
			(\stmt,\wit)\in \calR \wedge \\ \langle \pip(\mathsf{pp},\stmt,\wit),\vip(\mathsf{pp},\stmt)\rangle=1
			\end{array}}{
			\begin{array}{l}
			\mathsf{pp}\gets \gen(\secparam);\\
			(\stmt,\wit)\gets \adv(\secparam,\mathsf{pp})
			\end{array}
		}
		\end{align*}
		\normalsize
		%------------
		\item{\bf Soundness}: For all deterministic polynomial time $\prover^*$ and $\ppt$  $\adv$, the following should be at most $\negl(\lambda)$.
		
		{\footnotesize %\\~
			
			\begin{align*}
			\condprob{
				\begin{array}{c}
				\stmt\not\in \calL \wedge \\
				\langle
				\prover^*(\mathsf{pp},\stmt,s),\vip(\mathsf{pp},u)\rangle=1
				\end{array}}{
				\begin{array}{l}
				\sigma\gets \gen(\secparam);\\
				(\stmt,s)\gets \adv(\secparam,\mathsf{pp})
				\end{array}
			} 
			\end{align*}
		}
		%--------------
	\end{enumerate}
	
\end{definition}
